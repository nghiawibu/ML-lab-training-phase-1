{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 500\n",
    "NUM_CLASSES = 20\n",
    "unknown_ID = 0\n",
    "padding_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index,value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "            \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):  \n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = range(len(self._data))\n",
    "            random.seed(2018)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = parent_path + '/' + newsgroup + '/'\n",
    "\n",
    "            files = [(filename, dir_path + filename) for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
    "            files.sort()\n",
    "            label = group_id\n",
    "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
    "            for filename, filepath in files:\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            word_count[word] += 1\n",
    "                    content = ' '.join(words)\n",
    "                    assert(len(content.splitlines())==1)\n",
    "                    data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
    "        return data\n",
    "    word_count = defaultdict(int)\n",
    "    path = '../datasets/20news-bydate'\n",
    "    parts = [path + '/' + dir_name + '/' for dir_name in os.listdir(path) if not os.path.isfile(path+dir_name)]\n",
    "    train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
    "\n",
    "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    train_data = collect_data_from(\n",
    "        parent_path=train_path,\n",
    "        newsgroup_list=newsgroup_list,\n",
    "        word_count=word_count\n",
    "    )\n",
    "    vocab = [word for word,freq in \n",
    "            zip(word_count.keys(), word_count.values()) if freq > 10]\n",
    "    vocab.sort()\n",
    "    with open('../datasets/w2v/vocab_raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    test_data = collect_data_from(\n",
    "        parent_path=test_path,\n",
    "        newsgroup_list=newsgroup_list\n",
    "    )\n",
    "\n",
    "    with open('../datasets/w2v/20news-train-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open('../datasets/w2v/20news-test-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path):\n",
    "    with open('../datasets/w2v/vocab_raw.txt') as f:\n",
    "        vocab = dict([(word, word_ID+2) for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')[0],line.split('<fff>')[1],line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    encoded_data = []\n",
    "\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_DOC_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(unknown_ID))\n",
    "        \n",
    "        if(len(words)<MAX_DOC_LENGTH):\n",
    "            num_padding = MAX_DOC_LENGTH - len(words)\n",
    "            for _ in range(num_padding):\n",
    "                encoded_text.append(str(padding_ID))\n",
    "\n",
    "        encoded_data.append(str(label)+'<fff>'+str(doc_id)+'<fff>'+str(sentence_length)+'<fff>' +' '.join(encoded_text))\n",
    "\n",
    "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
    "    file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "    with open(dir_name + '/' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data('../datasets/w2v/20news-train-raw.txt')\n",
    "encode_data('../datasets/w2v/20news-test-raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 lstm_size,\n",
    "                 batch_size\n",
    "                 ):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._lstm_size = lstm_size\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._data = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
    "        self._labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        self._sentence_lengths = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        self._final_tokens = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
    "\n",
    "    def embedding_layer(self, indices):\n",
    "        pretrained_vectors = []\n",
    "        pretrained_vectors.append(np.zeros(self._embedding_size))\n",
    "        np.random.seed(2022)\n",
    "        for _ in range (self._vocab_size + 1):\n",
    "            pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
    "\n",
    "        pretrained_vectors = np.array(pretrained_vectors)\n",
    "\n",
    "        self._embedding_matrix = tf.compat.v1.get_variable(\n",
    "            name='embedding',\n",
    "            shape=(self._vocab_size + 2, self._embedding_size),\n",
    "            initializer=tf.constant_initializer(pretrained_vectors)\n",
    "        )\n",
    "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
    "\n",
    "    def LSTM_layer(self, embeddings):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
    "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
    "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "\n",
    "        lstm_inputs = tf.unstack(\n",
    "            tf.transpose(embeddings, perm=[1,0,2])\n",
    "        )\n",
    "\n",
    "        lstm_outputs, last_state = tf.nn.static_rnn(\n",
    "            cell=lstm_cell,\n",
    "            inputs=lstm_inputs,\n",
    "            initial_state=initial_state,\n",
    "            sequence_length=self._sentence_lengths\n",
    "        )\n",
    "\n",
    "        lstm_outputs = tf.unstack(\n",
    "            tf.transpose(lstm_outputs, perm=[1,0,2])    \n",
    "        )\n",
    "        lstm_outputs = tf.concat(\n",
    "            lstm_outputs,\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        mask = tf.sequence_mask(\n",
    "            lengths=self._sentence_lengths,\n",
    "            maxlen=MAX_DOC_LENGTH,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        lstm_outputs = mask * lstm_outputs\n",
    "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
    "        lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
    "        lstm_outputs_average = lstm_outputs_sum/tf.expand_dims(\n",
    "            tf.cast(self._sentence_lengths, tf.float32),\n",
    "            -1\n",
    "        )\n",
    "        return lstm_outputs_average\n",
    "\n",
    "    def build_graph(self):\n",
    "        embeddings =  self.embedding_layer(self._data)\n",
    "        lstm_outputs = self.LSTM_layer(embeddings)\n",
    "        weights = tf.compat.v1.get_variable(\n",
    "            name='final_layer_weights',\n",
    "            shape=(self._lstm_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        biases = tf.compat.v1.get_variable(\n",
    "            name='final_layer_biases',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices=self._labels,\n",
    "            depth=NUM_CLASSES,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits (\n",
    "            labels=labels_one_hot,\n",
    "            logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.sqeeze(predicted_labels)\n",
    "\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_report = []\n",
    "accuracy_report = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_RNN():\n",
    "    with open('../datasets/w2v/vocab_raw.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "    \n",
    "    tf.compat.v1.set_random_seed(2023)\n",
    "    rnn = RNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=300,\n",
    "        lstm_size=50,\n",
    "        batch_size=50\n",
    "    )\n",
    "    predicted_labels, loss = rnn.build_graph()\n",
    "    train_op = rnn.trainer(loss=loss,learning_rate=0.01)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        train_data_reader = DataReader(\n",
    "            data_path='../datasets/w2v/20news-train-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "\n",
    "        test_data_reader = DataReader(\n",
    "            data_path='../datasets/w2v/20news-test-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "\n",
    "        step=0\n",
    "        MAX_STEP=1000 ** 2\n",
    "\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        while step<MAX_STEP:\n",
    "            next_train_batch = train_data_reader.next_batch()\n",
    "            train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
    "            plabels_eval, loss_eval, _ = sess.run(\n",
    "                [predicted_labels, loss, train_op],\n",
    "                feed_dict={\n",
    "                    rnn._data: train_data,\n",
    "                    rnn._labels: train_labels,\n",
    "                    rnn._sentence_lengths: train_sentence_lengths,\n",
    "                    rnn._final_tokens: train_final_tokens\n",
    "                }\n",
    "            )\n",
    "            step+=1\n",
    "            if step % 20 == 0:\n",
    "                print(\"loss: \", loss_eval)\n",
    "                \n",
    "            if train_data_reader._current_part == 0:\n",
    "                num_true_preds = 0\n",
    "                while True:\n",
    "                    next_test_batch = test_data_reader.next_batch()\n",
    "                    test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
    "\n",
    "                    test_plabels_eval = sess.run(\n",
    "                        predicted_labels,\n",
    "                        feed_dict = {...}\n",
    "                    )\n",
    "                    matches = np.equal(test_plabels_eval, test_labels)\n",
    "                    num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "                    if test_data_reader._batch_id == 0:\n",
    "                        break\n",
    "                \n",
    "                print(\"epoch: \", train_data_reader._num_epoch)\n",
    "                print(\"Accuracy on test data: \", num_true_preds/len(test_data_reader._data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_internal\n    ret = Operation(\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 74, in variable_op_v2\n    return gen_state_ops.variable_v2(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\MLLT\\code\\session4\\rnn.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_and_evaluate_RNN()\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\MLLT\\code\\session4\\rnn.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_and_evaluate_RNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mset_random_seed(\u001b[39m2023\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m rnn \u001b[39m=\u001b[39m RNN(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     vocab_size\u001b[39m=\u001b[39mvocab_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     embedding_size\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     lstm_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m predicted_labels, loss \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39;49mbuild_graph()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_op \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mtrainer(loss\u001b[39m=\u001b[39mloss,learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m sess:\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\MLLT\\code\\session4\\rnn.ipynb Cell 11\u001b[0m in \u001b[0;36mRNN.build_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_graph\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     lstm_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLSTM_layer(embeddings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     weights \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_variable(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfinal_layer_weights\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lstm_size, NUM_CLASSES),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mrandom_normal_initializer(seed\u001b[39m=\u001b[39m\u001b[39m2018\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\MLLT\\code\\session4\\rnn.ipynb Cell 11\u001b[0m in \u001b[0;36mRNN.embedding_layer\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     pretrained_vectors\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(loc\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, scale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pretrained_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(pretrained_vectors)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_matrix \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mget_variable(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     shape\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_size \u001b[39m+\u001b[39;49m \u001b[39m2\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_size),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     initializer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mconstant_initializer(pretrained_vectors)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/MLLT/code/session4/rnn.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39membedding_lookup(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_matrix, indices)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1579\u001b[0m, in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mget_variable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_variable\u001b[39m(name,\n\u001b[0;32m   1565\u001b[0m                  shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1577\u001b[0m                  synchronization\u001b[39m=\u001b[39mVariableSynchronization\u001b[39m.\u001b[39mAUTO,\n\u001b[0;32m   1578\u001b[0m                  aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE):\n\u001b[1;32m-> 1579\u001b[0m   \u001b[39mreturn\u001b[39;00m get_variable_scope()\u001b[39m.\u001b[39;49mget_variable(\n\u001b[0;32m   1580\u001b[0m       _get_default_variable_store(),\n\u001b[0;32m   1581\u001b[0m       name,\n\u001b[0;32m   1582\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1583\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1584\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m   1585\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m   1586\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1587\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1588\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1589\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m   1590\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1591\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m   1592\u001b[0m       custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[0;32m   1593\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1594\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1595\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1322\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1321\u001b[0m   dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mreturn\u001b[39;00m var_store\u001b[39m.\u001b[39;49mget_variable(\n\u001b[0;32m   1323\u001b[0m     full_name,\n\u001b[0;32m   1324\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1325\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1326\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m   1327\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m   1328\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m   1329\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1330\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1331\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1332\u001b[0m     partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m   1333\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1334\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m   1335\u001b[0m     custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[0;32m   1336\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1337\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1338\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:578\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    576\u001b[0m   \u001b[39mreturn\u001b[39;00m custom_getter(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcustom_getter_kwargs)\n\u001b[0;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 578\u001b[0m   \u001b[39mreturn\u001b[39;00m _true_getter(\n\u001b[0;32m    579\u001b[0m       name,\n\u001b[0;32m    580\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    581\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    582\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    583\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m    584\u001b[0m       reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m    585\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    586\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    587\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    588\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m    589\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    590\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    591\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    592\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    593\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:531\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars:\n\u001b[0;32m    526\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mvariable was found: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0. Perhaps a variable of the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname was already created with partitioning?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[1;32m--> 531\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_variable(\n\u001b[0;32m    532\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    533\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    534\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    535\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    536\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m    537\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m    538\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    539\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    540\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    541\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    542\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    543\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    544\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    545\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:894\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    889\u001b[0m   \u001b[39m# Throw away internal tf entries and only take a few lines. In some\u001b[39;00m\n\u001b[0;32m    890\u001b[0m   \u001b[39m# cases the traceback can be longer (e.g. if someone uses factory\u001b[39;00m\n\u001b[0;32m    891\u001b[0m   \u001b[39m# functions to create variables) so we take more than needed in the\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[39m# default case.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   tb \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tb \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtensorflow/python\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m x[\u001b[39m0\u001b[39m]][:\u001b[39m5\u001b[39m]\n\u001b[1;32m--> 894\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Originally defined at:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    895\u001b[0m                    (err_msg, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(traceback\u001b[39m.\u001b[39mformat_list(tb))))\n\u001b[0;32m    896\u001b[0m found_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars[name]\n\u001b[0;32m    897\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m shape\u001b[39m.\u001b[39mis_compatible_with(found_var\u001b[39m.\u001b[39mget_shape()):\n",
      "\u001b[1;31mValueError\u001b[0m: Variable embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_internal\n    ret = Operation(\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 74, in variable_op_v2\n    return gen_state_ops.variable_v2(\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_RNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e12ac12a945f382e98440b0c94bbcb50fa5b9249500f0365ebc1c5d407b008b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
