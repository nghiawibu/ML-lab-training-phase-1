{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Hierarchical Attentional Networks for Document Classification\n",
    "# http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk import tokenize\n",
    "\n",
    "maxlen = 100\n",
    "max_sentences = 15\n",
    "max_words = 20000\n",
    "embedding_dim = 100\n",
    "validation_split = 0.2\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "glove_dir = \"./glove.6B\"\n",
    "embeddings_index = {}\n",
    "\n",
    "\n",
    "# class defining the custom attention layer\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "\n",
    "def remove_html(str_a):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', str_a)\n",
    "\n",
    "\n",
    "# replace all non-ASCII (\\x00-\\x7F) characters with a space\n",
    "def replace_non_ascii(str_a):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', str_a)\n",
    "\n",
    "\n",
    "# Tokenization/string cleaning for dataset\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "input_data = pd.read_csv('../datasets/labeledTrainData.tsv', sep='\\t')\n",
    "\n",
    "for idx in range(input_data.review.shape[0]):\n",
    "    text = BeautifulSoup(input_data.review[idx], features=\"html5lib\")\n",
    "    text = clean_str(text.get_text().encode('ascii', 'ignore'))\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    labels.append(input_data.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), max_sentences, maxlen), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < maxlen and tokenizer.word_index[word] < max_words:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of reviews (data) tensor:', data.shape)\n",
    "print('Shape of sentiment (label) tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in training and validation set')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))\n",
    "\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                            input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "lstm_word = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "attn_word = HierarchicalAttentionNetwork(100)(lstm_word)\n",
    "sentenceEncoder = Model(sentence_input, attn_word)\n",
    "\n",
    "review_input = Input(shape=(max_sentences, maxlen), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentenceEncoder)(review_input)\n",
    "lstm_sentence = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "attn_sentence = HierarchicalAttentionNetwork(100)(lstm_sentence)\n",
    "preds = Dense(2, activation='softmax')(attn_sentence)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=10, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e12ac12a945f382e98440b0c94bbcb50fa5b9249500f0365ebc1c5d407b008b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
