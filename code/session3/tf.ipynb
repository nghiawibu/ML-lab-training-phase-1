{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._X = tf.compat.v1.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.compat.v1.placeholder(tf.int32, shape=[None, ])\n",
    "\n",
    "        weights_1 = tf.compat.v1.get_variable(name='weights_input_hidden', shape=(self._vocab_size, self._hidden_size), initializer=tf.random_normal_initializer(seed=2018))\n",
    "        biases_1 = tf.compat.v1.get_variable(name='biases_input_hidden', shape=(self._hidden_size), initializer=tf.random_normal_initializer(seed=2018))\n",
    "        weights_2 = tf.compat.v1.get_variable(name='weights_output_hidden', shape=(self._hidden_size, NUM_CLASSES), initializer=tf.random_normal_initializer(seed=2018))\n",
    "        biases_2 = tf.compat.v1.get_variable(name='biases_output_hidden', shape=(NUM_CLASSES), initializer=tf.random_normal_initializer(seed=2018))     \n",
    "\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "\n",
    "        return predicted_labels, loss\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/20news-bydate/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "mlp = MLP(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=50\n",
    ")\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss=loss, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index,value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "            \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):  \n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2018)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_data_reader = DataReader(\n",
    "        data_path='../datasets/20news-bydate/train_tf_idf.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    test_data_reader = DataReader(\n",
    "        data_path='../datasets/20news-bydate/test_tf_idf.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = ','.join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "\n",
    "    with open('../datasets/saved-paras/'+filename, 'w') as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 2.385011672973633\n",
      "step: 2, loss: 0.05340058356523514\n",
      "step: 3, loss: 0.0010517913615331054\n",
      "step: 4, loss: 3.8310623494908214e-05\n",
      "step: 5, loss: 3.0159897050907603e-06\n",
      "step: 6, loss: 3.9100635262911965e-07\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 0.0\n",
      "step: 10, loss: 9.413915634155273\n",
      "step: 11, loss: 21.154146194458008\n",
      "step: 12, loss: 17.496200561523438\n",
      "step: 13, loss: 12.8384428024292\n",
      "step: 14, loss: 8.523136138916016\n",
      "step: 15, loss: 4.672196865081787\n",
      "step: 16, loss: 1.800886631011963\n",
      "step: 17, loss: 0.4612773060798645\n",
      "step: 18, loss: 0.15451422333717346\n",
      "step: 19, loss: 0.09243655949831009\n",
      "step: 20, loss: 0.09362736344337463\n",
      "step: 21, loss: 0.07769663631916046\n",
      "step: 22, loss: 4.246654033660889\n",
      "step: 23, loss: 5.437527656555176\n",
      "step: 24, loss: 4.275923252105713\n",
      "step: 25, loss: 3.7616355419158936\n",
      "step: 26, loss: 2.8618884086608887\n",
      "step: 27, loss: 2.4857499599456787\n",
      "step: 28, loss: 2.1409740447998047\n",
      "step: 29, loss: 1.992520809173584\n",
      "step: 30, loss: 1.6402075290679932\n",
      "step: 31, loss: 1.4555199146270752\n",
      "step: 32, loss: 1.297157883644104\n",
      "step: 33, loss: 1.548988699913025\n",
      "step: 34, loss: 2.998657464981079\n",
      "step: 35, loss: 3.037388563156128\n",
      "step: 36, loss: 2.9795281887054443\n",
      "step: 37, loss: 2.9087722301483154\n",
      "step: 38, loss: 2.842686653137207\n",
      "step: 39, loss: 2.682054042816162\n",
      "step: 40, loss: 2.499103546142578\n",
      "step: 41, loss: 2.3443024158477783\n",
      "step: 42, loss: 2.2534306049346924\n",
      "step: 43, loss: 2.099309206008911\n",
      "step: 44, loss: 1.8981622457504272\n",
      "step: 45, loss: 2.031710624694824\n",
      "step: 46, loss: 4.3914408683776855\n",
      "step: 47, loss: 4.278098106384277\n",
      "step: 48, loss: 4.237406253814697\n",
      "step: 49, loss: 4.1156415939331055\n",
      "step: 50, loss: 3.959256649017334\n",
      "step: 51, loss: 3.801194429397583\n",
      "step: 52, loss: 3.6149072647094727\n",
      "step: 53, loss: 3.4492099285125732\n",
      "step: 54, loss: 3.272714138031006\n",
      "step: 55, loss: 3.1153008937835693\n",
      "step: 56, loss: 2.931488037109375\n",
      "step: 57, loss: 3.778547763824463\n",
      "step: 58, loss: 4.6034016609191895\n",
      "step: 59, loss: 4.480742931365967\n",
      "step: 60, loss: 4.3568034172058105\n",
      "step: 61, loss: 4.1824798583984375\n",
      "step: 62, loss: 4.000524997711182\n",
      "step: 63, loss: 3.8525562286376953\n",
      "step: 64, loss: 3.653031587600708\n",
      "step: 65, loss: 3.4527997970581055\n",
      "step: 66, loss: 3.2760627269744873\n",
      "step: 67, loss: 3.0957257747650146\n",
      "step: 68, loss: 2.886643886566162\n",
      "step: 69, loss: 4.426121234893799\n",
      "step: 70, loss: 5.1440205574035645\n",
      "step: 71, loss: 5.044002056121826\n",
      "step: 72, loss: 4.8865647315979\n",
      "step: 73, loss: 4.714115142822266\n",
      "step: 74, loss: 4.545037746429443\n",
      "step: 75, loss: 4.369809627532959\n",
      "step: 76, loss: 4.169715404510498\n",
      "step: 77, loss: 3.973287343978882\n",
      "step: 78, loss: 3.788975715637207\n",
      "step: 79, loss: 3.583043098449707\n",
      "step: 80, loss: 3.4127745628356934\n",
      "step: 81, loss: 5.170485973358154\n",
      "step: 82, loss: 5.109210014343262\n",
      "step: 83, loss: 4.960005283355713\n",
      "step: 84, loss: 4.812299728393555\n",
      "step: 85, loss: 4.636660575866699\n",
      "step: 86, loss: 4.450804233551025\n",
      "step: 87, loss: 4.261071681976318\n",
      "step: 88, loss: 4.054254531860352\n",
      "step: 89, loss: 3.8518738746643066\n",
      "step: 90, loss: 3.6376383304595947\n",
      "step: 91, loss: 3.4267468452453613\n",
      "step: 92, loss: 3.4388234615325928\n",
      "step: 93, loss: 5.250669002532959\n",
      "step: 94, loss: 5.125838756561279\n",
      "step: 95, loss: 4.9809393882751465\n",
      "step: 96, loss: 4.813884258270264\n",
      "step: 97, loss: 4.633574962615967\n",
      "step: 98, loss: 4.438106536865234\n",
      "step: 99, loss: 4.240173816680908\n",
      "step: 100, loss: 4.032166004180908\n",
      "step: 101, loss: 3.8180267810821533\n",
      "step: 102, loss: 3.601248264312744\n",
      "step: 103, loss: 3.3871147632598877\n",
      "step: 104, loss: 3.51438045501709\n",
      "step: 105, loss: 5.5279340744018555\n",
      "step: 106, loss: 5.408126354217529\n",
      "step: 107, loss: 5.258626937866211\n",
      "step: 108, loss: 5.100799560546875\n",
      "step: 109, loss: 4.887925624847412\n",
      "step: 110, loss: 4.702889442443848\n",
      "step: 111, loss: 4.478992462158203\n",
      "step: 112, loss: 4.272588729858398\n",
      "step: 113, loss: 4.047524929046631\n",
      "step: 114, loss: 3.816657781600952\n",
      "step: 115, loss: 3.591168165206909\n",
      "step: 116, loss: 2.6901350021362305\n",
      "step: 117, loss: 0.0\n",
      "step: 118, loss: 0.0\n",
      "step: 119, loss: 0.0\n",
      "step: 120, loss: 0.0\n",
      "step: 121, loss: 0.0\n",
      "step: 122, loss: 0.0\n",
      "step: 123, loss: 0.0\n",
      "step: 124, loss: 0.0\n",
      "step: 125, loss: 0.0\n",
      "step: 126, loss: 0.0\n",
      "step: 127, loss: 0.0\n",
      "step: 128, loss: 0.0\n",
      "step: 129, loss: 0.0\n",
      "step: 130, loss: 0.0\n",
      "step: 131, loss: 0.0\n",
      "step: 132, loss: 0.0\n",
      "step: 133, loss: 0.0\n",
      "step: 134, loss: 0.0\n",
      "step: 135, loss: 0.0\n",
      "step: 136, loss: 0.0\n",
      "step: 137, loss: 0.0\n",
      "step: 138, loss: 0.0\n",
      "step: 139, loss: 0.0\n",
      "step: 140, loss: 0.0\n",
      "step: 141, loss: 0.0\n",
      "step: 142, loss: 0.0\n",
      "step: 143, loss: 0.0\n",
      "step: 144, loss: 0.0\n",
      "step: 145, loss: 0.0\n",
      "step: 146, loss: 0.0\n",
      "step: 147, loss: 0.0\n",
      "step: 148, loss: 0.0\n",
      "step: 149, loss: 0.0\n",
      "step: 150, loss: 0.0\n",
      "step: 151, loss: 0.0\n",
      "step: 152, loss: 0.0\n",
      "step: 153, loss: 0.0\n",
      "step: 154, loss: 0.0\n",
      "step: 155, loss: 0.0\n",
      "step: 156, loss: 0.0\n",
      "step: 157, loss: 0.0\n",
      "step: 158, loss: 0.0\n",
      "step: 159, loss: 0.0\n",
      "step: 160, loss: 0.0\n",
      "step: 161, loss: 0.0\n",
      "step: 162, loss: 0.0\n",
      "step: 163, loss: 0.0\n",
      "step: 164, loss: 0.0\n",
      "step: 165, loss: 0.0\n",
      "step: 166, loss: 0.0\n",
      "step: 167, loss: 0.0\n",
      "step: 168, loss: 0.0\n",
      "step: 169, loss: 0.0\n",
      "step: 170, loss: 0.0\n",
      "step: 171, loss: 0.0\n",
      "step: 172, loss: 0.0\n",
      "step: 173, loss: 0.0\n",
      "step: 174, loss: 0.0\n",
      "step: 175, loss: 0.0\n",
      "step: 176, loss: 0.0\n",
      "step: 177, loss: 0.0\n",
      "step: 178, loss: 0.0\n",
      "step: 179, loss: 0.0\n",
      "step: 180, loss: 0.0\n",
      "step: 181, loss: 0.0\n",
      "step: 182, loss: 0.0\n",
      "step: 183, loss: 0.0\n",
      "step: 184, loss: 0.0\n",
      "step: 185, loss: 0.0\n",
      "step: 186, loss: 0.0\n",
      "step: 187, loss: 0.0\n",
      "step: 188, loss: 0.0\n",
      "step: 189, loss: 0.0\n",
      "step: 190, loss: 0.0\n",
      "step: 191, loss: 0.0\n",
      "step: 192, loss: 0.0\n",
      "step: 193, loss: 0.0\n",
      "step: 194, loss: 0.0\n",
      "step: 195, loss: 0.0\n",
      "step: 196, loss: 0.0\n",
      "step: 197, loss: 0.0\n",
      "step: 198, loss: 0.0\n",
      "step: 199, loss: 0.0\n",
      "step: 200, loss: 0.0\n",
      "step: 201, loss: 0.0\n",
      "step: 202, loss: 0.0\n",
      "step: 203, loss: 0.0\n",
      "step: 204, loss: 0.0\n",
      "step: 205, loss: 0.0\n",
      "step: 206, loss: 0.0\n",
      "step: 207, loss: 0.0\n",
      "step: 208, loss: 0.0\n",
      "step: 209, loss: 0.0\n",
      "step: 210, loss: 0.0\n",
      "step: 211, loss: 0.0\n",
      "step: 212, loss: 0.0\n",
      "step: 213, loss: 0.0\n",
      "step: 214, loss: 0.0\n",
      "step: 215, loss: 0.0\n",
      "step: 216, loss: 0.0\n",
      "step: 217, loss: 0.0\n",
      "step: 218, loss: 0.0\n",
      "step: 219, loss: 0.0\n",
      "step: 220, loss: 0.0\n",
      "step: 221, loss: 0.0\n",
      "step: 222, loss: 0.0\n",
      "step: 223, loss: 0.0\n",
      "step: 224, loss: 0.0\n",
      "step: 225, loss: 0.0\n",
      "step: 226, loss: 1.3718812465667725\n",
      "step: 227, loss: 1.418242335319519\n",
      "step: 228, loss: 1.3933895826339722\n",
      "step: 229, loss: 1.0796284675598145\n",
      "step: 230, loss: 1.1173558235168457\n",
      "step: 231, loss: 1.038913607597351\n",
      "step: 232, loss: 1.2648718357086182\n",
      "step: 233, loss: 1.1524852514266968\n",
      "step: 234, loss: 0.9287754893302917\n",
      "step: 235, loss: 0.9703854322433472\n",
      "step: 236, loss: 1.3146761655807495\n",
      "step: 237, loss: 1.0902247428894043\n",
      "step: 238, loss: 1.0856733322143555\n",
      "step: 239, loss: 1.2058029174804688\n",
      "step: 240, loss: 1.1146453619003296\n",
      "step: 241, loss: 1.3329401016235352\n",
      "step: 242, loss: 1.0646297931671143\n",
      "step: 243, loss: 1.140548825263977\n",
      "step: 244, loss: 1.1309014558792114\n",
      "step: 245, loss: 1.1609448194503784\n",
      "step: 246, loss: 1.022217869758606\n",
      "step: 247, loss: 0.9511862397193909\n",
      "step: 248, loss: 0.8899115920066833\n",
      "step: 249, loss: 1.2154607772827148\n",
      "step: 250, loss: 1.227372646331787\n",
      "step: 251, loss: 0.9423307776451111\n",
      "step: 252, loss: 1.2534109354019165\n",
      "step: 253, loss: 1.049949049949646\n",
      "step: 254, loss: 0.9070363640785217\n",
      "step: 255, loss: 0.9799284934997559\n",
      "step: 256, loss: 1.0316636562347412\n",
      "step: 257, loss: 1.008062720298767\n",
      "step: 258, loss: 1.0449345111846924\n",
      "step: 259, loss: 1.0860254764556885\n",
      "step: 260, loss: 1.170782446861267\n",
      "step: 261, loss: 1.2197445631027222\n",
      "step: 262, loss: 1.2303000688552856\n",
      "step: 263, loss: 1.0472749471664429\n",
      "step: 264, loss: 1.1032520532608032\n",
      "step: 265, loss: 1.0753638744354248\n",
      "step: 266, loss: 1.059798240661621\n",
      "step: 267, loss: 0.8904263377189636\n",
      "step: 268, loss: 0.7441990375518799\n",
      "step: 269, loss: 0.9068489074707031\n",
      "step: 270, loss: 1.150935173034668\n",
      "step: 271, loss: 1.2964346408843994\n",
      "step: 272, loss: 1.0635184049606323\n",
      "step: 273, loss: 1.0500805377960205\n",
      "step: 274, loss: 1.0427637100219727\n",
      "step: 275, loss: 0.7590980529785156\n",
      "step: 276, loss: 1.1296545267105103\n",
      "step: 277, loss: 1.4897353649139404\n",
      "step: 278, loss: 1.1643458604812622\n",
      "step: 279, loss: 0.9559578895568848\n",
      "step: 280, loss: 1.3435593843460083\n",
      "step: 281, loss: 0.7027453780174255\n",
      "step: 282, loss: 1.4948630332946777\n",
      "step: 283, loss: 0.6630133986473083\n",
      "step: 284, loss: 1.042752981185913\n",
      "step: 285, loss: 1.0020108222961426\n",
      "step: 286, loss: 1.1575487852096558\n",
      "step: 287, loss: 1.1470125913619995\n",
      "step: 288, loss: 1.3785494565963745\n",
      "step: 289, loss: 0.9749572277069092\n",
      "step: 290, loss: 1.035506010055542\n",
      "step: 291, loss: 1.5223057270050049\n",
      "step: 292, loss: 1.0988037586212158\n",
      "step: 293, loss: 1.1628623008728027\n",
      "step: 294, loss: 1.2035458087921143\n",
      "step: 295, loss: 1.1427403688430786\n",
      "step: 296, loss: 1.3433269262313843\n",
      "step: 297, loss: 0.8963916301727295\n",
      "step: 298, loss: 0.8704376220703125\n",
      "step: 299, loss: 1.1345213651657104\n",
      "step: 300, loss: 0.8966160416603088\n",
      "step: 301, loss: 1.0015537738800049\n",
      "step: 302, loss: 0.7064493298530579\n",
      "step: 303, loss: 0.9443017840385437\n",
      "step: 304, loss: 0.9999915957450867\n",
      "step: 305, loss: 0.9224970936775208\n",
      "step: 306, loss: 0.7703428864479065\n",
      "step: 307, loss: 1.1324478387832642\n",
      "step: 308, loss: 0.8126024007797241\n",
      "step: 309, loss: 0.7187635898590088\n",
      "step: 310, loss: 0.7296903133392334\n",
      "step: 311, loss: 0.507037878036499\n",
      "step: 312, loss: 0.612769603729248\n",
      "step: 313, loss: 1.1967765092849731\n",
      "step: 314, loss: 0.911955714225769\n",
      "step: 315, loss: 1.537988305091858\n",
      "step: 316, loss: 1.1236687898635864\n",
      "step: 317, loss: 0.680314302444458\n",
      "step: 318, loss: 0.9120832681655884\n",
      "step: 319, loss: 0.7444326281547546\n",
      "step: 320, loss: 1.4101399183273315\n",
      "step: 321, loss: 0.7771325707435608\n",
      "step: 322, loss: 0.9323617815971375\n",
      "step: 323, loss: 1.0975914001464844\n",
      "step: 324, loss: 0.8216596841812134\n",
      "step: 325, loss: 1.0589362382888794\n",
      "step: 326, loss: 0.5456655621528625\n",
      "step: 327, loss: 0.8784774541854858\n",
      "step: 328, loss: 0.9089672565460205\n",
      "step: 329, loss: 0.8688937425613403\n",
      "step: 330, loss: 1.0091798305511475\n",
      "step: 331, loss: 0.8947174549102783\n",
      "step: 332, loss: 0.8120272755622864\n",
      "step: 333, loss: 0.7600340843200684\n",
      "step: 334, loss: 0.7197628617286682\n",
      "step: 335, loss: 0.9105424284934998\n",
      "step: 336, loss: 0.9383468627929688\n",
      "step: 337, loss: 0.991279661655426\n",
      "step: 338, loss: 0.7010713219642639\n",
      "step: 339, loss: 0.6013273596763611\n",
      "step: 340, loss: 0.9141225218772888\n",
      "step: 341, loss: 1.0045452117919922\n",
      "step: 342, loss: 0.6155978441238403\n",
      "step: 343, loss: 0.8694851398468018\n",
      "step: 344, loss: 0.9254698753356934\n",
      "step: 345, loss: 0.719007134437561\n",
      "step: 346, loss: 0.7517420053482056\n",
      "step: 347, loss: 0.8677600026130676\n",
      "step: 348, loss: 0.8897716403007507\n",
      "step: 349, loss: 0.9096148014068604\n",
      "step: 350, loss: 0.7763838171958923\n",
      "step: 351, loss: 0.6454936265945435\n",
      "step: 352, loss: 1.0351985692977905\n",
      "step: 353, loss: 0.8984881639480591\n",
      "step: 354, loss: 0.8803024291992188\n",
      "step: 355, loss: 0.800963282585144\n",
      "step: 356, loss: 0.8869808316230774\n",
      "step: 357, loss: 0.7294509410858154\n",
      "step: 358, loss: 0.8266523480415344\n",
      "step: 359, loss: 0.7528932094573975\n",
      "step: 360, loss: 0.6478669047355652\n",
      "step: 361, loss: 1.274491310119629\n",
      "step: 362, loss: 0.8216016888618469\n",
      "step: 363, loss: 0.9779411554336548\n",
      "step: 364, loss: 0.542117178440094\n",
      "step: 365, loss: 0.6236410737037659\n",
      "step: 366, loss: 1.0252599716186523\n",
      "step: 367, loss: 0.8070098161697388\n",
      "step: 368, loss: 0.6473307013511658\n",
      "step: 369, loss: 0.5469201803207397\n",
      "step: 370, loss: 0.7833666801452637\n",
      "step: 371, loss: 0.7046195268630981\n",
      "step: 372, loss: 0.6013290286064148\n",
      "step: 373, loss: 0.6175472736358643\n",
      "step: 374, loss: 0.6855714321136475\n",
      "step: 375, loss: 0.9323016405105591\n",
      "step: 376, loss: 0.4896257817745209\n",
      "step: 377, loss: 0.841159999370575\n",
      "step: 378, loss: 0.7184886336326599\n",
      "step: 379, loss: 0.5464911460876465\n",
      "step: 380, loss: 0.5977550745010376\n",
      "step: 381, loss: 0.6778567433357239\n",
      "step: 382, loss: 0.7937696576118469\n",
      "step: 383, loss: 0.6565586924552917\n",
      "step: 384, loss: 0.4797780215740204\n",
      "step: 385, loss: 1.0676987171173096\n",
      "step: 386, loss: 0.49944567680358887\n",
      "step: 387, loss: 0.6513270735740662\n",
      "step: 388, loss: 0.705528974533081\n",
      "step: 389, loss: 0.5575717687606812\n",
      "step: 390, loss: 0.7385132312774658\n",
      "step: 391, loss: 0.7502716183662415\n",
      "step: 392, loss: 0.7066588401794434\n",
      "step: 393, loss: 0.6632874011993408\n",
      "step: 394, loss: 0.5388287901878357\n",
      "step: 395, loss: 0.5046455264091492\n",
      "step: 396, loss: 0.8251048922538757\n",
      "step: 397, loss: 0.9342966675758362\n",
      "step: 398, loss: 0.4616285562515259\n",
      "step: 399, loss: 0.5844570994377136\n",
      "step: 400, loss: 0.45222437381744385\n",
      "step: 401, loss: 0.2786692678928375\n",
      "step: 402, loss: 0.5629105567932129\n",
      "step: 403, loss: 0.5366383194923401\n",
      "step: 404, loss: 0.7914984226226807\n",
      "step: 405, loss: 0.5552079677581787\n",
      "step: 406, loss: 0.6191183924674988\n",
      "step: 407, loss: 0.45504388213157654\n",
      "step: 408, loss: 0.7669233679771423\n",
      "step: 409, loss: 0.9878343343734741\n",
      "step: 410, loss: 0.5529242753982544\n",
      "step: 411, loss: 0.4533417522907257\n",
      "step: 412, loss: 1.0393651723861694\n",
      "step: 413, loss: 0.631089985370636\n",
      "step: 414, loss: 0.7714113593101501\n",
      "step: 415, loss: 0.6281713247299194\n",
      "step: 416, loss: 1.3120602369308472\n",
      "step: 417, loss: 0.6014173030853271\n",
      "step: 418, loss: 0.5999730229377747\n",
      "step: 419, loss: 0.5416327118873596\n",
      "step: 420, loss: 0.5997080206871033\n",
      "step: 421, loss: 0.45342838764190674\n",
      "step: 422, loss: 0.5247280597686768\n",
      "step: 423, loss: 0.4306572675704956\n",
      "step: 424, loss: 0.4726005792617798\n",
      "step: 425, loss: 0.529543399810791\n",
      "step: 426, loss: 0.8417777419090271\n",
      "step: 427, loss: 0.4413396120071411\n",
      "step: 428, loss: 0.7132327556610107\n",
      "step: 429, loss: 0.6141276359558105\n",
      "step: 430, loss: 0.6293388605117798\n",
      "step: 431, loss: 0.46230706572532654\n",
      "step: 432, loss: 0.5537794232368469\n",
      "step: 433, loss: 0.561536967754364\n",
      "step: 434, loss: 0.9792342185974121\n",
      "step: 435, loss: 0.5648906230926514\n",
      "step: 436, loss: 0.5769428014755249\n",
      "step: 437, loss: 0.887147068977356\n",
      "step: 438, loss: 0.7511765360832214\n",
      "step: 439, loss: 0.6133621335029602\n",
      "step: 440, loss: 0.6776666045188904\n",
      "step: 441, loss: 0.6023515462875366\n",
      "step: 442, loss: 0.4004720449447632\n",
      "step: 443, loss: 0.5706951022148132\n",
      "step: 444, loss: 0.7335938811302185\n",
      "step: 445, loss: 0.715969443321228\n",
      "step: 446, loss: 0.5311019420623779\n",
      "step: 447, loss: 0.5202912092208862\n",
      "step: 448, loss: 0.4993128478527069\n",
      "step: 449, loss: 0.48188409209251404\n",
      "step: 450, loss: 1.001418113708496\n",
      "step: 451, loss: 0.7291175723075867\n",
      "step: 452, loss: 0.38076573610305786\n",
      "step: 453, loss: 0.34945082664489746\n",
      "step: 454, loss: 0.29026517271995544\n",
      "step: 455, loss: 0.3307934105396271\n",
      "step: 456, loss: 0.27729013562202454\n",
      "step: 457, loss: 0.31433171033859253\n",
      "step: 458, loss: 0.4036519527435303\n",
      "step: 459, loss: 0.46519166231155396\n",
      "step: 460, loss: 0.4740558862686157\n",
      "step: 461, loss: 0.32961583137512207\n",
      "step: 462, loss: 0.3524206578731537\n",
      "step: 463, loss: 0.5075175166130066\n",
      "step: 464, loss: 0.46569398045539856\n",
      "step: 465, loss: 0.3777851164340973\n",
      "step: 466, loss: 0.29597532749176025\n",
      "step: 467, loss: 0.40524619817733765\n",
      "step: 468, loss: 0.3074433505535126\n",
      "step: 469, loss: 0.5156957507133484\n",
      "step: 470, loss: 0.34235599637031555\n",
      "step: 471, loss: 0.4247235357761383\n",
      "step: 472, loss: 0.2599879503250122\n",
      "step: 473, loss: 0.30884236097335815\n",
      "step: 474, loss: 0.3643227517604828\n",
      "step: 475, loss: 0.4933220148086548\n",
      "step: 476, loss: 0.3954154849052429\n",
      "step: 477, loss: 0.47432610392570496\n",
      "step: 478, loss: 0.5289924740791321\n",
      "step: 479, loss: 0.49747398495674133\n",
      "step: 480, loss: 0.2832607924938202\n",
      "step: 481, loss: 0.25692862272262573\n",
      "step: 482, loss: 0.2959382236003876\n",
      "step: 483, loss: 0.19010348618030548\n",
      "step: 484, loss: 0.2409219592809677\n",
      "step: 485, loss: 1.0277396440505981\n",
      "step: 486, loss: 0.47173988819122314\n",
      "step: 487, loss: 0.13564029335975647\n",
      "step: 488, loss: 0.317968487739563\n",
      "step: 489, loss: 0.5326955318450928\n",
      "step: 490, loss: 0.29775431752204895\n",
      "step: 491, loss: 0.2973173260688782\n",
      "step: 492, loss: 0.33843764662742615\n",
      "step: 493, loss: 0.46201252937316895\n",
      "step: 494, loss: 0.46392449736595154\n",
      "step: 495, loss: 0.2625894844532013\n",
      "step: 496, loss: 0.4326208531856537\n",
      "step: 497, loss: 0.34057989716529846\n",
      "step: 498, loss: 0.8197523355484009\n",
      "step: 499, loss: 0.46422672271728516\n",
      "step: 500, loss: 0.3661685585975647\n",
      "step: 501, loss: 0.14117716252803802\n",
      "step: 502, loss: 0.3528198301792145\n",
      "step: 503, loss: 0.40350142121315\n",
      "step: 504, loss: 0.4536379873752594\n",
      "step: 505, loss: 0.2561356723308563\n",
      "step: 506, loss: 0.3146708309650421\n",
      "step: 507, loss: 0.20332996547222137\n",
      "step: 508, loss: 0.21608877182006836\n",
      "step: 509, loss: 0.22685843706130981\n",
      "step: 510, loss: 0.3174324333667755\n",
      "step: 511, loss: 0.3165823817253113\n",
      "step: 512, loss: 0.39799290895462036\n",
      "step: 513, loss: 0.3339308798313141\n",
      "step: 514, loss: 0.08246656507253647\n",
      "step: 515, loss: 0.4786606729030609\n",
      "step: 516, loss: 0.3161955177783966\n",
      "step: 517, loss: 0.3144673705101013\n",
      "step: 518, loss: 0.3909298777580261\n",
      "step: 519, loss: 0.5314750075340271\n",
      "step: 520, loss: 0.1992795616388321\n",
      "step: 521, loss: 0.3180200457572937\n",
      "step: 522, loss: 0.2890118360519409\n",
      "step: 523, loss: 0.3762214183807373\n",
      "step: 524, loss: 0.40628230571746826\n",
      "step: 525, loss: 0.26743537187576294\n",
      "step: 526, loss: 0.3287874460220337\n",
      "step: 527, loss: 0.4057590961456299\n",
      "step: 528, loss: 0.44223305583000183\n",
      "step: 529, loss: 0.2998313903808594\n",
      "step: 530, loss: 0.2031058669090271\n",
      "step: 531, loss: 0.9555832743644714\n",
      "step: 532, loss: 0.3188331127166748\n",
      "step: 533, loss: 0.44891583919525146\n",
      "step: 534, loss: 0.22808341681957245\n",
      "step: 535, loss: 0.2644788324832916\n",
      "step: 536, loss: 0.1489126980304718\n",
      "step: 537, loss: 0.4758242070674896\n",
      "step: 538, loss: 0.2694210112094879\n",
      "step: 539, loss: 0.21250705420970917\n",
      "step: 540, loss: 0.35952597856521606\n",
      "step: 541, loss: 0.45673903822898865\n",
      "step: 542, loss: 0.2724394202232361\n",
      "step: 543, loss: 0.42628714442253113\n",
      "step: 544, loss: 0.415739506483078\n",
      "step: 545, loss: 0.443267285823822\n",
      "step: 546, loss: 0.3659341335296631\n",
      "step: 547, loss: 0.5379946231842041\n",
      "step: 548, loss: 0.5023812055587769\n",
      "step: 549, loss: 0.2952137887477875\n",
      "step: 550, loss: 0.22493401169776917\n",
      "step: 551, loss: 0.5825302600860596\n",
      "step: 552, loss: 0.35663414001464844\n",
      "step: 553, loss: 0.31891483068466187\n",
      "step: 554, loss: 0.636624813079834\n",
      "step: 555, loss: 0.5704564452171326\n",
      "step: 556, loss: 0.08307163417339325\n",
      "step: 557, loss: 0.4346384108066559\n",
      "step: 558, loss: 0.30488529801368713\n",
      "step: 559, loss: 0.227491095662117\n",
      "step: 560, loss: 0.6792035102844238\n",
      "step: 561, loss: 0.5259816646575928\n",
      "step: 562, loss: 0.2173750251531601\n",
      "step: 563, loss: 0.4507576823234558\n",
      "step: 564, loss: 0.4257742166519165\n",
      "step: 565, loss: 0.2273894101381302\n",
      "step: 566, loss: 0.4793030619621277\n",
      "step: 567, loss: 0.4081142842769623\n",
      "step: 568, loss: 0.49726563692092896\n",
      "step: 569, loss: 0.2207799106836319\n",
      "step: 570, loss: 0.22404968738555908\n",
      "step: 571, loss: 0.288746178150177\n",
      "step: 572, loss: 0.35192498564720154\n",
      "step: 573, loss: 0.3245108425617218\n",
      "step: 574, loss: 0.18105344474315643\n",
      "step: 575, loss: 0.33440613746643066\n",
      "step: 576, loss: 0.4242810308933258\n",
      "step: 577, loss: 0.49980881810188293\n",
      "step: 578, loss: 0.24270859360694885\n",
      "step: 579, loss: 0.3372248113155365\n",
      "step: 580, loss: 0.369834303855896\n",
      "step: 581, loss: 0.45395219326019287\n",
      "step: 582, loss: 0.2544044256210327\n",
      "step: 583, loss: 0.2667233645915985\n",
      "step: 584, loss: 0.24947446584701538\n",
      "step: 585, loss: 0.4578041136264801\n",
      "step: 586, loss: 0.3328050374984741\n",
      "step: 587, loss: 0.18914659321308136\n",
      "step: 588, loss: 0.1671324223279953\n",
      "step: 589, loss: 0.8916236758232117\n",
      "step: 590, loss: 0.14054878056049347\n",
      "step: 591, loss: 0.35279959440231323\n",
      "step: 592, loss: 0.4776661992073059\n",
      "step: 593, loss: 0.28154709935188293\n",
      "step: 594, loss: 0.2667401432991028\n",
      "step: 595, loss: 0.2594088613986969\n",
      "step: 596, loss: 0.14528857171535492\n",
      "step: 597, loss: 0.2329346090555191\n",
      "step: 598, loss: 0.19133424758911133\n",
      "step: 599, loss: 0.33661773800849915\n",
      "step: 600, loss: 0.30839282274246216\n",
      "step: 601, loss: 0.33061572909355164\n",
      "step: 602, loss: 0.2786276340484619\n",
      "step: 603, loss: 0.421250581741333\n",
      "step: 604, loss: 0.42310118675231934\n",
      "step: 605, loss: 0.3543449342250824\n",
      "step: 606, loss: 0.051080312579870224\n",
      "step: 607, loss: 0.30162209272384644\n",
      "step: 608, loss: 0.3182389438152313\n",
      "step: 609, loss: 0.19850702583789825\n",
      "step: 610, loss: 0.23575012385845184\n",
      "step: 611, loss: 0.2667097747325897\n",
      "step: 612, loss: 0.18968459963798523\n",
      "step: 613, loss: 0.3701525926589966\n",
      "step: 614, loss: 0.3227168917655945\n",
      "step: 615, loss: 0.6413736939430237\n",
      "step: 616, loss: 0.34819769859313965\n",
      "step: 617, loss: 0.6017606258392334\n",
      "step: 618, loss: 0.2004239410161972\n",
      "step: 619, loss: 0.22014153003692627\n",
      "step: 620, loss: 0.3633880317211151\n",
      "step: 621, loss: 0.42319729924201965\n",
      "step: 622, loss: 0.3236828148365021\n",
      "step: 623, loss: 0.28521281480789185\n",
      "step: 624, loss: 0.5810157656669617\n",
      "step: 625, loss: 0.5551016330718994\n",
      "step: 626, loss: 0.6246059536933899\n",
      "step: 627, loss: 0.9244470000267029\n",
      "step: 628, loss: 0.2428084909915924\n",
      "step: 629, loss: 0.43555665016174316\n",
      "step: 630, loss: 0.299970418214798\n",
      "step: 631, loss: 0.3817901611328125\n",
      "step: 632, loss: 0.3368348777294159\n",
      "step: 633, loss: 0.4701908230781555\n",
      "step: 634, loss: 0.15962938964366913\n",
      "step: 635, loss: 0.40161094069480896\n",
      "step: 636, loss: 0.23249240219593048\n",
      "step: 637, loss: 0.26774969696998596\n",
      "step: 638, loss: 0.5443042516708374\n",
      "step: 639, loss: 0.18165311217308044\n",
      "step: 640, loss: 0.5664572715759277\n",
      "step: 641, loss: 0.12461450695991516\n",
      "step: 642, loss: 0.3884894847869873\n",
      "step: 643, loss: 0.3005070984363556\n",
      "step: 644, loss: 0.347505122423172\n",
      "step: 645, loss: 0.4452274441719055\n",
      "step: 646, loss: 0.08584678918123245\n",
      "step: 647, loss: 0.2995758652687073\n",
      "step: 648, loss: 0.2954568862915039\n",
      "step: 649, loss: 0.29396969079971313\n",
      "step: 650, loss: 0.638526976108551\n",
      "step: 651, loss: 0.3808039426803589\n",
      "step: 652, loss: 0.21082571148872375\n",
      "step: 653, loss: 0.2313261479139328\n",
      "step: 654, loss: 0.44185248017311096\n",
      "step: 655, loss: 0.19106654822826385\n",
      "step: 656, loss: 0.4406318664550781\n",
      "step: 657, loss: 0.580302357673645\n",
      "step: 658, loss: 0.3837476968765259\n",
      "step: 659, loss: 0.50590980052948\n",
      "step: 660, loss: 0.37822794914245605\n",
      "step: 661, loss: 0.5453788638114929\n",
      "step: 662, loss: 0.1992560178041458\n",
      "step: 663, loss: 0.595950722694397\n",
      "step: 664, loss: 0.332927405834198\n",
      "step: 665, loss: 0.5382683277130127\n",
      "step: 666, loss: 0.31737565994262695\n",
      "step: 667, loss: 0.43240538239479065\n",
      "step: 668, loss: 0.23691503703594208\n",
      "step: 669, loss: 0.32559487223625183\n",
      "step: 670, loss: 0.423629492521286\n",
      "step: 671, loss: 0.24912621080875397\n",
      "step: 672, loss: 0.35715875029563904\n",
      "step: 673, loss: 0.4771873950958252\n",
      "step: 674, loss: 0.4736256003379822\n",
      "step: 675, loss: 0.2823723554611206\n",
      "step: 676, loss: 0.24811705946922302\n",
      "step: 677, loss: 0.3450821042060852\n",
      "step: 678, loss: 0.2258628010749817\n",
      "step: 679, loss: 0.3504886329174042\n",
      "step: 680, loss: 0.24537916481494904\n",
      "step: 681, loss: 0.3265424072742462\n",
      "step: 682, loss: 0.22084194421768188\n",
      "step: 683, loss: 0.16573329269886017\n",
      "step: 684, loss: 0.09808024764060974\n",
      "step: 685, loss: 0.2740391194820404\n",
      "step: 686, loss: 0.26787030696868896\n",
      "step: 687, loss: 0.20541368424892426\n",
      "step: 688, loss: 0.21965910494327545\n",
      "step: 689, loss: 0.1507474184036255\n",
      "step: 690, loss: 0.15975886583328247\n",
      "step: 691, loss: 0.13251173496246338\n",
      "step: 692, loss: 0.09404373168945312\n",
      "step: 693, loss: 0.27527937293052673\n",
      "step: 694, loss: 0.14070211350917816\n",
      "step: 695, loss: 0.2193111777305603\n",
      "step: 696, loss: 0.27976879477500916\n",
      "step: 697, loss: 0.18174763023853302\n",
      "step: 698, loss: 0.07883433252573013\n",
      "step: 699, loss: 0.10783734172582626\n",
      "step: 700, loss: 0.1637686789035797\n",
      "step: 701, loss: 0.2949938476085663\n",
      "step: 702, loss: 0.2813498377799988\n",
      "step: 703, loss: 0.23616722226142883\n",
      "step: 704, loss: 0.2060243934392929\n",
      "step: 705, loss: 0.30355584621429443\n",
      "step: 706, loss: 0.3921963572502136\n",
      "step: 707, loss: 0.29392558336257935\n",
      "step: 708, loss: 0.20533102750778198\n",
      "step: 709, loss: 0.19812004268169403\n",
      "step: 710, loss: 0.26333627104759216\n",
      "step: 711, loss: 0.2202203869819641\n",
      "step: 712, loss: 0.23071876168251038\n",
      "step: 713, loss: 0.0854978859424591\n",
      "step: 714, loss: 0.2904900312423706\n",
      "step: 715, loss: 0.11692558974027634\n",
      "step: 716, loss: 0.13448622822761536\n",
      "step: 717, loss: 0.14337831735610962\n",
      "step: 718, loss: 0.1574203073978424\n",
      "step: 719, loss: 0.3816543519496918\n",
      "step: 720, loss: 0.10053632408380508\n",
      "step: 721, loss: 0.21456500887870789\n",
      "step: 722, loss: 0.1628137230873108\n",
      "step: 723, loss: 0.3484684228897095\n",
      "step: 724, loss: 0.286065012216568\n",
      "step: 725, loss: 0.11774442344903946\n",
      "step: 726, loss: 0.1704534888267517\n",
      "step: 727, loss: 0.0713924691081047\n",
      "step: 728, loss: 0.1866762489080429\n",
      "step: 729, loss: 0.20523539185523987\n",
      "step: 730, loss: 0.09023144841194153\n",
      "step: 731, loss: 0.5797116756439209\n",
      "step: 732, loss: 0.0886305645108223\n",
      "step: 733, loss: 0.160847008228302\n",
      "step: 734, loss: 0.22054153680801392\n",
      "step: 735, loss: 0.16060782968997955\n",
      "step: 736, loss: 0.27242153882980347\n",
      "step: 737, loss: 0.13543280959129333\n",
      "step: 738, loss: 0.27599602937698364\n",
      "step: 739, loss: 0.028241010382771492\n",
      "step: 740, loss: 0.14709587395191193\n",
      "step: 741, loss: 0.15887561440467834\n",
      "step: 742, loss: 0.1064520999789238\n",
      "step: 743, loss: 0.16840197145938873\n",
      "step: 744, loss: 0.1706773340702057\n",
      "step: 745, loss: 0.12936879694461823\n",
      "step: 746, loss: 0.20570094883441925\n",
      "step: 747, loss: 0.1538694202899933\n",
      "step: 748, loss: 0.20868580043315887\n",
      "step: 749, loss: 0.18012076616287231\n",
      "step: 750, loss: 0.14597713947296143\n",
      "step: 751, loss: 0.19051429629325867\n",
      "step: 752, loss: 0.09046950936317444\n",
      "step: 753, loss: 0.3357699513435364\n",
      "step: 754, loss: 0.32800713181495667\n",
      "step: 755, loss: 0.15501007437705994\n",
      "step: 756, loss: 0.20388247072696686\n",
      "step: 757, loss: 0.3825109004974365\n",
      "step: 758, loss: 0.14683398604393005\n",
      "step: 759, loss: 0.0485709011554718\n",
      "step: 760, loss: 0.21928399801254272\n",
      "step: 761, loss: 0.2086210697889328\n",
      "step: 762, loss: 0.043953124433755875\n",
      "step: 763, loss: 0.3104712963104248\n",
      "step: 764, loss: 0.22282561659812927\n",
      "step: 765, loss: 0.12590600550174713\n",
      "step: 766, loss: 0.13481071591377258\n",
      "step: 767, loss: 0.26730942726135254\n",
      "step: 768, loss: 0.16488690674304962\n",
      "step: 769, loss: 0.16279259324073792\n",
      "step: 770, loss: 0.261081337928772\n",
      "step: 771, loss: 0.23216566443443298\n",
      "step: 772, loss: 0.156868577003479\n",
      "step: 773, loss: 0.21847419440746307\n",
      "step: 774, loss: 0.3126559853553772\n",
      "step: 775, loss: 0.08025985956192017\n",
      "step: 776, loss: 0.2195417433977127\n",
      "step: 777, loss: 0.20139403641223907\n",
      "step: 778, loss: 0.24458558857440948\n",
      "step: 779, loss: 0.2474391758441925\n",
      "step: 780, loss: 0.11526253074407578\n",
      "step: 781, loss: 0.169413760304451\n",
      "step: 782, loss: 0.2396068572998047\n",
      "step: 783, loss: 0.2561510503292084\n",
      "step: 784, loss: 0.009690449573099613\n",
      "step: 785, loss: 0.20045119524002075\n",
      "step: 786, loss: 0.2021150439977646\n",
      "step: 787, loss: 0.15781332552433014\n",
      "step: 788, loss: 0.37780845165252686\n",
      "step: 789, loss: 0.3250247538089752\n",
      "step: 790, loss: 0.15990005433559418\n",
      "step: 791, loss: 0.12525440752506256\n",
      "step: 792, loss: 0.2015645056962967\n",
      "step: 793, loss: 0.168531596660614\n",
      "step: 794, loss: 0.3268882632255554\n",
      "step: 795, loss: 0.14351394772529602\n",
      "step: 796, loss: 0.052791886031627655\n",
      "step: 797, loss: 0.35235312581062317\n",
      "step: 798, loss: 0.07593805342912674\n",
      "step: 799, loss: 0.24586144089698792\n",
      "step: 800, loss: 0.2750444710254669\n",
      "step: 801, loss: 0.24026276171207428\n",
      "step: 802, loss: 0.3173210620880127\n",
      "step: 803, loss: 0.17801643908023834\n",
      "step: 804, loss: 0.3112476170063019\n",
      "step: 805, loss: 0.1731971800327301\n",
      "step: 806, loss: 0.17824336886405945\n",
      "step: 807, loss: 0.3916621506214142\n",
      "step: 808, loss: 0.1860491782426834\n",
      "step: 809, loss: 0.3089786469936371\n",
      "step: 810, loss: 0.1450038105249405\n",
      "step: 811, loss: 0.1888694316148758\n",
      "step: 812, loss: 0.14237433671951294\n",
      "step: 813, loss: 0.1902506947517395\n",
      "step: 814, loss: 0.15149399638175964\n",
      "step: 815, loss: 0.3472155034542084\n",
      "step: 816, loss: 0.27745646238327026\n",
      "step: 817, loss: 0.21538367867469788\n",
      "step: 818, loss: 0.33261117339134216\n",
      "step: 819, loss: 0.005869671702384949\n",
      "step: 820, loss: 0.2560428977012634\n",
      "step: 821, loss: 0.11949426680803299\n",
      "step: 822, loss: 0.20838439464569092\n",
      "step: 823, loss: 0.27828139066696167\n",
      "step: 824, loss: 0.18496118485927582\n",
      "step: 825, loss: 0.20807117223739624\n",
      "step: 826, loss: 0.0556965097784996\n",
      "step: 827, loss: 0.13893061876296997\n",
      "step: 828, loss: 0.22178444266319275\n",
      "step: 829, loss: 0.1594875603914261\n",
      "step: 830, loss: 0.1343984305858612\n",
      "step: 831, loss: 0.18248966336250305\n",
      "step: 832, loss: 0.4319411516189575\n",
      "step: 833, loss: 0.17935636639595032\n",
      "step: 834, loss: 0.10770782828330994\n",
      "step: 835, loss: 0.21097223460674286\n",
      "step: 836, loss: 0.12384090572595596\n",
      "step: 837, loss: 0.24949394166469574\n",
      "step: 838, loss: 0.20339061319828033\n",
      "step: 839, loss: 0.16174207627773285\n",
      "step: 840, loss: 0.38001444935798645\n",
      "step: 841, loss: 0.17423532903194427\n",
      "step: 842, loss: 0.011726261116564274\n",
      "step: 843, loss: 0.40550172328948975\n",
      "step: 844, loss: 0.30157414078712463\n",
      "step: 845, loss: 0.326828271150589\n",
      "step: 846, loss: 0.3521251976490021\n",
      "step: 847, loss: 0.0823180079460144\n",
      "step: 848, loss: 0.09685535728931427\n",
      "step: 849, loss: 0.12464239448308945\n",
      "step: 850, loss: 0.16985145211219788\n",
      "step: 851, loss: 0.08239652961492538\n",
      "step: 852, loss: 0.14758558571338654\n",
      "step: 853, loss: 0.2522975206375122\n",
      "step: 854, loss: 0.2765386998653412\n",
      "step: 855, loss: 0.38966304063796997\n",
      "step: 856, loss: 0.21801374852657318\n",
      "step: 857, loss: 0.33526450395584106\n",
      "step: 858, loss: 0.038211219012737274\n",
      "step: 859, loss: 0.18697477877140045\n",
      "step: 860, loss: 0.13549602031707764\n",
      "step: 861, loss: 0.20300020277500153\n",
      "step: 862, loss: 0.12054140120744705\n",
      "step: 863, loss: 0.5001831650733948\n",
      "step: 864, loss: 0.14779090881347656\n",
      "step: 865, loss: 0.2591794729232788\n",
      "step: 866, loss: 0.0629533976316452\n",
      "step: 867, loss: 0.20598743855953217\n",
      "step: 868, loss: 0.3444859981536865\n",
      "step: 869, loss: 0.14797310531139374\n",
      "step: 870, loss: 0.010083857923746109\n",
      "step: 871, loss: 0.10919973254203796\n",
      "step: 872, loss: 0.15865574777126312\n",
      "step: 873, loss: 0.0796988233923912\n",
      "step: 874, loss: 0.10430882126092911\n",
      "step: 875, loss: 0.17982512712478638\n",
      "step: 876, loss: 0.24134889245033264\n",
      "step: 877, loss: 0.10685884207487106\n",
      "step: 878, loss: 0.14720387756824493\n",
      "step: 879, loss: 0.09783339500427246\n",
      "step: 880, loss: 0.10082004219293594\n",
      "step: 881, loss: 0.18077075481414795\n",
      "step: 882, loss: 0.19146710634231567\n",
      "step: 883, loss: 0.09322946518659592\n",
      "step: 884, loss: 0.0706837922334671\n",
      "step: 885, loss: 0.11924310028553009\n",
      "step: 886, loss: 0.1753840297460556\n",
      "step: 887, loss: 0.13825781643390656\n",
      "step: 888, loss: 0.1295650601387024\n",
      "step: 889, loss: 0.12050361931324005\n",
      "step: 890, loss: 0.151497483253479\n",
      "step: 891, loss: 0.3767141103744507\n",
      "step: 892, loss: 0.1663929522037506\n",
      "step: 893, loss: 0.3164249062538147\n",
      "step: 894, loss: 0.2637341618537903\n",
      "step: 895, loss: 0.09079775959253311\n",
      "step: 896, loss: 0.16288697719573975\n",
      "step: 897, loss: 0.23595058917999268\n",
      "step: 898, loss: 0.0908285602927208\n",
      "step: 899, loss: 0.3038937747478485\n",
      "step: 900, loss: 0.2964562475681305\n",
      "step: 901, loss: 0.21729105710983276\n",
      "step: 902, loss: 0.20725394785404205\n",
      "step: 903, loss: 0.13503165543079376\n",
      "step: 904, loss: 0.11079160869121552\n",
      "step: 905, loss: 0.13033445179462433\n",
      "step: 906, loss: 0.2380920648574829\n",
      "step: 907, loss: 0.10145723074674606\n",
      "step: 908, loss: 0.05678318440914154\n",
      "step: 909, loss: 0.16706161201000214\n",
      "step: 910, loss: 0.09457815438508987\n",
      "step: 911, loss: 0.14232933521270752\n",
      "step: 912, loss: 0.12666447460651398\n",
      "step: 913, loss: 0.806648850440979\n",
      "step: 914, loss: 0.15661339461803436\n",
      "step: 915, loss: 0.11618410050868988\n",
      "step: 916, loss: 0.1747034639120102\n",
      "step: 917, loss: 0.08857791870832443\n",
      "step: 918, loss: 0.05573531985282898\n",
      "step: 919, loss: 0.11377587169408798\n",
      "step: 920, loss: 0.0022071567364037037\n",
      "step: 921, loss: 0.10586291551589966\n",
      "step: 922, loss: 0.2628558576107025\n",
      "step: 923, loss: 0.09943859279155731\n",
      "step: 924, loss: 0.06553259491920471\n",
      "step: 925, loss: 0.11946287006139755\n",
      "step: 926, loss: 0.08911269903182983\n",
      "step: 927, loss: 0.08770176023244858\n",
      "step: 928, loss: 0.1013231948018074\n",
      "step: 929, loss: 0.20923766493797302\n",
      "step: 930, loss: 0.05314522609114647\n",
      "step: 931, loss: 0.020230431109666824\n",
      "step: 932, loss: 0.13976101577281952\n",
      "step: 933, loss: 0.17900165915489197\n",
      "step: 934, loss: 0.18858934938907623\n",
      "step: 935, loss: 0.18486443161964417\n",
      "step: 936, loss: 0.06333313137292862\n",
      "step: 937, loss: 0.15941689908504486\n",
      "step: 938, loss: 0.19613845646381378\n",
      "step: 939, loss: 0.09400244057178497\n",
      "step: 940, loss: 0.1513659507036209\n",
      "step: 941, loss: 0.06409887969493866\n",
      "step: 942, loss: 0.1268908977508545\n",
      "step: 943, loss: 0.13900813460350037\n",
      "step: 944, loss: 0.17400573194026947\n",
      "step: 945, loss: 0.08182000368833542\n",
      "step: 946, loss: 0.032623566687107086\n",
      "step: 947, loss: 0.053603868931531906\n",
      "step: 948, loss: 0.1236015111207962\n",
      "step: 949, loss: 0.1401730477809906\n",
      "step: 950, loss: 0.06577131152153015\n",
      "step: 951, loss: 0.28510162234306335\n",
      "step: 952, loss: 0.0328168123960495\n",
      "step: 953, loss: 0.1361064463853836\n",
      "step: 954, loss: 0.1219763308763504\n",
      "step: 955, loss: 0.10094745457172394\n",
      "step: 956, loss: 0.16138292849063873\n",
      "step: 957, loss: 0.08193009346723557\n",
      "step: 958, loss: 0.03905732184648514\n",
      "step: 959, loss: 0.07495804876089096\n",
      "step: 960, loss: 0.20876525342464447\n",
      "step: 961, loss: 0.04778805747628212\n",
      "step: 962, loss: 0.19496317207813263\n",
      "step: 963, loss: 0.12755832076072693\n",
      "step: 964, loss: 0.115915946662426\n",
      "step: 965, loss: 0.07523766905069351\n",
      "step: 966, loss: 0.12869837880134583\n",
      "step: 967, loss: 0.2765554189682007\n",
      "step: 968, loss: 0.12111470848321915\n",
      "step: 969, loss: 0.13260935246944427\n",
      "step: 970, loss: 0.18135866522789001\n",
      "step: 971, loss: 0.27518168091773987\n",
      "step: 972, loss: 0.09745578467845917\n",
      "step: 973, loss: 0.10211038589477539\n",
      "step: 974, loss: 0.08399650454521179\n",
      "step: 975, loss: 0.10678362846374512\n",
      "step: 976, loss: 0.10762524604797363\n",
      "step: 977, loss: 0.25373566150665283\n",
      "step: 978, loss: 0.24451398849487305\n",
      "step: 979, loss: 0.24102085828781128\n",
      "step: 980, loss: 0.08779848366975784\n",
      "step: 981, loss: 0.1443144679069519\n",
      "step: 982, loss: 0.12649749219417572\n",
      "step: 983, loss: 0.18911691009998322\n",
      "step: 984, loss: 0.19018016755580902\n",
      "step: 985, loss: 0.07319473475217819\n",
      "step: 986, loss: 0.3179226815700531\n",
      "step: 987, loss: 0.17887188494205475\n",
      "step: 988, loss: 0.15986891090869904\n",
      "step: 989, loss: 0.07073076069355011\n",
      "step: 990, loss: 0.13694460690021515\n",
      "step: 991, loss: 0.18005938827991486\n",
      "step: 992, loss: 0.07753916829824448\n",
      "step: 993, loss: 0.04210670292377472\n",
      "step: 994, loss: 0.18838922679424286\n",
      "step: 995, loss: 0.17669731378555298\n",
      "step: 996, loss: 0.20269159972667694\n",
      "step: 997, loss: 0.10997006297111511\n",
      "step: 998, loss: 0.08319629728794098\n",
      "step: 999, loss: 0.037283264100551605\n",
      "step: 1000, loss: 0.08601562678813934\n",
      "step: 1001, loss: 0.10506286472082138\n",
      "step: 1002, loss: 0.16572633385658264\n",
      "step: 1003, loss: 0.09861133247613907\n",
      "step: 1004, loss: 0.23162546753883362\n",
      "step: 1005, loss: 0.11080870777368546\n",
      "step: 1006, loss: 0.1558895856142044\n",
      "step: 1007, loss: 0.08929355442523956\n",
      "step: 1008, loss: 0.1332589089870453\n",
      "step: 1009, loss: 0.08724796026945114\n",
      "step: 1010, loss: 0.17672157287597656\n",
      "step: 1011, loss: 0.0680195689201355\n",
      "step: 1012, loss: 0.059108734130859375\n",
      "step: 1013, loss: 0.11685197800397873\n",
      "step: 1014, loss: 0.26468101143836975\n",
      "step: 1015, loss: 0.17943710088729858\n",
      "step: 1016, loss: 0.11123599112033844\n",
      "step: 1017, loss: 0.018504412844777107\n",
      "step: 1018, loss: 0.21002335846424103\n",
      "step: 1019, loss: 0.3439432382583618\n",
      "step: 1020, loss: 0.10864387452602386\n",
      "step: 1021, loss: 0.16327467560768127\n",
      "step: 1022, loss: 0.1179642453789711\n",
      "step: 1023, loss: 0.08831918984651566\n",
      "step: 1024, loss: 0.03106093965470791\n",
      "step: 1025, loss: 0.21592950820922852\n",
      "step: 1026, loss: 0.1463758498430252\n",
      "step: 1027, loss: 0.142934650182724\n",
      "step: 1028, loss: 0.1837233155965805\n",
      "step: 1029, loss: 0.14748185873031616\n",
      "step: 1030, loss: 0.17041847109794617\n",
      "step: 1031, loss: 0.05036553740501404\n",
      "step: 1032, loss: 0.10714813321828842\n",
      "step: 1033, loss: 0.14384368062019348\n",
      "step: 1034, loss: 0.10775472968816757\n",
      "step: 1035, loss: 0.1126851812005043\n",
      "step: 1036, loss: 0.2741042673587799\n",
      "step: 1037, loss: 0.14123952388763428\n",
      "step: 1038, loss: 0.22781439125537872\n",
      "step: 1039, loss: 0.2189062535762787\n",
      "step: 1040, loss: 0.211182102560997\n",
      "step: 1041, loss: 0.15457846224308014\n",
      "step: 1042, loss: 0.24239464104175568\n",
      "step: 1043, loss: 0.05310218781232834\n",
      "step: 1044, loss: 0.053254470229148865\n",
      "step: 1045, loss: 0.1421528160572052\n",
      "step: 1046, loss: 0.11492395401000977\n",
      "step: 1047, loss: 0.07156936824321747\n",
      "step: 1048, loss: 0.13236835598945618\n",
      "step: 1049, loss: 0.29070866107940674\n",
      "step: 1050, loss: 0.03902731463313103\n",
      "step: 1051, loss: 0.11027009785175323\n",
      "step: 1052, loss: 0.18630556762218475\n",
      "step: 1053, loss: 0.19720935821533203\n",
      "step: 1054, loss: 0.13758496940135956\n",
      "step: 1055, loss: 0.09026584774255753\n",
      "step: 1056, loss: 0.18716755509376526\n",
      "step: 1057, loss: 0.10276437550783157\n",
      "step: 1058, loss: 0.037778716534376144\n",
      "step: 1059, loss: 0.11825201660394669\n",
      "step: 1060, loss: 0.19250722229480743\n",
      "step: 1061, loss: 0.17138594388961792\n",
      "step: 1062, loss: 0.13398291170597076\n",
      "step: 1063, loss: 0.1472734957933426\n",
      "step: 1064, loss: 0.059383805841207504\n",
      "step: 1065, loss: 0.1882612556219101\n",
      "step: 1066, loss: 0.24501952528953552\n",
      "step: 1067, loss: 0.04351290315389633\n",
      "step: 1068, loss: 0.12369771301746368\n",
      "step: 1069, loss: 0.13900472223758698\n",
      "step: 1070, loss: 0.2763984799385071\n",
      "step: 1071, loss: 0.14514556527137756\n",
      "step: 1072, loss: 0.028732366859912872\n",
      "step: 1073, loss: 0.09465374052524567\n",
      "step: 1074, loss: 0.05381512641906738\n",
      "step: 1075, loss: 0.18692393600940704\n",
      "step: 1076, loss: 0.1278669536113739\n",
      "step: 1077, loss: 0.00985061563551426\n",
      "step: 1078, loss: 0.1883145123720169\n",
      "step: 1079, loss: 0.10865110158920288\n",
      "step: 1080, loss: 0.1718311756849289\n",
      "step: 1081, loss: 0.14535246789455414\n",
      "step: 1082, loss: 0.17430664598941803\n",
      "step: 1083, loss: 0.05893218144774437\n",
      "step: 1084, loss: 0.12652452290058136\n",
      "step: 1085, loss: 0.020842473953962326\n",
      "step: 1086, loss: 0.13459502160549164\n",
      "step: 1087, loss: 0.07567249238491058\n",
      "step: 1088, loss: 0.14694423973560333\n",
      "step: 1089, loss: 0.1384744793176651\n",
      "step: 1090, loss: 0.15176604688167572\n",
      "step: 1091, loss: 0.06092926114797592\n",
      "step: 1092, loss: 0.15666607022285461\n",
      "step: 1093, loss: 0.25799983739852905\n",
      "step: 1094, loss: 0.11589483171701431\n",
      "step: 1095, loss: 0.10837925225496292\n",
      "step: 1096, loss: 0.12089476734399796\n",
      "step: 1097, loss: 0.12992088496685028\n",
      "step: 1098, loss: 0.10881495475769043\n",
      "step: 1099, loss: 0.13459846377372742\n",
      "step: 1100, loss: 0.009289998561143875\n",
      "step: 1101, loss: 0.10675731301307678\n",
      "step: 1102, loss: 0.04839971661567688\n",
      "step: 1103, loss: 0.12636326253414154\n",
      "step: 1104, loss: 0.1344427913427353\n",
      "step: 1105, loss: 0.17068371176719666\n",
      "step: 1106, loss: 0.3853317201137543\n",
      "step: 1107, loss: 0.13292579352855682\n",
      "step: 1108, loss: 0.04004774987697601\n",
      "step: 1109, loss: 0.033841852098703384\n",
      "step: 1110, loss: 0.25273966789245605\n",
      "step: 1111, loss: 0.14285579323768616\n",
      "step: 1112, loss: 0.17856672406196594\n",
      "step: 1113, loss: 0.14822271466255188\n",
      "step: 1114, loss: 0.08938749134540558\n",
      "step: 1115, loss: 0.07932499051094055\n",
      "step: 1116, loss: 0.05015358328819275\n",
      "step: 1117, loss: 0.12387362122535706\n",
      "step: 1118, loss: 0.11321720480918884\n",
      "step: 1119, loss: 0.28884047269821167\n",
      "step: 1120, loss: 0.264535516500473\n",
      "step: 1121, loss: 0.2997089922428131\n",
      "step: 1122, loss: 0.2108321189880371\n",
      "step: 1123, loss: 0.10621721297502518\n",
      "step: 1124, loss: 0.06755927950143814\n",
      "step: 1125, loss: 0.1369025558233261\n",
      "step: 1126, loss: 0.16225780546665192\n",
      "step: 1127, loss: 0.13203483819961548\n",
      "step: 1128, loss: 0.08620882034301758\n",
      "step: 1129, loss: 0.11990026384592056\n",
      "step: 1130, loss: 0.21115238964557648\n",
      "step: 1131, loss: 0.08992812782526016\n",
      "step: 1132, loss: 0.03949630632996559\n",
      "step: 1133, loss: 0.0768253430724144\n",
      "step: 1134, loss: 0.1081148236989975\n",
      "step: 1135, loss: 0.15074783563613892\n",
      "step: 1136, loss: 0.15084974467754364\n",
      "step: 1137, loss: 0.04678370803594589\n",
      "step: 1138, loss: 0.009289269335567951\n",
      "step: 1139, loss: 0.027823276817798615\n",
      "step: 1140, loss: 0.12684765458106995\n",
      "step: 1141, loss: 0.28892025351524353\n",
      "step: 1142, loss: 0.03710220009088516\n",
      "step: 1143, loss: 0.06122318282723427\n",
      "step: 1144, loss: 0.08323220908641815\n",
      "step: 1145, loss: 0.032473400235176086\n",
      "step: 1146, loss: 0.05498652160167694\n",
      "step: 1147, loss: 0.032857511192560196\n",
      "step: 1148, loss: 0.09688163548707962\n",
      "step: 1149, loss: 0.08454983681440353\n",
      "step: 1150, loss: 0.45028167963027954\n",
      "step: 1151, loss: 0.17243503034114838\n",
      "step: 1152, loss: 0.09844215214252472\n",
      "step: 1153, loss: 0.17407868802547455\n",
      "step: 1154, loss: 0.15906748175621033\n",
      "step: 1155, loss: 0.15605032444000244\n",
      "step: 1156, loss: 0.12314166873693466\n",
      "step: 1157, loss: 0.17361506819725037\n",
      "step: 1158, loss: 0.062013980001211166\n",
      "step: 1159, loss: 0.10201618820428848\n",
      "step: 1160, loss: 0.22639568150043488\n",
      "step: 1161, loss: 0.08477599918842316\n",
      "step: 1162, loss: 0.1323891133069992\n",
      "step: 1163, loss: 0.06374046206474304\n",
      "step: 1164, loss: 0.17484624683856964\n",
      "step: 1165, loss: 0.03950792923569679\n",
      "step: 1166, loss: 0.04434655234217644\n",
      "step: 1167, loss: 0.03845524787902832\n",
      "step: 1168, loss: 0.06570447236299515\n",
      "step: 1169, loss: 0.12394046783447266\n",
      "step: 1170, loss: 0.15079428255558014\n",
      "step: 1171, loss: 0.050483837723731995\n",
      "step: 1172, loss: 0.06360792368650436\n",
      "step: 1173, loss: 0.09176607429981232\n",
      "step: 1174, loss: 0.24022530019283295\n",
      "step: 1175, loss: 0.1198648065328598\n",
      "step: 1176, loss: 0.18796859681606293\n",
      "step: 1177, loss: 0.09044865518808365\n",
      "step: 1178, loss: 0.08036085963249207\n",
      "step: 1179, loss: 0.11442622542381287\n",
      "step: 1180, loss: 0.11990100145339966\n",
      "step: 1181, loss: 0.07364120334386826\n",
      "step: 1182, loss: 0.059966783970594406\n",
      "step: 1183, loss: 0.0029188625048846006\n",
      "step: 1184, loss: 0.10588613152503967\n",
      "step: 1185, loss: 0.07470013946294785\n",
      "step: 1186, loss: 0.2774789035320282\n",
      "step: 1187, loss: 0.12583887577056885\n",
      "step: 1188, loss: 0.06293876469135284\n",
      "step: 1189, loss: 0.08855956047773361\n",
      "step: 1190, loss: 0.16185830533504486\n",
      "step: 1191, loss: 0.252774178981781\n",
      "step: 1192, loss: 0.1973053365945816\n",
      "step: 1193, loss: 0.07968278974294662\n",
      "step: 1194, loss: 0.07181825488805771\n",
      "step: 1195, loss: 0.014425736851990223\n",
      "step: 1196, loss: 0.04179662838578224\n",
      "step: 1197, loss: 0.052078064531087875\n",
      "step: 1198, loss: 0.11842542886734009\n",
      "step: 1199, loss: 0.15896356105804443\n",
      "step: 1200, loss: 0.0013413351261988282\n",
      "step: 1201, loss: 0.10582137852907181\n",
      "step: 1202, loss: 0.11134897172451019\n",
      "step: 1203, loss: 0.12838585674762726\n",
      "step: 1204, loss: 0.07515601813793182\n",
      "step: 1205, loss: 0.06613468378782272\n",
      "step: 1206, loss: 0.1905997097492218\n",
      "step: 1207, loss: 0.04733899235725403\n",
      "step: 1208, loss: 0.11938817799091339\n",
      "step: 1209, loss: 0.16532115638256073\n",
      "step: 1210, loss: 0.10691439360380173\n",
      "step: 1211, loss: 0.004616629332304001\n",
      "step: 1212, loss: 0.13025197386741638\n",
      "step: 1213, loss: 0.10844888538122177\n",
      "step: 1214, loss: 0.08399061113595963\n",
      "step: 1215, loss: 0.32298678159713745\n",
      "step: 1216, loss: 0.038298074156045914\n",
      "step: 1217, loss: 0.0642922893166542\n",
      "step: 1218, loss: 0.036831699311733246\n",
      "step: 1219, loss: 0.10659899562597275\n",
      "step: 1220, loss: 0.06109916791319847\n",
      "step: 1221, loss: 0.16254137456417084\n",
      "step: 1222, loss: 0.22725339233875275\n",
      "step: 1223, loss: 0.30262497067451477\n",
      "step: 1224, loss: 0.09953903406858444\n",
      "step: 1225, loss: 0.1597471535205841\n",
      "step: 1226, loss: 0.04261601343750954\n",
      "step: 1227, loss: 0.05795716121792793\n",
      "step: 1228, loss: 0.09701738506555557\n",
      "step: 1229, loss: 0.04666014015674591\n",
      "step: 1230, loss: 0.042430099099874496\n",
      "step: 1231, loss: 0.09908825904130936\n",
      "step: 1232, loss: 0.1017254889011383\n",
      "step: 1233, loss: 0.012323555536568165\n",
      "step: 1234, loss: 0.06414362788200378\n",
      "step: 1235, loss: 0.017187103629112244\n",
      "step: 1236, loss: 0.03577309101819992\n",
      "step: 1237, loss: 0.18167760968208313\n",
      "step: 1238, loss: 0.10476735979318619\n",
      "step: 1239, loss: 0.03782859817147255\n",
      "step: 1240, loss: 0.048870302736759186\n",
      "step: 1241, loss: 0.16430670022964478\n",
      "step: 1242, loss: 0.006154290400445461\n",
      "step: 1243, loss: 0.041132088750600815\n",
      "step: 1244, loss: 0.2188311070203781\n",
      "step: 1245, loss: 0.06274903565645218\n",
      "step: 1246, loss: 0.1007089614868164\n",
      "step: 1247, loss: 0.14686720073223114\n",
      "step: 1248, loss: 0.013457362540066242\n",
      "step: 1249, loss: 0.11653667688369751\n",
      "step: 1250, loss: 0.10710088908672333\n",
      "step: 1251, loss: 0.01435940619558096\n",
      "step: 1252, loss: 0.16009417176246643\n",
      "step: 1253, loss: 0.23860161006450653\n",
      "step: 1254, loss: 0.09449135512113571\n",
      "step: 1255, loss: 0.05834145098924637\n",
      "step: 1256, loss: 0.12501126527786255\n",
      "step: 1257, loss: 0.040754467248916626\n",
      "step: 1258, loss: 0.01567421853542328\n",
      "step: 1259, loss: 0.06897849589586258\n",
      "step: 1260, loss: 0.04234331101179123\n",
      "step: 1261, loss: 0.07930519431829453\n",
      "step: 1262, loss: 0.05155033990740776\n",
      "step: 1263, loss: 0.09215770661830902\n",
      "step: 1264, loss: 0.12666171789169312\n",
      "step: 1265, loss: 0.04393783211708069\n",
      "step: 1266, loss: 0.08234598487615585\n",
      "step: 1267, loss: 0.12393143028020859\n",
      "step: 1268, loss: 0.16081883013248444\n",
      "step: 1269, loss: 0.031569354236125946\n",
      "step: 1270, loss: 0.12018761038780212\n",
      "step: 1271, loss: 0.1017128974199295\n",
      "step: 1272, loss: 0.0324966162443161\n",
      "step: 1273, loss: 0.022870395332574844\n",
      "step: 1274, loss: 0.07659570872783661\n",
      "step: 1275, loss: 0.15292254090309143\n",
      "step: 1276, loss: 0.14788566529750824\n",
      "step: 1277, loss: 0.011456443928182125\n",
      "step: 1278, loss: 0.054461847990751266\n",
      "step: 1279, loss: 0.1277357041835785\n",
      "step: 1280, loss: 0.07555924355983734\n",
      "step: 1281, loss: 0.10387689620256424\n",
      "step: 1282, loss: 0.04291176795959473\n",
      "step: 1283, loss: 0.05590108036994934\n",
      "step: 1284, loss: 0.06637534499168396\n",
      "step: 1285, loss: 0.247722327709198\n",
      "step: 1286, loss: 0.12936857342720032\n",
      "step: 1287, loss: 0.097481869161129\n",
      "step: 1288, loss: 0.023179426789283752\n",
      "step: 1289, loss: 0.010980511084198952\n",
      "step: 1290, loss: 0.0362296923995018\n",
      "step: 1291, loss: 0.16510799527168274\n",
      "step: 1292, loss: 0.17609669268131256\n",
      "step: 1293, loss: 0.041355717927217484\n",
      "step: 1294, loss: 0.06655001640319824\n",
      "step: 1295, loss: 0.07725091278553009\n",
      "step: 1296, loss: 0.03308824449777603\n",
      "step: 1297, loss: 0.0440947562456131\n",
      "step: 1298, loss: 0.03642436861991882\n",
      "step: 1299, loss: 0.15171971917152405\n",
      "step: 1300, loss: 0.08278718590736389\n",
      "step: 1301, loss: 0.1052052229642868\n",
      "step: 1302, loss: 0.05056562274694443\n",
      "step: 1303, loss: 0.0030955488327890635\n",
      "step: 1304, loss: 0.16934753954410553\n",
      "step: 1305, loss: 0.1356753557920456\n",
      "step: 1306, loss: 0.18372316658496857\n",
      "step: 1307, loss: 0.042376451194286346\n",
      "step: 1308, loss: 0.027063755318522453\n",
      "step: 1309, loss: 0.10754894465208054\n",
      "step: 1310, loss: 0.09588198363780975\n",
      "step: 1311, loss: 0.08577097952365875\n",
      "step: 1312, loss: 0.06740513443946838\n",
      "step: 1313, loss: 0.05714784562587738\n",
      "step: 1314, loss: 0.05162516608834267\n",
      "step: 1315, loss: 0.04233338311314583\n",
      "step: 1316, loss: 0.04676897078752518\n",
      "step: 1317, loss: 0.06466871500015259\n",
      "step: 1318, loss: 0.04215623065829277\n",
      "step: 1319, loss: 0.06255856156349182\n",
      "step: 1320, loss: 0.04518388211727142\n",
      "step: 1321, loss: 0.043939609080553055\n",
      "step: 1322, loss: 0.057589150965213776\n",
      "step: 1323, loss: 0.22691693902015686\n",
      "step: 1324, loss: 0.26699352264404297\n",
      "step: 1325, loss: 0.03865538164973259\n",
      "step: 1326, loss: 0.060913678258657455\n",
      "step: 1327, loss: 0.05079151317477226\n",
      "step: 1328, loss: 0.04992342367768288\n",
      "step: 1329, loss: 0.00379274133592844\n",
      "step: 1330, loss: 0.017875324934720993\n",
      "step: 1331, loss: 0.1079791709780693\n",
      "step: 1332, loss: 0.1538635492324829\n",
      "step: 1333, loss: 0.1457483470439911\n",
      "step: 1334, loss: 0.14090056717395782\n",
      "step: 1335, loss: 0.05964069440960884\n",
      "step: 1336, loss: 0.03782268613576889\n",
      "step: 1337, loss: 0.08084341883659363\n",
      "step: 1338, loss: 0.1244889423251152\n",
      "step: 1339, loss: 0.16514548659324646\n",
      "step: 1340, loss: 0.1169070303440094\n",
      "step: 1341, loss: 0.014438027516007423\n",
      "step: 1342, loss: 0.0655585303902626\n",
      "step: 1343, loss: 0.14218942821025848\n",
      "step: 1344, loss: 0.1416289508342743\n",
      "step: 1345, loss: 0.10785798728466034\n",
      "step: 1346, loss: 0.045547258108854294\n",
      "step: 1347, loss: 0.10398545116186142\n",
      "step: 1348, loss: 0.0874439999461174\n",
      "step: 1349, loss: 0.06506871432065964\n",
      "step: 1350, loss: 0.0330406092107296\n",
      "step: 1351, loss: 0.045214708894491196\n",
      "step: 1352, loss: 0.03921470418572426\n",
      "step: 1353, loss: 0.07295785844326019\n",
      "step: 1354, loss: 0.03851478919386864\n",
      "step: 1355, loss: 0.063882015645504\n",
      "step: 1356, loss: 0.040990859270095825\n",
      "step: 1357, loss: 0.058926817029714584\n",
      "step: 1358, loss: 0.08801508694887161\n",
      "step: 1359, loss: 0.04833485186100006\n",
      "step: 1360, loss: 0.0645863264799118\n",
      "step: 1361, loss: 0.029154803603887558\n",
      "step: 1362, loss: 0.054418906569480896\n",
      "step: 1363, loss: 0.18210510909557343\n",
      "step: 1364, loss: 0.05214957892894745\n",
      "step: 1365, loss: 0.06342171877622604\n",
      "step: 1366, loss: 0.024363430216908455\n",
      "step: 1367, loss: 0.06951331347227097\n",
      "step: 1368, loss: 0.0814782902598381\n",
      "step: 1369, loss: 0.1395527869462967\n",
      "step: 1370, loss: 0.02416253834962845\n",
      "step: 1371, loss: 0.06064993888139725\n",
      "step: 1372, loss: 0.18968425691127777\n",
      "step: 1373, loss: 0.010246463119983673\n",
      "step: 1374, loss: 0.01698143035173416\n",
      "step: 1375, loss: 0.06328333169221878\n",
      "step: 1376, loss: 0.04390251263976097\n",
      "step: 1377, loss: 0.007755005732178688\n",
      "step: 1378, loss: 0.0348186157643795\n",
      "step: 1379, loss: 0.15855136513710022\n",
      "step: 1380, loss: 0.20744331181049347\n",
      "step: 1381, loss: 0.0710422545671463\n",
      "step: 1382, loss: 0.07067912071943283\n",
      "step: 1383, loss: 0.06699374318122864\n",
      "step: 1384, loss: 0.08771719038486481\n",
      "step: 1385, loss: 0.06977342814207077\n",
      "step: 1386, loss: 0.1304071843624115\n",
      "step: 1387, loss: 0.08047451078891754\n",
      "step: 1388, loss: 0.1012856513261795\n",
      "step: 1389, loss: 0.08703198283910751\n",
      "step: 1390, loss: 0.010836882516741753\n",
      "step: 1391, loss: 0.03652498871088028\n",
      "step: 1392, loss: 0.06030634418129921\n",
      "step: 1393, loss: 0.09733304381370544\n",
      "step: 1394, loss: 0.0865299329161644\n",
      "step: 1395, loss: 0.06853213161230087\n",
      "step: 1396, loss: 0.031985215842723846\n",
      "step: 1397, loss: 0.034180764108896255\n",
      "step: 1398, loss: 0.046878647059202194\n",
      "step: 1399, loss: 0.056124549359083176\n",
      "step: 1400, loss: 0.08046022057533264\n",
      "step: 1401, loss: 0.05899514630436897\n",
      "step: 1402, loss: 0.0430045984685421\n",
      "step: 1403, loss: 0.013449042104184628\n",
      "step: 1404, loss: 0.10886578261852264\n",
      "step: 1405, loss: 0.041712239384651184\n",
      "step: 1406, loss: 0.0376683808863163\n",
      "step: 1407, loss: 0.09488008171319962\n",
      "step: 1408, loss: 0.002747602527961135\n",
      "step: 1409, loss: 0.11344362050294876\n",
      "step: 1410, loss: 0.14938154816627502\n",
      "step: 1411, loss: 0.18549947440624237\n",
      "step: 1412, loss: 0.024171622470021248\n",
      "step: 1413, loss: 0.02659710682928562\n",
      "step: 1414, loss: 0.06360511481761932\n",
      "step: 1415, loss: 0.07322724163532257\n",
      "step: 1416, loss: 0.03823421522974968\n",
      "step: 1417, loss: 0.004073993302881718\n",
      "step: 1418, loss: 0.08876859396696091\n",
      "step: 1419, loss: 0.10064554214477539\n",
      "step: 1420, loss: 0.012388941831886768\n",
      "step: 1421, loss: 0.058122750371694565\n",
      "step: 1422, loss: 0.11120115220546722\n",
      "step: 1423, loss: 0.10419158637523651\n",
      "step: 1424, loss: 0.0507119856774807\n",
      "step: 1425, loss: 0.10108253359794617\n",
      "step: 1426, loss: 0.12425442039966583\n",
      "step: 1427, loss: 0.005730943754315376\n",
      "step: 1428, loss: 0.047876425087451935\n",
      "step: 1429, loss: 0.09108539670705795\n",
      "step: 1430, loss: 0.0131721505895257\n",
      "step: 1431, loss: 0.008125480264425278\n",
      "step: 1432, loss: 0.029288673773407936\n",
      "step: 1433, loss: 0.1821291744709015\n",
      "step: 1434, loss: 0.025789879262447357\n",
      "step: 1435, loss: 0.14466845989227295\n",
      "step: 1436, loss: 0.062437087297439575\n",
      "step: 1437, loss: 0.11802785098552704\n",
      "step: 1438, loss: 0.04786394163966179\n",
      "step: 1439, loss: 0.10306331515312195\n",
      "step: 1440, loss: 0.06850877404212952\n",
      "step: 1441, loss: 0.04511597752571106\n",
      "step: 1442, loss: 0.042401500046253204\n",
      "step: 1443, loss: 0.06920479983091354\n",
      "step: 1444, loss: 0.007404955103993416\n",
      "step: 1445, loss: 0.0802779272198677\n",
      "step: 1446, loss: 0.024854741990566254\n",
      "step: 1447, loss: 0.008398747071623802\n",
      "step: 1448, loss: 0.05000820755958557\n",
      "step: 1449, loss: 0.12499090284109116\n",
      "step: 1450, loss: 0.012182354927062988\n",
      "step: 1451, loss: 0.08400654047727585\n",
      "step: 1452, loss: 0.08096584677696228\n",
      "step: 1453, loss: 0.047857023775577545\n",
      "step: 1454, loss: 0.22254982590675354\n",
      "step: 1455, loss: 0.10236702859401703\n",
      "step: 1456, loss: 0.08623187988996506\n",
      "step: 1457, loss: 0.15632398426532745\n",
      "step: 1458, loss: 0.09178155660629272\n",
      "step: 1459, loss: 0.015300285071134567\n",
      "step: 1460, loss: 0.1434398591518402\n",
      "step: 1461, loss: 0.041649166494607925\n",
      "step: 1462, loss: 0.09826585650444031\n",
      "step: 1463, loss: 0.09747596085071564\n",
      "step: 1464, loss: 0.006624361500144005\n",
      "step: 1465, loss: 0.11612169444561005\n",
      "step: 1466, loss: 0.09446361660957336\n",
      "step: 1467, loss: 0.11423137784004211\n",
      "step: 1468, loss: 0.07385510206222534\n",
      "step: 1469, loss: 0.008822346106171608\n",
      "step: 1470, loss: 0.005990236531943083\n",
      "step: 1471, loss: 0.14580249786376953\n",
      "step: 1472, loss: 0.1354558765888214\n",
      "step: 1473, loss: 0.06385969370603561\n",
      "step: 1474, loss: 0.05813802778720856\n",
      "step: 1475, loss: 0.0911506712436676\n",
      "step: 1476, loss: 0.017427213490009308\n",
      "step: 1477, loss: 0.04865524172782898\n",
      "step: 1478, loss: 0.0021698824129998684\n",
      "step: 1479, loss: 0.052562154829502106\n",
      "step: 1480, loss: 0.013686385937035084\n",
      "step: 1481, loss: 0.031818557530641556\n",
      "step: 1482, loss: 0.00578401330858469\n",
      "step: 1483, loss: 0.03650842607021332\n",
      "step: 1484, loss: 0.01439273077994585\n",
      "step: 1485, loss: 0.10073696076869965\n",
      "step: 1486, loss: 0.06815627217292786\n",
      "step: 1487, loss: 0.041835423558950424\n",
      "step: 1488, loss: 0.06194641441106796\n",
      "step: 1489, loss: 0.0074804676696658134\n",
      "step: 1490, loss: 0.10685697197914124\n",
      "step: 1491, loss: 0.1680016964673996\n",
      "step: 1492, loss: 0.03641638904809952\n",
      "step: 1493, loss: 0.03280289098620415\n",
      "step: 1494, loss: 0.05520161613821983\n",
      "step: 1495, loss: 0.029525332152843475\n",
      "step: 1496, loss: 0.0011034425115212798\n",
      "step: 1497, loss: 0.011309009045362473\n",
      "step: 1498, loss: 0.01327357068657875\n",
      "step: 1499, loss: 0.05707221478223801\n",
      "step: 1500, loss: 0.0851275846362114\n",
      "step: 1501, loss: 0.005077195819467306\n",
      "step: 1502, loss: 0.004963067825883627\n",
      "step: 1503, loss: 0.04213520511984825\n",
      "step: 1504, loss: 0.023297719657421112\n",
      "step: 1505, loss: 0.09710618108510971\n",
      "step: 1506, loss: 0.015953194350004196\n",
      "step: 1507, loss: 0.05317511036992073\n",
      "step: 1508, loss: 0.027077436447143555\n",
      "step: 1509, loss: 0.007806790992617607\n",
      "step: 1510, loss: 0.031028391793370247\n",
      "step: 1511, loss: 0.14513157308101654\n",
      "step: 1512, loss: 0.09871877729892731\n",
      "step: 1513, loss: 0.005803308915346861\n",
      "step: 1514, loss: 0.05931498110294342\n",
      "step: 1515, loss: 0.09286050498485565\n",
      "step: 1516, loss: 0.0525929294526577\n",
      "step: 1517, loss: 0.10309506207704544\n",
      "step: 1518, loss: 0.0504637137055397\n",
      "step: 1519, loss: 0.019211184233427048\n",
      "step: 1520, loss: 0.04025828465819359\n",
      "step: 1521, loss: 0.11946476250886917\n",
      "step: 1522, loss: 0.04740040376782417\n",
      "step: 1523, loss: 0.15319879353046417\n",
      "step: 1524, loss: 0.018552254885435104\n",
      "step: 1525, loss: 0.036242976784706116\n",
      "step: 1526, loss: 0.027774911373853683\n",
      "step: 1527, loss: 0.0034333565272390842\n",
      "step: 1528, loss: 0.10797170549631119\n",
      "step: 1529, loss: 0.12044577300548553\n",
      "step: 1530, loss: 0.02773980051279068\n",
      "step: 1531, loss: 0.07787314057350159\n",
      "step: 1532, loss: 0.08768831193447113\n",
      "step: 1533, loss: 0.0519351102411747\n",
      "step: 1534, loss: 0.08422275632619858\n",
      "step: 1535, loss: 0.0706184133887291\n",
      "step: 1536, loss: 0.011932014487683773\n",
      "step: 1537, loss: 0.03343115746974945\n",
      "step: 1538, loss: 0.13781969249248505\n",
      "step: 1539, loss: 0.11270143836736679\n",
      "step: 1540, loss: 0.009520169347524643\n",
      "step: 1541, loss: 0.3524554371833801\n",
      "step: 1542, loss: 0.19178977608680725\n",
      "step: 1543, loss: 0.03957612067461014\n",
      "step: 1544, loss: 0.14088016748428345\n",
      "step: 1545, loss: 0.05530219152569771\n",
      "step: 1546, loss: 0.03708692640066147\n",
      "step: 1547, loss: 0.09007440507411957\n",
      "step: 1548, loss: 0.029700126498937607\n",
      "step: 1549, loss: 0.05720255523920059\n",
      "step: 1550, loss: 0.029888277873396873\n",
      "step: 1551, loss: 0.07685588300228119\n",
      "step: 1552, loss: 0.1122690886259079\n",
      "step: 1553, loss: 0.015359402634203434\n",
      "step: 1554, loss: 0.08344795554876328\n",
      "step: 1555, loss: 0.06954885274171829\n",
      "step: 1556, loss: 0.10782954096794128\n",
      "step: 1557, loss: 0.05153839290142059\n",
      "step: 1558, loss: 0.017923012375831604\n",
      "step: 1559, loss: 0.026684371754527092\n",
      "step: 1560, loss: 0.0547068752348423\n",
      "step: 1561, loss: 0.07939467579126358\n",
      "step: 1562, loss: 0.009617491625249386\n",
      "step: 1563, loss: 0.0669899731874466\n",
      "step: 1564, loss: 0.0034534407313913107\n",
      "step: 1565, loss: 0.007475344464182854\n",
      "step: 1566, loss: 0.051579613238573074\n",
      "step: 1567, loss: 0.15571843087673187\n",
      "step: 1568, loss: 0.07587862014770508\n",
      "step: 1569, loss: 0.12395357340574265\n",
      "step: 1570, loss: 0.009808633476495743\n",
      "step: 1571, loss: 0.014245270751416683\n",
      "step: 1572, loss: 0.06827505677938461\n",
      "step: 1573, loss: 0.16891473531723022\n",
      "step: 1574, loss: 0.0833558738231659\n",
      "step: 1575, loss: 0.09275083243846893\n",
      "step: 1576, loss: 0.0730009451508522\n",
      "step: 1577, loss: 0.005901770666241646\n",
      "step: 1578, loss: 0.06070367991924286\n",
      "step: 1579, loss: 0.013396450318396091\n",
      "step: 1580, loss: 0.011341811157763004\n",
      "step: 1581, loss: 0.06493702530860901\n",
      "step: 1582, loss: 0.008792992681264877\n",
      "step: 1583, loss: 0.03801066800951958\n",
      "step: 1584, loss: 0.10881402343511581\n",
      "step: 1585, loss: 0.0075039369985461235\n",
      "step: 1586, loss: 0.026248488575220108\n",
      "step: 1587, loss: 0.10113274306058884\n",
      "step: 1588, loss: 0.060267459601163864\n",
      "step: 1589, loss: 0.07058394700288773\n",
      "step: 1590, loss: 0.051741667091846466\n",
      "step: 1591, loss: 0.05408246070146561\n",
      "step: 1592, loss: 0.04687762260437012\n",
      "step: 1593, loss: 0.0089891217648983\n",
      "step: 1594, loss: 0.07289689779281616\n",
      "step: 1595, loss: 0.006169723812490702\n",
      "step: 1596, loss: 0.0476032979786396\n",
      "step: 1597, loss: 0.07850389182567596\n",
      "step: 1598, loss: 0.0200046319514513\n",
      "step: 1599, loss: 0.08357597142457962\n",
      "step: 1600, loss: 0.01789698749780655\n",
      "step: 1601, loss: 0.11159434169530869\n",
      "step: 1602, loss: 0.004822090733796358\n",
      "step: 1603, loss: 0.02004323899745941\n",
      "step: 1604, loss: 0.00876162201166153\n",
      "step: 1605, loss: 0.024899227544665337\n",
      "step: 1606, loss: 0.1416919082403183\n",
      "step: 1607, loss: 0.030182279646396637\n",
      "step: 1608, loss: 0.013074550777673721\n",
      "step: 1609, loss: 0.030985333025455475\n",
      "step: 1610, loss: 0.06781356036663055\n",
      "step: 1611, loss: 0.0799240842461586\n",
      "step: 1612, loss: 0.0054292259737849236\n",
      "step: 1613, loss: 0.0033135346602648497\n",
      "step: 1614, loss: 0.05172770097851753\n",
      "step: 1615, loss: 0.028886828571558\n",
      "step: 1616, loss: 0.001237020012922585\n",
      "step: 1617, loss: 0.02845478057861328\n",
      "step: 1618, loss: 0.05234008654952049\n",
      "step: 1619, loss: 0.05688438564538956\n",
      "step: 1620, loss: 0.10589927434921265\n",
      "step: 1621, loss: 0.030693862587213516\n",
      "step: 1622, loss: 0.006476507987827063\n",
      "step: 1623, loss: 0.04378904402256012\n",
      "step: 1624, loss: 0.003876300761476159\n",
      "step: 1625, loss: 0.0029092838522046804\n",
      "step: 1626, loss: 0.12880122661590576\n",
      "step: 1627, loss: 0.11055459827184677\n",
      "step: 1628, loss: 0.028607293963432312\n",
      "step: 1629, loss: 0.057455677539110184\n",
      "step: 1630, loss: 0.0634620189666748\n",
      "step: 1631, loss: 0.05697090923786163\n",
      "step: 1632, loss: 0.12269464135169983\n",
      "step: 1633, loss: 0.04342887923121452\n",
      "step: 1634, loss: 0.005610835738480091\n",
      "step: 1635, loss: 0.06607198715209961\n",
      "step: 1636, loss: 0.03992064297199249\n",
      "step: 1637, loss: 0.04583965614438057\n",
      "step: 1638, loss: 0.045936040580272675\n",
      "step: 1639, loss: 0.06917591392993927\n",
      "step: 1640, loss: 0.007735440041869879\n",
      "step: 1641, loss: 0.05729704722762108\n",
      "step: 1642, loss: 0.1240091547369957\n",
      "step: 1643, loss: 0.08460026979446411\n",
      "step: 1644, loss: 0.008446943946182728\n",
      "step: 1645, loss: 0.0617162361741066\n",
      "step: 1646, loss: 0.05417022854089737\n",
      "step: 1647, loss: 0.004988203290849924\n",
      "step: 1648, loss: 0.08087901771068573\n",
      "step: 1649, loss: 0.061696723103523254\n",
      "step: 1650, loss: 0.07245347648859024\n",
      "step: 1651, loss: 0.08776196837425232\n",
      "step: 1652, loss: 0.03485926240682602\n",
      "step: 1653, loss: 0.053505733609199524\n",
      "step: 1654, loss: 0.003309963969513774\n",
      "step: 1655, loss: 0.08236989378929138\n",
      "step: 1656, loss: 0.10946443676948547\n",
      "step: 1657, loss: 0.08180940896272659\n",
      "step: 1658, loss: 0.06533191353082657\n",
      "step: 1659, loss: 0.08751947432756424\n",
      "step: 1660, loss: 0.051227327436208725\n",
      "step: 1661, loss: 0.02866358496248722\n",
      "step: 1662, loss: 0.09941475093364716\n",
      "step: 1663, loss: 0.012211017310619354\n",
      "step: 1664, loss: 0.022201262414455414\n",
      "step: 1665, loss: 0.03337336704134941\n",
      "step: 1666, loss: 0.12808240950107574\n",
      "step: 1667, loss: 0.06062856689095497\n",
      "step: 1668, loss: 0.09724690765142441\n",
      "step: 1669, loss: 0.05488932505249977\n",
      "step: 1670, loss: 0.0037840288132429123\n",
      "step: 1671, loss: 0.03563984856009483\n",
      "step: 1672, loss: 0.12378774583339691\n",
      "step: 1673, loss: 0.08035299181938171\n",
      "step: 1674, loss: 0.06876092404127121\n",
      "step: 1675, loss: 0.009562915191054344\n",
      "step: 1676, loss: 0.09132354706525803\n",
      "step: 1677, loss: 0.004032638855278492\n",
      "step: 1678, loss: 0.13783439993858337\n",
      "step: 1679, loss: 0.10301051288843155\n",
      "step: 1680, loss: 0.004446838051080704\n",
      "step: 1681, loss: 0.07219342142343521\n",
      "step: 1682, loss: 0.04666381701827049\n",
      "step: 1683, loss: 0.04179765656590462\n",
      "step: 1684, loss: 0.10917317122220993\n",
      "step: 1685, loss: 0.006461364682763815\n",
      "step: 1686, loss: 0.016050001606345177\n",
      "step: 1687, loss: 0.03798913583159447\n",
      "step: 1688, loss: 0.09815019369125366\n",
      "step: 1689, loss: 0.16034618020057678\n",
      "step: 1690, loss: 0.006343272514641285\n",
      "step: 1691, loss: 0.057488471269607544\n",
      "step: 1692, loss: 0.013991033658385277\n",
      "step: 1693, loss: 0.056065354496240616\n",
      "step: 1694, loss: 0.020090894773602486\n",
      "step: 1695, loss: 0.03875676169991493\n",
      "step: 1696, loss: 0.03311948478221893\n",
      "step: 1697, loss: 0.03206285089254379\n",
      "step: 1698, loss: 0.0081657525151968\n",
      "step: 1699, loss: 0.07158256322145462\n",
      "step: 1700, loss: 0.03959902375936508\n",
      "step: 1701, loss: 0.15072354674339294\n",
      "step: 1702, loss: 0.018107639625668526\n",
      "step: 1703, loss: 0.0584372952580452\n",
      "step: 1704, loss: 0.025128740817308426\n",
      "step: 1705, loss: 0.3098464906215668\n",
      "step: 1706, loss: 0.0703817680478096\n",
      "step: 1707, loss: 0.010347967967391014\n",
      "step: 1708, loss: 0.03868609294295311\n",
      "step: 1709, loss: 0.11949185281991959\n",
      "step: 1710, loss: 0.002304377732798457\n",
      "step: 1711, loss: 0.14203472435474396\n",
      "step: 1712, loss: 0.0037635313346982002\n",
      "step: 1713, loss: 0.09257804602384567\n",
      "step: 1714, loss: 0.02667374536395073\n",
      "step: 1715, loss: 0.005854213144630194\n",
      "step: 1716, loss: 0.15627051889896393\n",
      "step: 1717, loss: 0.02364308387041092\n",
      "step: 1718, loss: 0.040420614182949066\n",
      "step: 1719, loss: 0.03248342126607895\n",
      "step: 1720, loss: 0.091435506939888\n",
      "step: 1721, loss: 0.029725661501288414\n",
      "step: 1722, loss: 0.010802101343870163\n",
      "step: 1723, loss: 0.018255650997161865\n",
      "step: 1724, loss: 0.03995991125702858\n",
      "step: 1725, loss: 0.004320777487009764\n",
      "step: 1726, loss: 0.009646160528063774\n",
      "step: 1727, loss: 0.011555928736925125\n",
      "step: 1728, loss: 0.038901425898075104\n",
      "step: 1729, loss: 0.007764801848679781\n",
      "step: 1730, loss: 0.06362064182758331\n",
      "step: 1731, loss: 0.008265272714197636\n",
      "step: 1732, loss: 0.04658162221312523\n",
      "step: 1733, loss: 0.10556541383266449\n",
      "step: 1734, loss: 0.05079605430364609\n",
      "step: 1735, loss: 0.03356611728668213\n",
      "step: 1736, loss: 0.13076873123645782\n",
      "step: 1737, loss: 0.03726659342646599\n",
      "step: 1738, loss: 0.11215978860855103\n",
      "step: 1739, loss: 0.04671085253357887\n",
      "step: 1740, loss: 0.12216996401548386\n",
      "step: 1741, loss: 0.04563723877072334\n",
      "step: 1742, loss: 0.04925866052508354\n",
      "step: 1743, loss: 0.030013851821422577\n",
      "step: 1744, loss: 0.026909226551651955\n",
      "step: 1745, loss: 0.0707017332315445\n",
      "step: 1746, loss: 0.03894234821200371\n",
      "step: 1747, loss: 0.09052272140979767\n",
      "step: 1748, loss: 0.012658383697271347\n",
      "step: 1749, loss: 0.004960760474205017\n",
      "step: 1750, loss: 0.09081055968999863\n",
      "step: 1751, loss: 0.012603742070496082\n",
      "step: 1752, loss: 0.005675848573446274\n",
      "step: 1753, loss: 0.05402735620737076\n",
      "step: 1754, loss: 0.03266981616616249\n",
      "step: 1755, loss: 0.02843235433101654\n",
      "step: 1756, loss: 0.0034008652437478304\n",
      "step: 1757, loss: 0.08088794350624084\n",
      "step: 1758, loss: 0.011985020712018013\n",
      "step: 1759, loss: 0.057100411504507065\n",
      "step: 1760, loss: 0.003506878623738885\n",
      "step: 1761, loss: 0.08897147327661514\n",
      "step: 1762, loss: 0.006923383101820946\n",
      "step: 1763, loss: 0.10238988697528839\n",
      "step: 1764, loss: 0.013167771510779858\n",
      "step: 1765, loss: 0.04993286728858948\n",
      "step: 1766, loss: 0.0021030406933277845\n",
      "step: 1767, loss: 0.04775329679250717\n",
      "step: 1768, loss: 0.07678671926259995\n",
      "step: 1769, loss: 0.03942841291427612\n",
      "step: 1770, loss: 0.036183785647153854\n",
      "step: 1771, loss: 0.09497363865375519\n",
      "step: 1772, loss: 0.11711801588535309\n",
      "step: 1773, loss: 0.10464074462652206\n",
      "step: 1774, loss: 0.06012563779950142\n",
      "step: 1775, loss: 0.041144561022520065\n",
      "step: 1776, loss: 0.12010389566421509\n",
      "step: 1777, loss: 0.007626812439411879\n",
      "step: 1778, loss: 0.00810116995126009\n",
      "step: 1779, loss: 0.05578728765249252\n",
      "step: 1780, loss: 0.08312331140041351\n",
      "step: 1781, loss: 0.004260651301592588\n",
      "step: 1782, loss: 0.08817711472511292\n",
      "step: 1783, loss: 0.008185675367712975\n",
      "step: 1784, loss: 0.008042321540415287\n",
      "step: 1785, loss: 0.094459667801857\n",
      "step: 1786, loss: 0.07152438908815384\n",
      "step: 1787, loss: 0.0734521895647049\n",
      "step: 1788, loss: 0.4310484826564789\n",
      "step: 1789, loss: 0.11614181846380234\n",
      "step: 1790, loss: 0.11362814158201218\n",
      "step: 1791, loss: 0.2023753672838211\n",
      "step: 1792, loss: 0.17166432738304138\n",
      "step: 1793, loss: 0.012526600621640682\n",
      "step: 1794, loss: 0.0043914783746004105\n",
      "step: 1795, loss: 0.2709047794342041\n",
      "step: 1796, loss: 0.01954576186835766\n",
      "step: 1797, loss: 0.0544862374663353\n",
      "step: 1798, loss: 0.03023018315434456\n",
      "step: 1799, loss: 0.10505428910255432\n",
      "step: 1800, loss: 0.04693840444087982\n",
      "step: 1801, loss: 0.07417997717857361\n",
      "step: 1802, loss: 0.007673587184399366\n",
      "step: 1803, loss: 0.006597878877073526\n",
      "step: 1804, loss: 0.018346097320318222\n",
      "step: 1805, loss: 0.07073280215263367\n",
      "step: 1806, loss: 0.0034968333784490824\n",
      "step: 1807, loss: 0.005208736751228571\n",
      "step: 1808, loss: 0.034700267016887665\n",
      "step: 1809, loss: 0.004518340341746807\n",
      "step: 1810, loss: 0.003486161818727851\n",
      "step: 1811, loss: 0.021843647584319115\n",
      "step: 1812, loss: 0.06152249500155449\n",
      "step: 1813, loss: 0.01023000106215477\n",
      "step: 1814, loss: 0.07665494084358215\n",
      "step: 1815, loss: 0.008885432034730911\n",
      "step: 1816, loss: 0.03109062649309635\n",
      "step: 1817, loss: 0.01835942454636097\n",
      "step: 1818, loss: 0.0072347200475633144\n",
      "step: 1819, loss: 0.02592860721051693\n",
      "step: 1820, loss: 0.00447301147505641\n",
      "step: 1821, loss: 0.008644208312034607\n",
      "step: 1822, loss: 0.08295852690935135\n",
      "step: 1823, loss: 0.004475885070860386\n",
      "step: 1824, loss: 0.041130706667900085\n",
      "step: 1825, loss: 0.003442103508859873\n",
      "step: 1826, loss: 0.03556988760828972\n",
      "step: 1827, loss: 0.046192292124032974\n",
      "step: 1828, loss: 0.003784009488299489\n",
      "step: 1829, loss: 0.09547556191682816\n",
      "step: 1830, loss: 0.06286939978599548\n",
      "step: 1831, loss: 0.012066557072103024\n",
      "step: 1832, loss: 0.05758580565452576\n",
      "step: 1833, loss: 0.060883522033691406\n",
      "step: 1834, loss: 0.09279023110866547\n",
      "step: 1835, loss: 0.062054142355918884\n",
      "step: 1836, loss: 0.009540258906781673\n",
      "step: 1837, loss: 0.01913386583328247\n",
      "step: 1838, loss: 0.007543466053903103\n",
      "step: 1839, loss: 0.331065833568573\n",
      "step: 1840, loss: 0.025409622117877007\n",
      "step: 1841, loss: 0.04905032739043236\n",
      "step: 1842, loss: 0.08380620926618576\n",
      "step: 1843, loss: 0.005709478631615639\n",
      "step: 1844, loss: 0.11119215190410614\n",
      "step: 1845, loss: 0.05706804245710373\n",
      "step: 1846, loss: 0.07183411717414856\n",
      "step: 1847, loss: 0.04614383727312088\n",
      "step: 1848, loss: 0.07328274101018906\n",
      "step: 1849, loss: 0.04693036153912544\n",
      "step: 1850, loss: 0.028884749859571457\n",
      "step: 1851, loss: 0.048681724816560745\n",
      "step: 1852, loss: 0.08895138651132584\n",
      "step: 1853, loss: 0.04824478551745415\n",
      "step: 1854, loss: 0.05344275385141373\n",
      "step: 1855, loss: 0.044369492679834366\n",
      "step: 1856, loss: 0.11246530711650848\n",
      "step: 1857, loss: 0.00879758782684803\n",
      "step: 1858, loss: 0.04966600984334946\n",
      "step: 1859, loss: 0.07043710350990295\n",
      "step: 1860, loss: 0.03569237142801285\n",
      "step: 1861, loss: 0.02617918886244297\n",
      "step: 1862, loss: 0.004279686603695154\n",
      "step: 1863, loss: 0.022113613784313202\n",
      "step: 1864, loss: 0.09058594703674316\n",
      "step: 1865, loss: 0.0037172448355704546\n",
      "step: 1866, loss: 0.10604532063007355\n",
      "step: 1867, loss: 0.011085805483162403\n",
      "step: 1868, loss: 0.08223894238471985\n",
      "step: 1869, loss: 0.002191815758123994\n",
      "step: 1870, loss: 0.01083395630121231\n",
      "step: 1871, loss: 0.013352510519325733\n",
      "step: 1872, loss: 0.0581817626953125\n",
      "step: 1873, loss: 0.05211680009961128\n",
      "step: 1874, loss: 0.005144176539033651\n",
      "step: 1875, loss: 0.033000364899635315\n",
      "step: 1876, loss: 0.04101526737213135\n",
      "step: 1877, loss: 0.016818417236208916\n",
      "step: 1878, loss: 0.0052967797964811325\n",
      "step: 1879, loss: 0.04753121733665466\n",
      "step: 1880, loss: 0.0747058093547821\n",
      "step: 1881, loss: 0.21875646710395813\n",
      "step: 1882, loss: 0.005823661107569933\n",
      "step: 1883, loss: 0.007061687298119068\n",
      "step: 1884, loss: 0.030967283993959427\n",
      "step: 1885, loss: 0.006089868023991585\n",
      "step: 1886, loss: 0.053098201751708984\n",
      "step: 1887, loss: 0.004582994617521763\n",
      "step: 1888, loss: 0.046930018812417984\n",
      "step: 1889, loss: 0.04370203614234924\n",
      "step: 1890, loss: 0.008974633179605007\n",
      "step: 1891, loss: 0.09120956063270569\n",
      "step: 1892, loss: 0.053984422236680984\n",
      "step: 1893, loss: 0.0525725893676281\n",
      "step: 1894, loss: 0.07147713005542755\n",
      "step: 1895, loss: 0.0472542867064476\n",
      "step: 1896, loss: 0.008039746433496475\n",
      "step: 1897, loss: 0.06679769605398178\n",
      "step: 1898, loss: 0.04713698476552963\n",
      "step: 1899, loss: 0.002368926303461194\n",
      "step: 1900, loss: 0.03891510143876076\n",
      "step: 1901, loss: 0.029297051951289177\n",
      "step: 1902, loss: 0.0373339019715786\n",
      "step: 1903, loss: 0.005499961320310831\n",
      "step: 1904, loss: 0.003216369776055217\n",
      "step: 1905, loss: 0.008952099829912186\n",
      "step: 1906, loss: 0.1318170130252838\n",
      "step: 1907, loss: 0.0917951837182045\n",
      "step: 1908, loss: 0.024873843416571617\n",
      "step: 1909, loss: 0.13335533440113068\n",
      "step: 1910, loss: 0.03380441293120384\n",
      "step: 1911, loss: 0.06134610250592232\n",
      "step: 1912, loss: 0.03955585137009621\n",
      "step: 1913, loss: 0.007137123961001635\n",
      "step: 1914, loss: 0.0800696536898613\n",
      "step: 1915, loss: 0.035603877156972885\n",
      "step: 1916, loss: 0.03471353277564049\n",
      "step: 1917, loss: 0.01025872491300106\n",
      "step: 1918, loss: 0.09538159519433975\n",
      "step: 1919, loss: 0.012970848008990288\n",
      "step: 1920, loss: 0.06347072869539261\n",
      "step: 1921, loss: 0.07000260055065155\n",
      "step: 1922, loss: 0.027581868693232536\n",
      "step: 1923, loss: 0.0021649219561368227\n",
      "step: 1924, loss: 0.05638410151004791\n",
      "step: 1925, loss: 0.04829038307070732\n",
      "step: 1926, loss: 0.02454197220504284\n",
      "step: 1927, loss: 0.05661727488040924\n",
      "step: 1928, loss: 0.005633797496557236\n",
      "step: 1929, loss: 0.029206596314907074\n",
      "step: 1930, loss: 0.00962456688284874\n",
      "step: 1931, loss: 0.12255512177944183\n",
      "step: 1932, loss: 0.03253009915351868\n",
      "step: 1933, loss: 0.07776980847120285\n",
      "step: 1934, loss: 0.10636585205793381\n",
      "step: 1935, loss: 0.022435087710618973\n",
      "step: 1936, loss: 0.0027664247900247574\n",
      "step: 1937, loss: 0.06682084500789642\n",
      "step: 1938, loss: 0.006925308611243963\n",
      "step: 1939, loss: 0.05651808902621269\n",
      "step: 1940, loss: 0.09653875231742859\n",
      "step: 1941, loss: 0.006262676324695349\n",
      "step: 1942, loss: 0.060408372431993484\n",
      "step: 1943, loss: 0.004826444201171398\n",
      "step: 1944, loss: 0.015154166147112846\n",
      "step: 1945, loss: 0.0934334546327591\n",
      "step: 1946, loss: 0.054188381880521774\n",
      "step: 1947, loss: 0.02555825561285019\n",
      "step: 1948, loss: 0.00495447451248765\n",
      "step: 1949, loss: 0.050000566989183426\n",
      "step: 1950, loss: 0.020854325965046883\n",
      "step: 1951, loss: 0.0067510176450014114\n",
      "step: 1952, loss: 0.050783947110176086\n",
      "step: 1953, loss: 0.011118430644273758\n",
      "step: 1954, loss: 0.1321934461593628\n",
      "step: 1955, loss: 0.06077822670340538\n",
      "step: 1956, loss: 0.006209234707057476\n",
      "step: 1957, loss: 0.21657836437225342\n",
      "step: 1958, loss: 0.016130130738019943\n",
      "step: 1959, loss: 0.04640880599617958\n",
      "step: 1960, loss: 0.014074995182454586\n",
      "step: 1961, loss: 0.21089990437030792\n",
      "step: 1962, loss: 0.0032728572841733694\n",
      "step: 1963, loss: 0.01914883218705654\n",
      "step: 1964, loss: 0.008413990028202534\n",
      "step: 1965, loss: 0.024096917361021042\n",
      "step: 1966, loss: 0.03680480644106865\n",
      "step: 1967, loss: 0.07506471127271652\n",
      "step: 1968, loss: 0.04935771971940994\n",
      "step: 1969, loss: 0.013414326123893261\n",
      "step: 1970, loss: 0.03710347041487694\n",
      "step: 1971, loss: 0.17750833928585052\n",
      "step: 1972, loss: 0.008810538798570633\n",
      "step: 1973, loss: 0.004168862011283636\n",
      "step: 1974, loss: 0.06873836368322372\n",
      "step: 1975, loss: 0.04196058213710785\n",
      "step: 1976, loss: 0.11962708830833435\n",
      "step: 1977, loss: 0.018605975434184074\n",
      "step: 1978, loss: 0.006406097207218409\n",
      "step: 1979, loss: 0.011184222996234894\n",
      "step: 1980, loss: 0.039371613413095474\n",
      "step: 1981, loss: 0.07766415923833847\n",
      "step: 1982, loss: 0.04034166783094406\n",
      "step: 1983, loss: 0.019424546509981155\n",
      "step: 1984, loss: 0.05598336085677147\n",
      "step: 1985, loss: 0.0471542552113533\n",
      "step: 1986, loss: 0.025791728869080544\n",
      "step: 1987, loss: 0.049142587929964066\n",
      "step: 1988, loss: 0.14525659382343292\n",
      "step: 1989, loss: 0.01794433780014515\n",
      "step: 1990, loss: 0.09080236405134201\n",
      "step: 1991, loss: 0.07415268570184708\n",
      "step: 1992, loss: 0.042330656200647354\n",
      "step: 1993, loss: 0.11436916142702103\n",
      "step: 1994, loss: 0.0057998113334178925\n",
      "step: 1995, loss: 0.03122142143547535\n",
      "step: 1996, loss: 0.016141969710588455\n",
      "step: 1997, loss: 0.09246083348989487\n",
      "step: 1998, loss: 0.003362097078934312\n",
      "step: 1999, loss: 0.019870491698384285\n",
      "step: 2000, loss: 0.055749617516994476\n",
      "step: 2001, loss: 0.008420691825449467\n",
      "step: 2002, loss: 0.010103707201778889\n",
      "step: 2003, loss: 0.01578940451145172\n",
      "step: 2004, loss: 0.05750013515353203\n",
      "step: 2005, loss: 0.050066184252500534\n",
      "step: 2006, loss: 0.0006114576826803386\n",
      "step: 2007, loss: 0.05477924272418022\n",
      "step: 2008, loss: 0.007261231075972319\n",
      "step: 2009, loss: 0.06418959051370621\n",
      "step: 2010, loss: 0.0055341897532343864\n",
      "step: 2011, loss: 0.0518733449280262\n",
      "step: 2012, loss: 0.08094792068004608\n",
      "step: 2013, loss: 0.07667161524295807\n",
      "step: 2014, loss: 0.0028520734049379826\n",
      "step: 2015, loss: 0.0034550768323242664\n",
      "step: 2016, loss: 0.005257314536720514\n",
      "step: 2017, loss: 0.12168977409601212\n",
      "step: 2018, loss: 0.015909073874354362\n",
      "step: 2019, loss: 0.11604417115449905\n",
      "step: 2020, loss: 0.04892733693122864\n",
      "step: 2021, loss: 0.0033967141062021255\n",
      "step: 2022, loss: 0.035597559064626694\n",
      "step: 2023, loss: 0.12603455781936646\n",
      "step: 2024, loss: 0.11848992109298706\n",
      "step: 2025, loss: 0.04672591760754585\n",
      "step: 2026, loss: 0.04238808527588844\n",
      "step: 2027, loss: 0.004798185080289841\n",
      "step: 2028, loss: 0.07311603426933289\n",
      "step: 2029, loss: 0.08775333315134048\n",
      "step: 2030, loss: 0.051368046551942825\n",
      "step: 2031, loss: 0.06810525804758072\n",
      "step: 2032, loss: 0.048372235149145126\n",
      "step: 2033, loss: 0.10745666176080704\n",
      "step: 2034, loss: 0.02758627012372017\n",
      "step: 2035, loss: 0.008175917901098728\n",
      "step: 2036, loss: 0.022228240966796875\n",
      "step: 2037, loss: 0.005849706940352917\n",
      "step: 2038, loss: 0.052563294768333435\n",
      "step: 2039, loss: 0.07671062648296356\n",
      "step: 2040, loss: 0.05039922147989273\n",
      "step: 2041, loss: 0.044768884778022766\n",
      "step: 2042, loss: 0.04865982010960579\n",
      "step: 2043, loss: 0.05632508918642998\n",
      "step: 2044, loss: 0.053714919835329056\n",
      "step: 2045, loss: 0.06300526857376099\n",
      "step: 2046, loss: 0.005697018001228571\n",
      "step: 2047, loss: 0.05562693253159523\n",
      "step: 2048, loss: 0.04738892242312431\n",
      "step: 2049, loss: 0.0036184340715408325\n",
      "step: 2050, loss: 0.024767305701971054\n",
      "step: 2051, loss: 0.26182135939598083\n",
      "step: 2052, loss: 0.052561014890670776\n",
      "step: 2053, loss: 0.053989898413419724\n",
      "step: 2054, loss: 0.04059285297989845\n",
      "step: 2055, loss: 0.07847587764263153\n",
      "step: 2056, loss: 0.021410787478089333\n",
      "step: 2057, loss: 0.009011042304337025\n",
      "step: 2058, loss: 0.008567852899432182\n",
      "step: 2059, loss: 0.09271935373544693\n",
      "step: 2060, loss: 0.0838792696595192\n",
      "step: 2061, loss: 0.018955053761601448\n",
      "step: 2062, loss: 0.007435182109475136\n",
      "step: 2063, loss: 0.009001252241432667\n",
      "step: 2064, loss: 0.046102944761514664\n",
      "step: 2065, loss: 0.0072421180084347725\n",
      "step: 2066, loss: 0.0044307284988462925\n",
      "step: 2067, loss: 0.007991126738488674\n",
      "step: 2068, loss: 0.1900315135717392\n",
      "step: 2069, loss: 0.024639330804347992\n",
      "step: 2070, loss: 0.010666663758456707\n",
      "step: 2071, loss: 0.0009303548140451312\n",
      "step: 2072, loss: 0.012165362946689129\n",
      "step: 2073, loss: 0.018975405022501945\n",
      "step: 2074, loss: 0.07722511887550354\n",
      "step: 2075, loss: 0.0013021122431382537\n",
      "step: 2076, loss: 0.029674498364329338\n",
      "step: 2077, loss: 0.0026986426673829556\n",
      "step: 2078, loss: 0.04420924186706543\n",
      "step: 2079, loss: 0.19541135430335999\n",
      "step: 2080, loss: 0.04134741425514221\n",
      "step: 2081, loss: 0.0459095761179924\n",
      "step: 2082, loss: 0.007593670394271612\n",
      "step: 2083, loss: 0.03967343643307686\n",
      "step: 2084, loss: 0.01286584697663784\n",
      "step: 2085, loss: 0.04633094370365143\n",
      "step: 2086, loss: 0.010834083892405033\n",
      "step: 2087, loss: 0.08453957736492157\n",
      "step: 2088, loss: 0.009108960628509521\n",
      "step: 2089, loss: 0.005586437415331602\n",
      "step: 2090, loss: 0.10937626659870148\n",
      "step: 2091, loss: 0.052787650376558304\n",
      "step: 2092, loss: 0.008270728401839733\n",
      "step: 2093, loss: 0.0040926276706159115\n",
      "step: 2094, loss: 0.09910299628973007\n",
      "step: 2095, loss: 0.012467049062252045\n",
      "step: 2096, loss: 0.009790928103029728\n",
      "step: 2097, loss: 0.019172225147485733\n",
      "step: 2098, loss: 0.015408159233629704\n",
      "step: 2099, loss: 0.009892767295241356\n",
      "step: 2100, loss: 0.00968070887029171\n",
      "step: 2101, loss: 0.1009242981672287\n",
      "step: 2102, loss: 0.04763253033161163\n",
      "step: 2103, loss: 0.010187041945755482\n",
      "step: 2104, loss: 0.05287817865610123\n",
      "step: 2105, loss: 0.0645190104842186\n",
      "step: 2106, loss: 0.09865535795688629\n",
      "step: 2107, loss: 0.001794491894543171\n",
      "step: 2108, loss: 0.007947128266096115\n",
      "step: 2109, loss: 0.00969085842370987\n",
      "step: 2110, loss: 0.0062448871321976185\n",
      "step: 2111, loss: 0.06577084213495255\n",
      "step: 2112, loss: 0.04205828532576561\n",
      "step: 2113, loss: 0.04926757887005806\n",
      "step: 2114, loss: 0.004412244074046612\n",
      "step: 2115, loss: 0.0465611070394516\n",
      "step: 2116, loss: 0.002592216245830059\n",
      "step: 2117, loss: 0.03923698514699936\n",
      "step: 2118, loss: 0.08101129531860352\n",
      "step: 2119, loss: 0.05941582843661308\n",
      "step: 2120, loss: 0.008327126502990723\n",
      "step: 2121, loss: 0.03861745446920395\n",
      "step: 2122, loss: 0.006991776172071695\n",
      "step: 2123, loss: 0.04208299145102501\n",
      "step: 2124, loss: 0.06581416726112366\n",
      "step: 2125, loss: 0.04670976474881172\n",
      "step: 2126, loss: 0.006442629732191563\n",
      "step: 2127, loss: 0.0786530077457428\n",
      "step: 2128, loss: 0.007396685425192118\n",
      "step: 2129, loss: 0.0021807588636875153\n",
      "step: 2130, loss: 0.037571560591459274\n",
      "step: 2131, loss: 0.07621609419584274\n",
      "step: 2132, loss: 0.030553588643670082\n",
      "step: 2133, loss: 0.054834235459566116\n",
      "step: 2134, loss: 0.09335114806890488\n",
      "step: 2135, loss: 0.003346246201545\n",
      "step: 2136, loss: 0.0228220634162426\n",
      "step: 2137, loss: 0.02963504008948803\n",
      "step: 2138, loss: 0.006204680074006319\n",
      "step: 2139, loss: 0.009621411561965942\n",
      "step: 2140, loss: 0.056211452931165695\n",
      "step: 2141, loss: 0.006848956923931837\n",
      "step: 2142, loss: 0.006517969537526369\n",
      "step: 2143, loss: 0.046221278607845306\n",
      "step: 2144, loss: 0.017229299992322922\n",
      "step: 2145, loss: 0.009208803065121174\n",
      "step: 2146, loss: 0.014995105564594269\n",
      "step: 2147, loss: 0.003627209458500147\n",
      "step: 2148, loss: 0.004832522943615913\n",
      "step: 2149, loss: 0.07633798569440842\n",
      "step: 2150, loss: 0.0145424148067832\n",
      "step: 2151, loss: 0.11017587780952454\n",
      "step: 2152, loss: 0.007439989596605301\n",
      "step: 2153, loss: 0.0030929085332900286\n",
      "step: 2154, loss: 0.008523361757397652\n",
      "step: 2155, loss: 0.010588166303932667\n",
      "step: 2156, loss: 0.03430251404643059\n",
      "step: 2157, loss: 0.04221378266811371\n",
      "step: 2158, loss: 0.009782864712178707\n",
      "step: 2159, loss: 0.006017909850925207\n",
      "step: 2160, loss: 0.007822465151548386\n",
      "step: 2161, loss: 0.003417189931496978\n",
      "step: 2162, loss: 0.1053374856710434\n",
      "step: 2163, loss: 0.14905720949172974\n",
      "step: 2164, loss: 0.051718272268772125\n",
      "step: 2165, loss: 0.0045714606530964375\n",
      "step: 2166, loss: 0.006944867316633463\n",
      "step: 2167, loss: 0.01397773064672947\n",
      "step: 2168, loss: 0.09131874889135361\n",
      "step: 2169, loss: 0.002698250114917755\n",
      "step: 2170, loss: 0.049950066953897476\n",
      "step: 2171, loss: 0.00606223801150918\n",
      "step: 2172, loss: 0.01899915747344494\n",
      "step: 2173, loss: 0.09188371896743774\n",
      "step: 2174, loss: 0.007981020957231522\n",
      "step: 2175, loss: 0.03669747710227966\n",
      "step: 2176, loss: 0.005071207415312529\n",
      "step: 2177, loss: 0.005782108288258314\n",
      "step: 2178, loss: 0.013747305609285831\n",
      "step: 2179, loss: 0.02804606221616268\n",
      "step: 2180, loss: 0.004237762186676264\n",
      "step: 2181, loss: 0.004844885319471359\n",
      "step: 2182, loss: 0.03778133541345596\n",
      "step: 2183, loss: 0.004138095770031214\n",
      "step: 2184, loss: 0.011374524794518948\n",
      "step: 2185, loss: 0.031348880380392075\n",
      "step: 2186, loss: 0.08454597741365433\n",
      "step: 2187, loss: 0.12604252994060516\n",
      "step: 2188, loss: 0.6797583699226379\n",
      "step: 2189, loss: 0.043957896530628204\n",
      "step: 2190, loss: 0.2806982398033142\n",
      "step: 2191, loss: 0.010190482251346111\n",
      "step: 2192, loss: 0.05899229645729065\n",
      "step: 2193, loss: 0.09058362990617752\n",
      "step: 2194, loss: 0.003898600349202752\n",
      "step: 2195, loss: 0.09782501310110092\n",
      "step: 2196, loss: 0.03251088783144951\n",
      "step: 2197, loss: 0.09163055568933487\n",
      "step: 2198, loss: 0.010166252963244915\n",
      "step: 2199, loss: 0.06474754214286804\n",
      "step: 2200, loss: 0.11831900477409363\n",
      "step: 2201, loss: 0.03698057308793068\n",
      "step: 2202, loss: 0.007019529119133949\n",
      "step: 2203, loss: 0.04412342235445976\n",
      "step: 2204, loss: 0.07718631625175476\n",
      "step: 2205, loss: 0.012920920737087727\n",
      "step: 2206, loss: 0.02750604785978794\n",
      "step: 2207, loss: 0.14205144345760345\n",
      "step: 2208, loss: 0.008075131103396416\n",
      "step: 2209, loss: 0.0332774855196476\n",
      "step: 2210, loss: 0.004857111722230911\n",
      "step: 2211, loss: 0.0138087784871459\n",
      "step: 2212, loss: 0.145952507853508\n",
      "step: 2213, loss: 0.0282159224152565\n",
      "step: 2214, loss: 0.035460904240608215\n",
      "step: 2215, loss: 0.008308550342917442\n",
      "step: 2216, loss: 0.0038271010853350163\n",
      "step: 2217, loss: 0.055329836905002594\n",
      "step: 2218, loss: 0.008724216371774673\n",
      "step: 2219, loss: 0.04968862980604172\n",
      "step: 2220, loss: 0.03778285160660744\n",
      "step: 2221, loss: 0.13513167202472687\n",
      "step: 2222, loss: 0.051183637231588364\n",
      "step: 2223, loss: 0.008742887526750565\n",
      "step: 2224, loss: 0.008040954358875751\n",
      "step: 2225, loss: 0.013396032154560089\n",
      "step: 2226, loss: 0.10368182510137558\n",
      "step: 2227, loss: 0.04186506196856499\n",
      "step: 2228, loss: 0.05592530220746994\n",
      "step: 2229, loss: 0.00989044364541769\n",
      "step: 2230, loss: 0.007976999506354332\n",
      "step: 2231, loss: 0.0033583836629986763\n",
      "step: 2232, loss: 0.03348558768630028\n",
      "step: 2233, loss: 0.14190681278705597\n",
      "step: 2234, loss: 0.0054488033056259155\n",
      "step: 2235, loss: 0.04194009304046631\n",
      "step: 2236, loss: 0.052008677273988724\n",
      "step: 2237, loss: 0.037966012954711914\n",
      "step: 2238, loss: 0.059040576219558716\n",
      "step: 2239, loss: 0.011449553072452545\n",
      "step: 2240, loss: 0.005675188731402159\n",
      "step: 2241, loss: 0.04543722793459892\n",
      "step: 2242, loss: 0.014571473933756351\n",
      "step: 2243, loss: 0.061736173927783966\n",
      "step: 2244, loss: 0.041170358657836914\n",
      "step: 2245, loss: 0.005750160198658705\n",
      "step: 2246, loss: 0.004299700725823641\n",
      "step: 2247, loss: 0.118374764919281\n",
      "step: 2248, loss: 0.03850286453962326\n",
      "step: 2249, loss: 0.015141386538743973\n",
      "step: 2250, loss: 0.013529658317565918\n",
      "step: 2251, loss: 0.03602609410881996\n",
      "step: 2252, loss: 0.0062176804058253765\n",
      "step: 2253, loss: 0.03369636833667755\n",
      "step: 2254, loss: 0.006652868818491697\n",
      "step: 2255, loss: 0.018356626853346825\n",
      "step: 2256, loss: 0.04041915014386177\n",
      "step: 2257, loss: 0.13178187608718872\n",
      "step: 2258, loss: 0.003367306664586067\n",
      "step: 2259, loss: 0.04352959990501404\n",
      "step: 2260, loss: 0.06717713177204132\n",
      "step: 2261, loss: 0.05269579589366913\n",
      "step: 2262, loss: 0.045999061316251755\n",
      "step: 2263, loss: 0.007109128404408693\n",
      "step: 2264, loss: 0.003408363088965416\n",
      "step: 2265, loss: 0.12276271730661392\n",
      "step: 2266, loss: 0.006706989370286465\n",
      "step: 2267, loss: 0.10414901375770569\n",
      "step: 2268, loss: 0.01316092535853386\n",
      "step: 2269, loss: 0.03002568520605564\n",
      "step: 2270, loss: 0.0008252934203483164\n",
      "step: 2271, loss: 0.004156217444688082\n",
      "step: 2272, loss: 0.003928795922547579\n",
      "step: 2273, loss: 0.0030675027519464493\n",
      "step: 2274, loss: 0.060294896364212036\n",
      "step: 2275, loss: 0.0026435693725943565\n",
      "step: 2276, loss: 0.0042245774529874325\n",
      "step: 2277, loss: 0.1534349024295807\n",
      "step: 2278, loss: 0.00507235387340188\n",
      "step: 2279, loss: 0.004788609687238932\n",
      "step: 2280, loss: 0.021090632304549217\n",
      "step: 2281, loss: 0.049378346651792526\n",
      "step: 2282, loss: 0.002232994418591261\n",
      "step: 2283, loss: 0.012321936897933483\n",
      "step: 2284, loss: 0.008709926158189774\n",
      "step: 2285, loss: 0.12288251519203186\n",
      "step: 2286, loss: 0.010927663184702396\n",
      "step: 2287, loss: 0.15768815577030182\n",
      "step: 2288, loss: 0.015963302925229073\n",
      "step: 2289, loss: 0.012886589393019676\n",
      "step: 2290, loss: 0.04714798927307129\n",
      "step: 2291, loss: 0.13849566876888275\n",
      "step: 2292, loss: 0.0074083139188587666\n",
      "step: 2293, loss: 0.050656452775001526\n",
      "step: 2294, loss: 0.14278869330883026\n",
      "step: 2295, loss: 0.007873386144638062\n",
      "step: 2296, loss: 0.04105401411652565\n",
      "step: 2297, loss: 0.06635458767414093\n",
      "step: 2298, loss: 0.005074169486761093\n",
      "step: 2299, loss: 0.05011270195245743\n",
      "step: 2300, loss: 0.03545767813920975\n",
      "step: 2301, loss: 0.010998832993209362\n",
      "step: 2302, loss: 0.09522972255945206\n",
      "step: 2303, loss: 0.05058804526925087\n",
      "step: 2304, loss: 0.01146323699504137\n",
      "step: 2305, loss: 0.06257060170173645\n",
      "step: 2306, loss: 0.045099612325429916\n",
      "step: 2307, loss: 0.007185152266174555\n",
      "step: 2308, loss: 0.00046272919280454516\n",
      "step: 2309, loss: 0.009636922739446163\n",
      "step: 2310, loss: 0.010669911280274391\n",
      "step: 2311, loss: 0.026751460507512093\n",
      "step: 2312, loss: 0.00376727432012558\n",
      "step: 2313, loss: 0.07115664333105087\n",
      "step: 2314, loss: 0.05594492331147194\n",
      "step: 2315, loss: 0.03085976652801037\n",
      "step: 2316, loss: 0.00894420500844717\n",
      "step: 2317, loss: 0.006175689864903688\n",
      "step: 2318, loss: 0.0016821727622300386\n",
      "step: 2319, loss: 0.06597241759300232\n",
      "step: 2320, loss: 0.00023600358690600842\n",
      "step: 2321, loss: 0.022514361888170242\n",
      "step: 2322, loss: 0.012106790207326412\n",
      "step: 2323, loss: 0.008871019817888737\n",
      "step: 2324, loss: 0.006853132043033838\n",
      "step: 2325, loss: 0.10642875730991364\n",
      "step: 2326, loss: 0.011529218405485153\n",
      "step: 2327, loss: 0.02727898396551609\n",
      "step: 2328, loss: 0.005588248837739229\n",
      "step: 2329, loss: 0.055592428892850876\n",
      "step: 2330, loss: 0.008576717227697372\n",
      "step: 2331, loss: 0.00840322207659483\n",
      "step: 2332, loss: 0.10347750037908554\n",
      "step: 2333, loss: 0.029505208134651184\n",
      "step: 2334, loss: 0.013863983564078808\n",
      "step: 2335, loss: 0.039878588169813156\n",
      "step: 2336, loss: 0.013606815598905087\n",
      "step: 2337, loss: 0.034047022461891174\n",
      "step: 2338, loss: 0.01364600658416748\n",
      "step: 2339, loss: 0.0028302655555307865\n",
      "step: 2340, loss: 0.003426463110372424\n",
      "step: 2341, loss: 0.009929669089615345\n",
      "step: 2342, loss: 0.007683671545237303\n",
      "step: 2343, loss: 0.024854378774762154\n",
      "step: 2344, loss: 0.006668846122920513\n",
      "step: 2345, loss: 0.042765066027641296\n",
      "step: 2346, loss: 0.0041464539244771\n",
      "step: 2347, loss: 0.002832847647368908\n",
      "step: 2348, loss: 0.08320361375808716\n",
      "step: 2349, loss: 0.01780051551759243\n",
      "step: 2350, loss: 0.003742028959095478\n",
      "step: 2351, loss: 0.046872492879629135\n",
      "step: 2352, loss: 0.0463416613638401\n",
      "step: 2353, loss: 0.03927043452858925\n",
      "step: 2354, loss: 0.002247082069516182\n",
      "step: 2355, loss: 0.006178857758641243\n",
      "step: 2356, loss: 0.010079874657094479\n",
      "step: 2357, loss: 0.008700599893927574\n",
      "step: 2358, loss: 0.05777479335665703\n",
      "step: 2359, loss: 0.08446343243122101\n",
      "step: 2360, loss: 0.003758061910048127\n",
      "step: 2361, loss: 0.019914820790290833\n",
      "step: 2362, loss: 0.1328624188899994\n",
      "step: 2363, loss: 0.05791277065873146\n",
      "step: 2364, loss: 0.06131494417786598\n",
      "step: 2365, loss: 0.0065692407079041\n",
      "step: 2366, loss: 0.006440548691898584\n",
      "step: 2367, loss: 0.0055174981243908405\n",
      "step: 2368, loss: 0.1029735580086708\n",
      "step: 2369, loss: 0.00447533605620265\n",
      "step: 2370, loss: 0.0036829106975346804\n",
      "step: 2371, loss: 0.05196169391274452\n",
      "step: 2372, loss: 0.006324732676148415\n",
      "step: 2373, loss: 0.0056289988569915295\n",
      "step: 2374, loss: 0.027172286063432693\n",
      "step: 2375, loss: 0.046762071549892426\n",
      "step: 2376, loss: 0.0019221333786845207\n",
      "step: 2377, loss: 0.14734695851802826\n",
      "step: 2378, loss: 0.002857186133041978\n",
      "step: 2379, loss: 0.006016343832015991\n",
      "step: 2380, loss: 0.006576084066182375\n",
      "step: 2381, loss: 0.004810917191207409\n",
      "step: 2382, loss: 0.004103460814803839\n",
      "step: 2383, loss: 0.09169095754623413\n",
      "step: 2384, loss: 0.058239925652742386\n",
      "step: 2385, loss: 0.005009170155972242\n",
      "step: 2386, loss: 0.007921823300421238\n",
      "step: 2387, loss: 0.006735505536198616\n",
      "step: 2388, loss: 0.009223992004990578\n",
      "step: 2389, loss: 0.048239436000585556\n",
      "step: 2390, loss: 0.003371533239260316\n",
      "step: 2391, loss: 0.009775541722774506\n",
      "step: 2392, loss: 0.06035434827208519\n",
      "step: 2393, loss: 0.036690808832645416\n",
      "step: 2394, loss: 0.002553703263401985\n",
      "step: 2395, loss: 0.04027651622891426\n",
      "step: 2396, loss: 0.12485181540250778\n",
      "step: 2397, loss: 0.04184099659323692\n",
      "step: 2398, loss: 0.09411725401878357\n",
      "step: 2399, loss: 0.005968817509710789\n",
      "step: 2400, loss: 0.08314840495586395\n",
      "step: 2401, loss: 0.012183126993477345\n",
      "step: 2402, loss: 0.05696450546383858\n",
      "step: 2403, loss: 0.1507316678762436\n",
      "step: 2404, loss: 0.06472165882587433\n",
      "step: 2405, loss: 0.04254046455025673\n",
      "step: 2406, loss: 0.016689063981175423\n",
      "step: 2407, loss: 0.08975482732057571\n",
      "step: 2408, loss: 0.008887228555977345\n",
      "step: 2409, loss: 0.07368085533380508\n",
      "step: 2410, loss: 0.0790288895368576\n",
      "step: 2411, loss: 0.004459701012820005\n",
      "step: 2412, loss: 0.004656792152673006\n",
      "step: 2413, loss: 0.03662911430001259\n",
      "step: 2414, loss: 0.0275444146245718\n",
      "step: 2415, loss: 0.055082887411117554\n",
      "step: 2416, loss: 0.052234627306461334\n",
      "step: 2417, loss: 0.019167838618159294\n",
      "step: 2418, loss: 0.004806201905012131\n",
      "step: 2419, loss: 0.005075827240943909\n",
      "step: 2420, loss: 0.0034511731937527657\n",
      "step: 2421, loss: 0.006200593896210194\n",
      "step: 2422, loss: 0.07130692154169083\n",
      "step: 2423, loss: 0.028563058003783226\n",
      "step: 2424, loss: 0.008613687008619308\n",
      "step: 2425, loss: 0.01901058666408062\n",
      "step: 2426, loss: 0.006257293280214071\n",
      "step: 2427, loss: 0.020265905186533928\n",
      "step: 2428, loss: 0.04272075742483139\n",
      "step: 2429, loss: 0.13690251111984253\n",
      "step: 2430, loss: 0.031664300709962845\n",
      "step: 2431, loss: 0.049352969974279404\n",
      "step: 2432, loss: 0.0038349225651472807\n",
      "step: 2433, loss: 0.010445820167660713\n",
      "step: 2434, loss: 0.052002713084220886\n",
      "step: 2435, loss: 0.006371465511620045\n",
      "step: 2436, loss: 0.013639255426824093\n",
      "step: 2437, loss: 0.06434112042188644\n",
      "step: 2438, loss: 0.062230147421360016\n",
      "step: 2439, loss: 0.020602302625775337\n",
      "step: 2440, loss: 0.11616324633359909\n",
      "step: 2441, loss: 0.0046255034394562244\n",
      "step: 2442, loss: 0.021405082195997238\n",
      "step: 2443, loss: 0.012055601924657822\n",
      "step: 2444, loss: 0.006839939393103123\n",
      "step: 2445, loss: 0.1059330478310585\n",
      "step: 2446, loss: 0.01201750710606575\n",
      "step: 2447, loss: 0.06589846312999725\n",
      "step: 2448, loss: 0.015497643500566483\n",
      "step: 2449, loss: 0.01010193582624197\n",
      "step: 2450, loss: 0.0631052702665329\n",
      "step: 2451, loss: 0.13543930649757385\n",
      "step: 2452, loss: 0.0028268038295209408\n",
      "step: 2453, loss: 0.03868793323636055\n",
      "step: 2454, loss: 0.04687414690852165\n",
      "step: 2455, loss: 0.009009326808154583\n",
      "step: 2456, loss: 0.029287287965416908\n",
      "step: 2457, loss: 0.26901161670684814\n",
      "step: 2458, loss: 0.041902847588062286\n",
      "step: 2459, loss: 0.01928568072617054\n",
      "step: 2460, loss: 0.009197644889354706\n",
      "step: 2461, loss: 0.007739596534520388\n",
      "step: 2462, loss: 0.004736193921416998\n",
      "step: 2463, loss: 0.005531023722141981\n",
      "step: 2464, loss: 0.05263153463602066\n",
      "step: 2465, loss: 0.0009307283326052129\n",
      "step: 2466, loss: 0.005674195941537619\n",
      "step: 2467, loss: 0.0068098269402980804\n",
      "step: 2468, loss: 0.0037453770637512207\n",
      "step: 2469, loss: 0.04944036155939102\n",
      "step: 2470, loss: 0.037715669721364975\n",
      "step: 2471, loss: 0.0021780754905194044\n",
      "step: 2472, loss: 0.030399415642023087\n",
      "step: 2473, loss: 0.04382941871881485\n",
      "step: 2474, loss: 0.08003915846347809\n",
      "step: 2475, loss: 0.009401856921613216\n",
      "step: 2476, loss: 0.003701828420162201\n",
      "step: 2477, loss: 0.004795591812580824\n",
      "step: 2478, loss: 0.05021194368600845\n",
      "step: 2479, loss: 0.0022813058458268642\n",
      "step: 2480, loss: 0.047143708914518356\n",
      "step: 2481, loss: 0.007531210780143738\n",
      "step: 2482, loss: 0.05565152317285538\n",
      "step: 2483, loss: 0.08692840486764908\n",
      "step: 2484, loss: 0.03624260798096657\n",
      "step: 2485, loss: 0.006536579690873623\n",
      "step: 2486, loss: 0.012266366742551327\n",
      "step: 2487, loss: 0.010866748169064522\n",
      "step: 2488, loss: 0.006601280998438597\n",
      "step: 2489, loss: 0.033608704805374146\n",
      "step: 2490, loss: 0.00691697932779789\n",
      "step: 2491, loss: 0.04567683860659599\n",
      "step: 2492, loss: 0.008799446746706963\n",
      "step: 2493, loss: 0.10331394523382187\n",
      "step: 2494, loss: 0.03506369888782501\n",
      "step: 2495, loss: 0.0019636282231658697\n",
      "step: 2496, loss: 0.00736627820879221\n",
      "step: 2497, loss: 0.1434216946363449\n",
      "step: 2498, loss: 0.048925258219242096\n",
      "step: 2499, loss: 0.005626338068395853\n",
      "step: 2500, loss: 0.04472746327519417\n",
      "step: 2501, loss: 0.07686007767915726\n",
      "step: 2502, loss: 0.000424012920120731\n",
      "step: 2503, loss: 0.04836643859744072\n",
      "step: 2504, loss: 0.058367326855659485\n",
      "step: 2505, loss: 0.10170673578977585\n",
      "step: 2506, loss: 0.005527779925614595\n",
      "step: 2507, loss: 0.016603561118245125\n",
      "step: 2508, loss: 0.0972432866692543\n",
      "step: 2509, loss: 0.003669286146759987\n",
      "step: 2510, loss: 0.003349139355123043\n",
      "step: 2511, loss: 0.0073745762929320335\n",
      "step: 2512, loss: 0.008478266187012196\n",
      "step: 2513, loss: 0.009274282492697239\n",
      "step: 2514, loss: 0.0362936332821846\n",
      "step: 2515, loss: 0.09695326536893845\n",
      "step: 2516, loss: 0.008292832411825657\n",
      "step: 2517, loss: 0.0007593591581098735\n",
      "step: 2518, loss: 0.0633876770734787\n",
      "step: 2519, loss: 0.039742209017276764\n",
      "step: 2520, loss: 0.04078056663274765\n",
      "step: 2521, loss: 0.0770922377705574\n",
      "step: 2522, loss: 0.0055633652955293655\n",
      "step: 2523, loss: 0.04133846238255501\n",
      "step: 2524, loss: 0.0020327947568148375\n",
      "step: 2525, loss: 0.004867931827902794\n",
      "step: 2526, loss: 0.0043046437203884125\n",
      "step: 2527, loss: 0.8587684631347656\n",
      "step: 2528, loss: 0.008313584141433239\n",
      "step: 2529, loss: 0.011521194130182266\n",
      "step: 2530, loss: 0.0083510996773839\n",
      "step: 2531, loss: 0.010856210254132748\n",
      "step: 2532, loss: 0.0030406436417251825\n",
      "step: 2533, loss: 0.09899298846721649\n",
      "step: 2534, loss: 0.03743597865104675\n",
      "step: 2535, loss: 0.06524963676929474\n",
      "step: 2536, loss: 0.008416163735091686\n",
      "step: 2537, loss: 0.006302875932306051\n",
      "step: 2538, loss: 0.009278791956603527\n",
      "step: 2539, loss: 0.004937568213790655\n",
      "step: 2540, loss: 0.07925767451524734\n",
      "step: 2541, loss: 0.009845164604485035\n",
      "step: 2542, loss: 0.0031206451822072268\n",
      "step: 2543, loss: 0.004006783943623304\n",
      "step: 2544, loss: 0.052395325154066086\n",
      "step: 2545, loss: 0.022952590137720108\n",
      "step: 2546, loss: 0.059965070337057114\n",
      "step: 2547, loss: 0.00991514790803194\n",
      "step: 2548, loss: 0.05704392120242119\n",
      "step: 2549, loss: 0.11766904592514038\n",
      "step: 2550, loss: 0.000567098380997777\n",
      "step: 2551, loss: 0.0418323278427124\n",
      "step: 2552, loss: 0.005705816671252251\n",
      "step: 2553, loss: 0.005440976470708847\n",
      "step: 2554, loss: 0.007543865591287613\n",
      "step: 2555, loss: 0.05102911964058876\n",
      "step: 2556, loss: 0.003971639554947615\n",
      "step: 2557, loss: 0.0027523613534867764\n",
      "step: 2558, loss: 0.004006721079349518\n",
      "step: 2559, loss: 0.01674610935151577\n",
      "step: 2560, loss: 0.04527650400996208\n",
      "step: 2561, loss: 0.07400450110435486\n",
      "step: 2562, loss: 0.018089352175593376\n",
      "step: 2563, loss: 0.05922767147421837\n",
      "step: 2564, loss: 0.0027751848101615906\n",
      "step: 2565, loss: 0.09332534670829773\n",
      "step: 2566, loss: 0.008681649342179298\n",
      "step: 2567, loss: 0.0540965273976326\n",
      "step: 2568, loss: 0.04106156900525093\n",
      "step: 2569, loss: 0.042361922562122345\n",
      "step: 2570, loss: 0.01344841718673706\n",
      "step: 2571, loss: 0.06218060851097107\n",
      "step: 2572, loss: 0.004420969169586897\n",
      "step: 2573, loss: 0.05123540014028549\n",
      "step: 2574, loss: 0.009098975919187069\n",
      "step: 2575, loss: 0.005704400595277548\n",
      "step: 2576, loss: 0.05719552934169769\n",
      "step: 2577, loss: 0.07582415640354156\n",
      "step: 2578, loss: 0.008650597184896469\n",
      "step: 2579, loss: 0.006375215016305447\n",
      "step: 2580, loss: 0.0016460183542221785\n",
      "step: 2581, loss: 0.03755660355091095\n",
      "step: 2582, loss: 0.0072767650708556175\n",
      "step: 2583, loss: 0.004625100642442703\n",
      "step: 2584, loss: 0.03528207913041115\n",
      "step: 2585, loss: 0.0011637319112196565\n",
      "step: 2586, loss: 0.011054395698010921\n",
      "step: 2587, loss: 0.002408204134553671\n",
      "step: 2588, loss: 0.06206388771533966\n",
      "step: 2589, loss: 0.009930106811225414\n",
      "step: 2590, loss: 0.029479216784238815\n",
      "step: 2591, loss: 0.010048734955489635\n",
      "step: 2592, loss: 0.03563718870282173\n",
      "step: 2593, loss: 0.11229650676250458\n",
      "step: 2594, loss: 0.008029668591916561\n",
      "step: 2595, loss: 0.01428199652582407\n",
      "step: 2596, loss: 0.0272557120770216\n",
      "step: 2597, loss: 0.007067942526191473\n",
      "step: 2598, loss: 0.005094046704471111\n",
      "step: 2599, loss: 0.04172617569565773\n",
      "step: 2600, loss: 0.012051785364747047\n",
      "step: 2601, loss: 0.0294274240732193\n",
      "step: 2602, loss: 0.0019864377100020647\n",
      "step: 2603, loss: 0.007231080438941717\n",
      "step: 2604, loss: 0.027702663093805313\n",
      "step: 2605, loss: 0.04729427769780159\n",
      "step: 2606, loss: 0.00605028634890914\n",
      "step: 2607, loss: 0.023508235812187195\n",
      "step: 2608, loss: 0.015745460987091064\n",
      "step: 2609, loss: 0.09465261548757553\n",
      "step: 2610, loss: 0.00336138135753572\n",
      "step: 2611, loss: 0.003275475697591901\n",
      "step: 2612, loss: 0.004764360375702381\n",
      "step: 2613, loss: 0.007645438425242901\n",
      "step: 2614, loss: 0.0029072752222418785\n",
      "step: 2615, loss: 0.025301015004515648\n",
      "step: 2616, loss: 0.06823594868183136\n",
      "step: 2617, loss: 0.01061252597719431\n",
      "step: 2618, loss: 0.0035719037987291813\n",
      "step: 2619, loss: 0.0037266977597028017\n",
      "step: 2620, loss: 0.0006633892771787941\n",
      "step: 2621, loss: 0.023480329662561417\n",
      "step: 2622, loss: 0.04128957912325859\n",
      "step: 2623, loss: 0.035821639001369476\n",
      "step: 2624, loss: 0.05866396427154541\n",
      "step: 2625, loss: 0.14574502408504486\n",
      "step: 2626, loss: 0.004590123426169157\n",
      "step: 2627, loss: 0.005974688567221165\n",
      "step: 2628, loss: 0.09512307494878769\n",
      "step: 2629, loss: 0.07636935263872147\n",
      "step: 2630, loss: 0.0899093821644783\n",
      "step: 2631, loss: 0.05491520091891289\n",
      "step: 2632, loss: 0.0047575668431818485\n",
      "step: 2633, loss: 0.06412451714277267\n",
      "step: 2634, loss: 0.05297939479351044\n",
      "step: 2635, loss: 0.006060529965907335\n",
      "step: 2636, loss: 0.044193610548973083\n",
      "step: 2637, loss: 0.04699667915701866\n",
      "step: 2638, loss: 0.032407455146312714\n",
      "step: 2639, loss: 0.003453546669334173\n",
      "step: 2640, loss: 0.010935814119875431\n",
      "step: 2641, loss: 0.003326424863189459\n",
      "step: 2642, loss: 0.13808420300483704\n",
      "step: 2643, loss: 0.08115878701210022\n",
      "step: 2644, loss: 0.03828014060854912\n",
      "step: 2645, loss: 0.0047505260445177555\n",
      "step: 2646, loss: 0.06731948256492615\n",
      "step: 2647, loss: 0.034947581589221954\n",
      "step: 2648, loss: 0.006612198427319527\n",
      "step: 2649, loss: 0.0019627646543085575\n",
      "step: 2650, loss: 0.002223439747467637\n",
      "step: 2651, loss: 0.011164878495037556\n",
      "step: 2652, loss: 0.04158889129757881\n",
      "step: 2653, loss: 0.017187323421239853\n",
      "step: 2654, loss: 0.0646786168217659\n",
      "step: 2655, loss: 0.22227342426776886\n",
      "step: 2656, loss: 0.0869332030415535\n",
      "step: 2657, loss: 0.002710237167775631\n",
      "step: 2658, loss: 0.009003912098705769\n",
      "step: 2659, loss: 0.013844073750078678\n",
      "step: 2660, loss: 0.06722157448530197\n",
      "step: 2661, loss: 0.08735060691833496\n",
      "step: 2662, loss: 0.011116057634353638\n",
      "step: 2663, loss: 0.04015029966831207\n",
      "step: 2664, loss: 0.04446197673678398\n",
      "step: 2665, loss: 0.003966968040913343\n",
      "step: 2666, loss: 0.04614246264100075\n",
      "step: 2667, loss: 0.014275171793997288\n",
      "step: 2668, loss: 0.048728376626968384\n",
      "step: 2669, loss: 0.038170598447322845\n",
      "step: 2670, loss: 0.0028536897152662277\n",
      "step: 2671, loss: 0.001356886001303792\n",
      "step: 2672, loss: 0.05936761200428009\n",
      "step: 2673, loss: 0.04337866231799126\n",
      "step: 2674, loss: 0.04042518138885498\n",
      "step: 2675, loss: 0.0024051093496382236\n",
      "step: 2676, loss: 0.0011224211193621159\n",
      "step: 2677, loss: 0.053345028311014175\n",
      "step: 2678, loss: 0.03286677226424217\n",
      "step: 2679, loss: 0.05536079779267311\n",
      "step: 2680, loss: 0.0678049623966217\n",
      "step: 2681, loss: 0.03839806467294693\n",
      "step: 2682, loss: 0.08948257565498352\n",
      "step: 2683, loss: 0.011672087013721466\n",
      "step: 2684, loss: 0.07984750717878342\n",
      "step: 2685, loss: 0.013825034722685814\n",
      "step: 2686, loss: 0.003603600664064288\n",
      "step: 2687, loss: 0.011079421266913414\n",
      "step: 2688, loss: 0.07902908325195312\n",
      "step: 2689, loss: 0.04005338251590729\n",
      "step: 2690, loss: 0.00959859136492014\n",
      "step: 2691, loss: 0.014528988860547543\n",
      "step: 2692, loss: 0.0046327984891831875\n",
      "step: 2693, loss: 0.03158193826675415\n",
      "step: 2694, loss: 0.033250369131565094\n",
      "step: 2695, loss: 0.008673877455294132\n",
      "step: 2696, loss: 0.005207775626331568\n",
      "step: 2697, loss: 0.23092803359031677\n",
      "step: 2698, loss: 0.046870965510606766\n",
      "step: 2699, loss: 0.0072705685161054134\n",
      "step: 2700, loss: 0.044317469000816345\n",
      "step: 2701, loss: 0.09540247172117233\n",
      "step: 2702, loss: 0.013031401671469212\n",
      "step: 2703, loss: 0.0076081049628555775\n",
      "step: 2704, loss: 0.0004861204943154007\n",
      "step: 2705, loss: 0.009899573400616646\n",
      "step: 2706, loss: 0.2486046552658081\n",
      "step: 2707, loss: 0.012671981006860733\n",
      "step: 2708, loss: 0.004455666057765484\n",
      "step: 2709, loss: 0.007425414863973856\n",
      "step: 2710, loss: 0.20259509980678558\n",
      "step: 2711, loss: 0.14091241359710693\n",
      "step: 2712, loss: 0.04607204720377922\n",
      "step: 2713, loss: 0.009067325852811337\n",
      "step: 2714, loss: 0.0034301362466067076\n",
      "step: 2715, loss: 0.052427567541599274\n",
      "step: 2716, loss: 0.012515540234744549\n",
      "step: 2717, loss: 0.004634886980056763\n",
      "step: 2718, loss: 0.012397739104926586\n",
      "step: 2719, loss: 0.04250861331820488\n",
      "step: 2720, loss: 0.00771349435672164\n",
      "step: 2721, loss: 0.009354477748274803\n",
      "step: 2722, loss: 0.052565935999155045\n",
      "step: 2723, loss: 0.039724815636873245\n",
      "step: 2724, loss: 0.004117825999855995\n",
      "step: 2725, loss: 0.0048410361632704735\n",
      "step: 2726, loss: 0.0037213226314634085\n",
      "step: 2727, loss: 0.04192396253347397\n",
      "step: 2728, loss: 0.041958753019571304\n",
      "step: 2729, loss: 0.1033191978931427\n",
      "step: 2730, loss: 0.005919463001191616\n",
      "step: 2731, loss: 0.06443151831626892\n",
      "step: 2732, loss: 0.0741107240319252\n",
      "step: 2733, loss: 0.0035135983489453793\n",
      "step: 2734, loss: 0.03788147494196892\n",
      "step: 2735, loss: 0.05548498034477234\n",
      "step: 2736, loss: 0.003685158444568515\n",
      "step: 2737, loss: 0.009254362434148788\n",
      "step: 2738, loss: 0.05372975766658783\n",
      "step: 2739, loss: 0.006334605161100626\n",
      "step: 2740, loss: 0.003858736250549555\n",
      "step: 2741, loss: 0.001442376640625298\n",
      "step: 2742, loss: 0.042119547724723816\n",
      "step: 2743, loss: 0.03965727984905243\n",
      "step: 2744, loss: 0.014648431912064552\n",
      "step: 2745, loss: 0.03500518202781677\n",
      "step: 2746, loss: 0.03780210390686989\n",
      "step: 2747, loss: 0.05304371193051338\n",
      "step: 2748, loss: 0.004963942803442478\n",
      "step: 2749, loss: 0.0004659485421143472\n",
      "step: 2750, loss: 0.003979406785219908\n",
      "step: 2751, loss: 0.0060759116895496845\n",
      "step: 2752, loss: 0.05427713319659233\n",
      "step: 2753, loss: 0.007458217907696962\n",
      "step: 2754, loss: 0.005483341868966818\n",
      "step: 2755, loss: 0.05495312809944153\n",
      "step: 2756, loss: 0.04529397189617157\n",
      "step: 2757, loss: 0.0514666847884655\n",
      "step: 2758, loss: 0.04239177331328392\n",
      "step: 2759, loss: 0.01262106653302908\n",
      "step: 2760, loss: 0.0002927358145825565\n",
      "step: 2761, loss: 0.04665973037481308\n",
      "step: 2762, loss: 0.0008453209884464741\n",
      "step: 2763, loss: 0.005062959156930447\n",
      "step: 2764, loss: 0.05097956582903862\n",
      "step: 2765, loss: 0.07807955145835876\n",
      "step: 2766, loss: 0.0015405328013002872\n",
      "step: 2767, loss: 0.06728139519691467\n",
      "step: 2768, loss: 0.007124943658709526\n",
      "step: 2769, loss: 0.07524770498275757\n",
      "step: 2770, loss: 0.024110984057188034\n",
      "step: 2771, loss: 0.047699347138404846\n",
      "step: 2772, loss: 0.04227445647120476\n",
      "step: 2773, loss: 0.009775872342288494\n",
      "step: 2774, loss: 0.007566916756331921\n",
      "step: 2775, loss: 0.05637617036700249\n",
      "step: 2776, loss: 0.005573272705078125\n",
      "step: 2777, loss: 0.01272802334278822\n",
      "step: 2778, loss: 0.0037543701473623514\n",
      "step: 2779, loss: 0.06949641555547714\n",
      "step: 2780, loss: 0.007069092243909836\n",
      "step: 2781, loss: 0.011547770351171494\n",
      "step: 2782, loss: 0.00564136216416955\n",
      "step: 2783, loss: 0.23553380370140076\n",
      "step: 2784, loss: 0.033776652067899704\n",
      "step: 2785, loss: 0.00999541487544775\n",
      "step: 2786, loss: 0.013148622587323189\n",
      "step: 2787, loss: 0.010823825374245644\n",
      "step: 2788, loss: 0.06463773548603058\n",
      "step: 2789, loss: 0.0030255033634603024\n",
      "step: 2790, loss: 0.011065984144806862\n",
      "step: 2791, loss: 0.06301773339509964\n",
      "step: 2792, loss: 0.2261924147605896\n",
      "step: 2793, loss: 0.013850578106939793\n",
      "step: 2794, loss: 0.050512515008449554\n",
      "step: 2795, loss: 0.04453861713409424\n",
      "step: 2796, loss: 0.005825909320265055\n",
      "step: 2797, loss: 0.047307729721069336\n",
      "step: 2798, loss: 0.06050541251897812\n",
      "step: 2799, loss: 0.026131263002753258\n",
      "step: 2800, loss: 0.02277945727109909\n",
      "step: 2801, loss: 0.0022519787307828665\n",
      "step: 2802, loss: 0.00915568694472313\n",
      "step: 2803, loss: 0.055896077305078506\n",
      "step: 2804, loss: 0.004065308719873428\n",
      "step: 2805, loss: 0.019917413592338562\n",
      "step: 2806, loss: 0.006223232951015234\n",
      "step: 2807, loss: 0.009745321236550808\n",
      "step: 2808, loss: 0.004077523946762085\n",
      "step: 2809, loss: 0.013188955374062061\n",
      "step: 2810, loss: 0.001139617757871747\n",
      "step: 2811, loss: 0.005236385855823755\n",
      "step: 2812, loss: 0.6041091680526733\n",
      "step: 2813, loss: 0.042806874960660934\n",
      "step: 2814, loss: 0.04053696617484093\n",
      "step: 2815, loss: 0.11720412224531174\n",
      "step: 2816, loss: 0.0057551185600459576\n",
      "step: 2817, loss: 0.05546065792441368\n",
      "step: 2818, loss: 0.0013373856199905276\n",
      "step: 2819, loss: 0.13041245937347412\n",
      "step: 2820, loss: 0.00670070294290781\n",
      "step: 2821, loss: 0.03909428417682648\n",
      "step: 2822, loss: 0.04180733114480972\n",
      "step: 2823, loss: 0.0024157860316336155\n",
      "step: 2824, loss: 0.005952831357717514\n",
      "step: 2825, loss: 0.12535561621189117\n",
      "step: 2826, loss: 0.05349821224808693\n",
      "step: 2827, loss: 0.047131385654211044\n",
      "step: 2828, loss: 0.03214651718735695\n",
      "step: 2829, loss: 0.08573510497808456\n",
      "step: 2830, loss: 0.004465755075216293\n",
      "step: 2831, loss: 0.0037604523822665215\n",
      "step: 2832, loss: 0.006508079823106527\n",
      "step: 2833, loss: 0.006072942167520523\n",
      "step: 2834, loss: 0.08493299782276154\n",
      "step: 2835, loss: 0.00817042589187622\n",
      "step: 2836, loss: 0.007924692705273628\n",
      "step: 2837, loss: 0.0064095910638570786\n",
      "step: 2838, loss: 0.004822435788810253\n",
      "step: 2839, loss: 0.05557142198085785\n",
      "step: 2840, loss: 0.05946595221757889\n",
      "step: 2841, loss: 0.021655261516571045\n",
      "step: 2842, loss: 0.006089574657380581\n",
      "step: 2843, loss: 0.008067621849477291\n",
      "step: 2844, loss: 0.05699361488223076\n",
      "step: 2845, loss: 0.009836067445576191\n",
      "step: 2846, loss: 0.20429807901382446\n",
      "step: 2847, loss: 0.005409061908721924\n",
      "step: 2848, loss: 0.0029754890128970146\n",
      "step: 2849, loss: 0.03662892431020737\n",
      "step: 2850, loss: 0.005094223655760288\n",
      "step: 2851, loss: 0.026481222361326218\n",
      "step: 2852, loss: 0.03816737234592438\n",
      "step: 2853, loss: 0.008040779270231724\n",
      "step: 2854, loss: 0.03181295096874237\n",
      "step: 2855, loss: 0.003472290001809597\n",
      "step: 2856, loss: 0.07951092720031738\n",
      "step: 2857, loss: 0.004295633640140295\n",
      "step: 2858, loss: 0.006363496650010347\n",
      "step: 2859, loss: 0.005882822908461094\n",
      "step: 2860, loss: 0.0006459215073846281\n",
      "step: 2861, loss: 0.001377531443722546\n",
      "step: 2862, loss: 0.05787744000554085\n",
      "step: 2863, loss: 0.03120408020913601\n",
      "step: 2864, loss: 0.06181032210588455\n",
      "step: 2865, loss: 0.06860921531915665\n",
      "step: 2866, loss: 0.0020833027083426714\n",
      "step: 2867, loss: 0.0068891835398972034\n",
      "step: 2868, loss: 0.036594707518815994\n",
      "step: 2869, loss: 0.0004472799482755363\n",
      "step: 2870, loss: 0.0021002120338380337\n",
      "step: 2871, loss: 0.008253233507275581\n",
      "step: 2872, loss: 0.0032606180757284164\n",
      "step: 2873, loss: 0.008743208833038807\n",
      "step: 2874, loss: 0.050618357956409454\n",
      "step: 2875, loss: 0.03431951254606247\n",
      "step: 2876, loss: 0.017476774752140045\n",
      "step: 2877, loss: 0.010588488541543484\n",
      "step: 2878, loss: 0.03527672961354256\n",
      "step: 2879, loss: 0.0661616101861\n",
      "step: 2880, loss: 0.0065613710321486\n",
      "step: 2881, loss: 0.0049060950987041\n",
      "step: 2882, loss: 0.07116583734750748\n",
      "step: 2883, loss: 0.08662330359220505\n",
      "step: 2884, loss: 0.239005446434021\n",
      "step: 2885, loss: 0.0016798716969788074\n",
      "step: 2886, loss: 0.00560902850702405\n",
      "step: 2887, loss: 0.004237890709191561\n",
      "step: 2888, loss: 0.022557757794857025\n",
      "step: 2889, loss: 0.0907837301492691\n",
      "step: 2890, loss: 0.017700424417853355\n",
      "step: 2891, loss: 0.002496051834896207\n",
      "step: 2892, loss: 0.000762111390940845\n",
      "step: 2893, loss: 0.002972259419038892\n",
      "step: 2894, loss: 0.013285703957080841\n",
      "step: 2895, loss: 0.005834910087287426\n",
      "step: 2896, loss: 0.04632249474525452\n",
      "step: 2897, loss: 0.03551613911986351\n",
      "step: 2898, loss: 0.00940502155572176\n",
      "step: 2899, loss: 0.002271369332447648\n",
      "step: 2900, loss: 0.035015203058719635\n",
      "step: 2901, loss: 0.06958674639463425\n",
      "step: 2902, loss: 0.005826096050441265\n",
      "step: 2903, loss: 0.06121046096086502\n",
      "step: 2904, loss: 0.0035871786531060934\n",
      "step: 2905, loss: 0.020681774243712425\n",
      "step: 2906, loss: 0.01790899597108364\n",
      "step: 2907, loss: 0.001867093495093286\n",
      "step: 2908, loss: 0.05460674688220024\n",
      "step: 2909, loss: 0.0508434921503067\n",
      "step: 2910, loss: 0.04224424809217453\n",
      "step: 2911, loss: 0.0034419477451592684\n",
      "step: 2912, loss: 0.05393048748373985\n",
      "step: 2913, loss: 0.00653028953820467\n",
      "step: 2914, loss: 0.004059291910380125\n",
      "step: 2915, loss: 0.037451181560754776\n",
      "step: 2916, loss: 0.003987700678408146\n",
      "step: 2917, loss: 0.06360870599746704\n",
      "step: 2918, loss: 0.09510406851768494\n",
      "step: 2919, loss: 0.012077562510967255\n",
      "step: 2920, loss: 0.006468004081398249\n",
      "step: 2921, loss: 0.0032605899032205343\n",
      "step: 2922, loss: 0.00775021081790328\n",
      "step: 2923, loss: 0.0019448940875008702\n",
      "step: 2924, loss: 0.05497375503182411\n",
      "step: 2925, loss: 0.11506502330303192\n",
      "step: 2926, loss: 0.002613491378724575\n",
      "step: 2927, loss: 0.04100567847490311\n",
      "step: 2928, loss: 0.007066587917506695\n",
      "step: 2929, loss: 0.003952380269765854\n",
      "step: 2930, loss: 0.006610362324863672\n",
      "step: 2931, loss: 0.1585012972354889\n",
      "step: 2932, loss: 0.06691904366016388\n",
      "step: 2933, loss: 0.004480577539652586\n",
      "step: 2934, loss: 0.09815964847803116\n",
      "step: 2935, loss: 0.0669490173459053\n",
      "step: 2936, loss: 0.02292488142848015\n",
      "step: 2937, loss: 0.0013856705045327544\n",
      "step: 2938, loss: 0.0032758191227912903\n",
      "step: 2939, loss: 0.006060827989131212\n",
      "step: 2940, loss: 0.006685191765427589\n",
      "step: 2941, loss: 0.08537789434194565\n",
      "step: 2942, loss: 0.006737728137522936\n",
      "step: 2943, loss: 0.056579090654850006\n",
      "step: 2944, loss: 0.041635170578956604\n",
      "step: 2945, loss: 0.019349142909049988\n",
      "step: 2946, loss: 0.05537872016429901\n",
      "step: 2947, loss: 0.042315155267715454\n",
      "step: 2948, loss: 0.0035839483607560396\n",
      "step: 2949, loss: 0.004190545529127121\n",
      "step: 2950, loss: 0.08014895766973495\n",
      "step: 2951, loss: 0.051851414144039154\n",
      "step: 2952, loss: 0.10533230006694794\n",
      "step: 2953, loss: 0.005640136543661356\n",
      "step: 2954, loss: 0.003027527127414942\n",
      "step: 2955, loss: 0.00787434633821249\n",
      "step: 2956, loss: 0.006378646939992905\n",
      "step: 2957, loss: 0.013582275249063969\n",
      "step: 2958, loss: 0.016712233424186707\n",
      "step: 2959, loss: 0.03786887601017952\n",
      "step: 2960, loss: 0.03385063633322716\n",
      "step: 2961, loss: 0.005463568028062582\n",
      "step: 2962, loss: 0.01340971514582634\n",
      "step: 2963, loss: 0.005137159954756498\n",
      "step: 2964, loss: 0.005456828977912664\n",
      "step: 2965, loss: 0.043686967343091965\n",
      "step: 2966, loss: 0.04050276800990105\n",
      "step: 2967, loss: 0.006132036913186312\n",
      "step: 2968, loss: 0.010590455494821072\n",
      "step: 2969, loss: 0.0553864985704422\n",
      "step: 2970, loss: 0.007262615952640772\n",
      "step: 2971, loss: 0.014113342389464378\n",
      "step: 2972, loss: 0.020656580105423927\n",
      "step: 2973, loss: 0.053404588252305984\n",
      "step: 2974, loss: 0.00420005340129137\n",
      "step: 2975, loss: 0.05146166682243347\n",
      "step: 2976, loss: 0.005287973675876856\n",
      "step: 2977, loss: 0.03138815239071846\n",
      "step: 2978, loss: 0.21770696341991425\n",
      "step: 2979, loss: 0.08592857420444489\n",
      "step: 2980, loss: 0.03824520483613014\n",
      "step: 2981, loss: 0.4842468202114105\n",
      "step: 2982, loss: 0.004433861002326012\n",
      "step: 2983, loss: 0.042726077139377594\n",
      "step: 2984, loss: 0.00839461013674736\n",
      "step: 2985, loss: 0.006318286061286926\n",
      "step: 2986, loss: 0.02429000660777092\n",
      "step: 2987, loss: 0.045130494982004166\n",
      "step: 2988, loss: 0.04607899487018585\n",
      "step: 2989, loss: 0.007462068926542997\n",
      "step: 2990, loss: 0.02953842654824257\n",
      "step: 2991, loss: 0.0018920681905001402\n",
      "step: 2992, loss: 0.0025939964689314365\n",
      "step: 2993, loss: 0.0046683792024850845\n",
      "step: 2994, loss: 0.006701917853206396\n",
      "step: 2995, loss: 0.012130048125982285\n",
      "step: 2996, loss: 0.0668884739279747\n",
      "step: 2997, loss: 0.01520276814699173\n",
      "step: 2998, loss: 0.004380214959383011\n",
      "step: 2999, loss: 0.005845450330525637\n",
      "step: 3000, loss: 0.007443899288773537\n",
      "step: 3001, loss: 0.008355384692549706\n",
      "step: 3002, loss: 0.05227521434426308\n",
      "step: 3003, loss: 0.20088793337345123\n",
      "step: 3004, loss: 0.08801767975091934\n",
      "step: 3005, loss: 0.0072204433381557465\n",
      "step: 3006, loss: 0.0156661719083786\n",
      "step: 3007, loss: 0.08340319991111755\n",
      "step: 3008, loss: 0.02938755787909031\n",
      "step: 3009, loss: 0.00652006221935153\n",
      "step: 3010, loss: 0.003264468628913164\n",
      "step: 3011, loss: 0.015945499762892723\n",
      "step: 3012, loss: 0.01693560928106308\n",
      "step: 3013, loss: 0.0077178822830319405\n",
      "step: 3014, loss: 0.04238034784793854\n",
      "step: 3015, loss: 0.018037039786577225\n",
      "step: 3016, loss: 0.004892114549875259\n",
      "step: 3017, loss: 0.021922338753938675\n",
      "step: 3018, loss: 0.04604212939739227\n",
      "step: 3019, loss: 0.04308922588825226\n",
      "step: 3020, loss: 0.04503561556339264\n",
      "step: 3021, loss: 0.008352807722985744\n",
      "step: 3022, loss: 0.010056103579699993\n",
      "step: 3023, loss: 0.0036003931891173124\n",
      "step: 3024, loss: 0.006207130383700132\n",
      "step: 3025, loss: 0.005786222871392965\n",
      "step: 3026, loss: 0.006816480308771133\n",
      "step: 3027, loss: 0.04685749113559723\n",
      "step: 3028, loss: 0.01620497554540634\n",
      "step: 3029, loss: 0.0021317561622709036\n",
      "step: 3030, loss: 0.0026490234304219484\n",
      "step: 3031, loss: 0.0011629937216639519\n",
      "step: 3032, loss: 0.02346588298678398\n",
      "step: 3033, loss: 0.007957084104418755\n",
      "step: 3034, loss: 0.043698616325855255\n",
      "step: 3035, loss: 0.0035762793850153685\n",
      "step: 3036, loss: 0.00654223095625639\n",
      "step: 3037, loss: 0.021361468359827995\n",
      "step: 3038, loss: 0.008741256780922413\n",
      "step: 3039, loss: 0.04608280211687088\n",
      "step: 3040, loss: 0.019974051043391228\n",
      "step: 3041, loss: 0.0024507062043994665\n",
      "step: 3042, loss: 0.007048199884593487\n",
      "step: 3043, loss: 0.004074264783412218\n",
      "step: 3044, loss: 0.29628050327301025\n",
      "step: 3045, loss: 0.0019570619333535433\n",
      "step: 3046, loss: 0.005682672373950481\n",
      "step: 3047, loss: 0.0020826279651373625\n",
      "step: 3048, loss: 0.01953510008752346\n",
      "step: 3049, loss: 0.02491549402475357\n",
      "step: 3050, loss: 0.008790317922830582\n",
      "step: 3051, loss: 0.04073628783226013\n",
      "step: 3052, loss: 0.03744847699999809\n",
      "step: 3053, loss: 0.03409193083643913\n",
      "step: 3054, loss: 0.03186844289302826\n",
      "step: 3055, loss: 0.03471913933753967\n",
      "step: 3056, loss: 0.04401051625609398\n",
      "step: 3057, loss: 0.07062724977731705\n",
      "step: 3058, loss: 0.035537585616111755\n",
      "step: 3059, loss: 0.005962665192782879\n",
      "step: 3060, loss: 0.04310814291238785\n",
      "step: 3061, loss: 0.033630724996328354\n",
      "step: 3062, loss: 0.05555383116006851\n",
      "step: 3063, loss: 0.0030639353208243847\n",
      "step: 3064, loss: 0.050096772611141205\n",
      "step: 3065, loss: 0.009397605434060097\n",
      "step: 3066, loss: 0.0687180683016777\n",
      "step: 3067, loss: 0.02125345729291439\n",
      "step: 3068, loss: 0.057520896196365356\n",
      "step: 3069, loss: 0.0035745257046073675\n",
      "step: 3070, loss: 0.0024439285043627024\n",
      "step: 3071, loss: 0.006868534721434116\n",
      "step: 3072, loss: 0.004698681645095348\n",
      "step: 3073, loss: 0.007423334289342165\n",
      "step: 3074, loss: 0.006537705194205046\n",
      "step: 3075, loss: 0.09684409946203232\n",
      "step: 3076, loss: 0.003994025755673647\n",
      "step: 3077, loss: 0.04913785308599472\n",
      "step: 3078, loss: 0.17328527569770813\n",
      "step: 3079, loss: 0.0036548448260873556\n",
      "step: 3080, loss: 0.00514911999925971\n",
      "step: 3081, loss: 0.01034387107938528\n",
      "step: 3082, loss: 0.0012849816121160984\n",
      "step: 3083, loss: 0.0017297097947448492\n",
      "step: 3084, loss: 0.010975196026265621\n",
      "step: 3085, loss: 0.03446359932422638\n",
      "step: 3086, loss: 0.002496967324987054\n",
      "step: 3087, loss: 0.0007165837450884283\n",
      "step: 3088, loss: 0.021054811775684357\n",
      "step: 3089, loss: 0.0027529525104910135\n",
      "step: 3090, loss: 0.06175171956419945\n",
      "step: 3091, loss: 0.006879774387925863\n",
      "step: 3092, loss: 0.012182733044028282\n",
      "step: 3093, loss: 0.011042728088796139\n",
      "step: 3094, loss: 0.04250412806868553\n",
      "step: 3095, loss: 0.006417014170438051\n",
      "step: 3096, loss: 0.009353229776024818\n",
      "step: 3097, loss: 0.03083590231835842\n",
      "step: 3098, loss: 0.0029096954967826605\n",
      "step: 3099, loss: 0.005053001455962658\n",
      "step: 3100, loss: 0.006977102719247341\n",
      "step: 3101, loss: 0.00810057669878006\n",
      "step: 3102, loss: 0.0028314166702330112\n",
      "step: 3103, loss: 0.041146401315927505\n",
      "step: 3104, loss: 0.06060433015227318\n",
      "step: 3105, loss: 0.00046854562242515385\n",
      "step: 3106, loss: 0.001377458916977048\n",
      "step: 3107, loss: 0.10112540423870087\n",
      "step: 3108, loss: 0.01518270280212164\n",
      "step: 3109, loss: 0.03110877424478531\n",
      "step: 3110, loss: 0.0013992408057674766\n",
      "step: 3111, loss: 0.05360754579305649\n",
      "step: 3112, loss: 0.005311311688274145\n",
      "step: 3113, loss: 0.03496779501438141\n",
      "step: 3114, loss: 0.007714490871876478\n",
      "step: 3115, loss: 0.07465167343616486\n",
      "step: 3116, loss: 0.006912651006132364\n",
      "step: 3117, loss: 0.13053040206432343\n",
      "step: 3118, loss: 0.000333089818013832\n",
      "step: 3119, loss: 0.004073380026966333\n",
      "step: 3120, loss: 0.008367794565856457\n",
      "step: 3121, loss: 0.012674588710069656\n",
      "step: 3122, loss: 0.05749758705496788\n",
      "step: 3123, loss: 0.08736902475357056\n",
      "step: 3124, loss: 0.09988085925579071\n",
      "step: 3125, loss: 0.002433489542454481\n",
      "step: 3126, loss: 0.012063673697412014\n",
      "step: 3127, loss: 0.048674918711185455\n",
      "step: 3128, loss: 0.015770381316542625\n",
      "step: 3129, loss: 0.0063851745799183846\n",
      "step: 3130, loss: 0.1500874012708664\n",
      "step: 3131, loss: 0.017667725682258606\n",
      "step: 3132, loss: 0.047733012586832047\n",
      "step: 3133, loss: 0.045430198311805725\n",
      "step: 3134, loss: 0.04215981438755989\n",
      "step: 3135, loss: 0.003500193590298295\n",
      "step: 3136, loss: 0.003525756299495697\n",
      "step: 3137, loss: 0.013441924005746841\n",
      "step: 3138, loss: 0.09586425870656967\n",
      "step: 3139, loss: 0.0007958143833093345\n",
      "step: 3140, loss: 0.04958765208721161\n",
      "step: 3141, loss: 0.003905808087438345\n",
      "step: 3142, loss: 0.05414871871471405\n",
      "step: 3143, loss: 0.0023113926872611046\n",
      "step: 3144, loss: 0.03993554785847664\n",
      "step: 3145, loss: 0.08291401714086533\n",
      "step: 3146, loss: 0.002417544601485133\n",
      "step: 3147, loss: 0.12518681585788727\n",
      "step: 3148, loss: 0.00317635596729815\n",
      "step: 3149, loss: 0.05512435734272003\n",
      "step: 3150, loss: 0.009991277009248734\n",
      "step: 3151, loss: 0.006744446698576212\n",
      "step: 3152, loss: 0.08098962903022766\n",
      "step: 3153, loss: 0.0018270894652232528\n",
      "step: 3154, loss: 0.006511632818728685\n",
      "step: 3155, loss: 0.05571189522743225\n",
      "step: 3156, loss: 0.027213502675294876\n",
      "step: 3157, loss: 0.04727399721741676\n",
      "step: 3158, loss: 0.05839553475379944\n",
      "step: 3159, loss: 0.0022352442611008883\n",
      "step: 3160, loss: 0.003968305420130491\n",
      "step: 3161, loss: 0.06723806262016296\n",
      "step: 3162, loss: 0.0075182076543569565\n",
      "step: 3163, loss: 0.04678542912006378\n",
      "step: 3164, loss: 0.001434100791811943\n",
      "step: 3165, loss: 0.00441640242934227\n",
      "step: 3166, loss: 0.0028584010433405638\n",
      "step: 3167, loss: 0.007462725508958101\n",
      "step: 3168, loss: 0.053624529391527176\n",
      "step: 3169, loss: 0.004166036378592253\n",
      "step: 3170, loss: 0.004237164743244648\n",
      "step: 3171, loss: 0.049790993332862854\n",
      "step: 3172, loss: 0.04573134332895279\n",
      "step: 3173, loss: 0.04061505198478699\n",
      "step: 3174, loss: 0.05379222333431244\n",
      "step: 3175, loss: 0.006437407806515694\n",
      "step: 3176, loss: 0.025914091616868973\n",
      "step: 3177, loss: 0.0059909154660999775\n",
      "step: 3178, loss: 0.0028699501417577267\n",
      "step: 3179, loss: 0.005552938673645258\n",
      "step: 3180, loss: 0.01737344078719616\n",
      "step: 3181, loss: 0.03314853459596634\n",
      "step: 3182, loss: 0.0047174603678286076\n",
      "step: 3183, loss: 0.005114582367241383\n",
      "step: 3184, loss: 0.05638828128576279\n",
      "step: 3185, loss: 0.00203870446421206\n",
      "step: 3186, loss: 0.006099355407059193\n",
      "step: 3187, loss: 0.07177653163671494\n",
      "step: 3188, loss: 0.011370357125997543\n",
      "step: 3189, loss: 0.050997331738471985\n",
      "step: 3190, loss: 0.0005804834654554725\n",
      "step: 3191, loss: 0.0173296257853508\n",
      "step: 3192, loss: 0.004724264144897461\n",
      "step: 3193, loss: 0.0024725291877985\n",
      "step: 3194, loss: 0.04983547702431679\n",
      "step: 3195, loss: 0.04285493493080139\n",
      "step: 3196, loss: 0.04014010354876518\n",
      "step: 3197, loss: 0.004097742959856987\n",
      "step: 3198, loss: 0.0026338982861489058\n",
      "step: 3199, loss: 0.05390261113643646\n",
      "step: 3200, loss: 0.019783005118370056\n",
      "step: 3201, loss: 0.009895929135382175\n",
      "step: 3202, loss: 0.0025562220253050327\n",
      "step: 3203, loss: 0.00151917920447886\n",
      "step: 3204, loss: 0.004299219232052565\n",
      "step: 3205, loss: 0.0038757147267460823\n",
      "step: 3206, loss: 0.036012399941682816\n",
      "step: 3207, loss: 0.0012048050994053483\n",
      "step: 3208, loss: 0.0066822124645113945\n",
      "step: 3209, loss: 0.0007894951268099248\n",
      "step: 3210, loss: 0.05101333558559418\n",
      "step: 3211, loss: 0.02059905230998993\n",
      "step: 3212, loss: 0.004245462361723185\n",
      "step: 3213, loss: 0.003009105334058404\n",
      "step: 3214, loss: 0.008636402897536755\n",
      "step: 3215, loss: 0.017246130853891373\n",
      "step: 3216, loss: 0.006225303281098604\n",
      "step: 3217, loss: 0.059318531304597855\n",
      "step: 3218, loss: 0.050607867538928986\n",
      "step: 3219, loss: 0.002074316842481494\n",
      "step: 3220, loss: 0.0067543815821409225\n",
      "step: 3221, loss: 0.004349694587290287\n",
      "step: 3222, loss: 0.015271008014678955\n",
      "step: 3223, loss: 0.04449545964598656\n",
      "step: 3224, loss: 0.05155172944068909\n",
      "step: 3225, loss: 0.002380967838689685\n",
      "step: 3226, loss: 0.049273837357759476\n",
      "step: 3227, loss: 0.004205285105854273\n",
      "step: 3228, loss: 0.0071797906421124935\n",
      "step: 3229, loss: 0.007251465693116188\n",
      "step: 3230, loss: 0.006467586848884821\n",
      "step: 3231, loss: 0.003036189591512084\n",
      "step: 3232, loss: 0.0046384395100176334\n",
      "step: 3233, loss: 0.010372737422585487\n",
      "step: 3234, loss: 0.027200859040021896\n",
      "step: 3235, loss: 0.0024419541005045176\n",
      "step: 3236, loss: 0.03450899198651314\n",
      "step: 3237, loss: 0.04774570092558861\n",
      "step: 3238, loss: 0.00682624289765954\n",
      "step: 3239, loss: 0.005361658986657858\n",
      "step: 3240, loss: 0.13769002258777618\n",
      "step: 3241, loss: 0.05293400585651398\n",
      "step: 3242, loss: 0.09598594903945923\n",
      "step: 3243, loss: 0.08344019204378128\n",
      "step: 3244, loss: 0.012391848489642143\n",
      "step: 3245, loss: 0.4986080229282379\n",
      "step: 3246, loss: 0.00956558994948864\n",
      "step: 3247, loss: 0.003406811272725463\n",
      "step: 3248, loss: 0.004624728113412857\n",
      "step: 3249, loss: 0.03243449702858925\n",
      "step: 3250, loss: 0.017063148319721222\n",
      "step: 3251, loss: 0.0042067402973771095\n",
      "step: 3252, loss: 0.035297099500894547\n",
      "step: 3253, loss: 0.06116311252117157\n",
      "step: 3254, loss: 0.0055729178711771965\n",
      "step: 3255, loss: 0.02974448725581169\n",
      "step: 3256, loss: 0.009164858609437943\n",
      "step: 3257, loss: 0.01265998836606741\n",
      "step: 3258, loss: 0.07355000823736191\n",
      "step: 3259, loss: 0.05712081119418144\n",
      "step: 3260, loss: 0.005777317099273205\n",
      "step: 3261, loss: 0.0037928938400000334\n",
      "step: 3262, loss: 0.009057787247002125\n",
      "step: 3263, loss: 0.008718887344002724\n",
      "step: 3264, loss: 0.012960528954863548\n",
      "step: 3265, loss: 0.005654158536344767\n",
      "step: 3266, loss: 0.003552717622369528\n",
      "step: 3267, loss: 0.01045182067900896\n",
      "step: 3268, loss: 0.0005700477631762624\n",
      "step: 3269, loss: 0.142519012093544\n",
      "step: 3270, loss: 0.05203841254115105\n",
      "step: 3271, loss: 0.05014796555042267\n",
      "step: 3272, loss: 0.07393056899309158\n",
      "step: 3273, loss: 0.018364951014518738\n",
      "step: 3274, loss: 0.002614417113363743\n",
      "step: 3275, loss: 0.04647098109126091\n",
      "step: 3276, loss: 0.09961257874965668\n",
      "step: 3277, loss: 0.05135263130068779\n",
      "step: 3278, loss: 0.005349938757717609\n",
      "step: 3279, loss: 0.0029043490067124367\n",
      "step: 3280, loss: 0.011483431793749332\n",
      "step: 3281, loss: 0.059800222516059875\n",
      "step: 3282, loss: 0.0814809575676918\n",
      "step: 3283, loss: 0.004834212828427553\n",
      "step: 3284, loss: 0.03442787006497383\n",
      "step: 3285, loss: 0.03213811665773392\n",
      "step: 3286, loss: 0.039225827902555466\n",
      "step: 3287, loss: 0.006205558776855469\n",
      "step: 3288, loss: 0.004698328208178282\n",
      "step: 3289, loss: 0.016044287011027336\n",
      "step: 3290, loss: 0.04288278892636299\n",
      "step: 3291, loss: 0.009220789186656475\n",
      "step: 3292, loss: 0.004388968925923109\n",
      "step: 3293, loss: 0.0036088209599256516\n",
      "step: 3294, loss: 0.00433284742757678\n",
      "step: 3295, loss: 0.04553954303264618\n",
      "step: 3296, loss: 0.17838074266910553\n",
      "step: 3297, loss: 0.009339649230241776\n",
      "step: 3298, loss: 0.0132537130266428\n",
      "step: 3299, loss: 0.04948684200644493\n",
      "step: 3300, loss: 0.0030984259210526943\n",
      "step: 3301, loss: 0.01902766525745392\n",
      "step: 3302, loss: 0.08042681962251663\n",
      "step: 3303, loss: 0.039748188108205795\n",
      "step: 3304, loss: 0.05760370194911957\n",
      "step: 3305, loss: 0.002827296033501625\n",
      "step: 3306, loss: 0.006193610839545727\n",
      "step: 3307, loss: 0.001556246425025165\n",
      "step: 3308, loss: 0.005874166265130043\n",
      "step: 3309, loss: 0.002801252994686365\n",
      "step: 3310, loss: 0.15660478174686432\n",
      "step: 3311, loss: 0.0423794761300087\n",
      "step: 3312, loss: 0.02828560397028923\n",
      "step: 3313, loss: 0.011649776250123978\n",
      "step: 3314, loss: 0.004801012109965086\n",
      "step: 3315, loss: 0.0024676036555320024\n",
      "step: 3316, loss: 0.005079826805740595\n",
      "step: 3317, loss: 0.005146376322954893\n",
      "step: 3318, loss: 0.03846830502152443\n",
      "step: 3319, loss: 0.005009858403354883\n",
      "step: 3320, loss: 0.04700606316328049\n",
      "step: 3321, loss: 0.05752154439687729\n",
      "step: 3322, loss: 0.045615360140800476\n",
      "step: 3323, loss: 0.05958820506930351\n",
      "step: 3324, loss: 0.006360345054417849\n",
      "step: 3325, loss: 0.02082461677491665\n",
      "step: 3326, loss: 0.0023660489823669195\n",
      "step: 3327, loss: 0.06111991032958031\n",
      "step: 3328, loss: 0.007027147337794304\n",
      "step: 3329, loss: 0.007544423919171095\n",
      "step: 3330, loss: 0.003872257424518466\n",
      "step: 3331, loss: 0.0017392832087352872\n",
      "step: 3332, loss: 0.02200942113995552\n",
      "step: 3333, loss: 0.004789999686181545\n",
      "step: 3334, loss: 0.017800673842430115\n",
      "step: 3335, loss: 0.01867557130753994\n",
      "step: 3336, loss: 0.11968658119440079\n",
      "step: 3337, loss: 0.0035256522241979837\n",
      "step: 3338, loss: 0.05739784240722656\n",
      "step: 3339, loss: 0.0067703453823924065\n",
      "step: 3340, loss: 0.005244724918156862\n",
      "step: 3341, loss: 0.08180251717567444\n",
      "step: 3342, loss: 0.0018229794222861528\n",
      "step: 3343, loss: 0.006049967836588621\n",
      "step: 3344, loss: 0.004497210960835218\n",
      "step: 3345, loss: 0.03297903388738632\n",
      "step: 3346, loss: 0.0011730146361514926\n",
      "step: 3347, loss: 0.04046497866511345\n",
      "step: 3348, loss: 0.037080299109220505\n",
      "step: 3349, loss: 0.004764002747833729\n",
      "step: 3350, loss: 0.049323298037052155\n",
      "step: 3351, loss: 0.0032955121714621782\n",
      "step: 3352, loss: 0.003357333829626441\n",
      "step: 3353, loss: 0.04500396549701691\n",
      "step: 3354, loss: 0.0013981129741296172\n",
      "step: 3355, loss: 0.017498472705483437\n",
      "step: 3356, loss: 0.003943846095353365\n",
      "step: 3357, loss: 0.053201332688331604\n",
      "step: 3358, loss: 0.036436956375837326\n",
      "step: 3359, loss: 0.0427950844168663\n",
      "step: 3360, loss: 0.0015842081047594547\n",
      "step: 3361, loss: 0.03139866515994072\n",
      "step: 3362, loss: 0.014530221000313759\n",
      "step: 3363, loss: 0.00990305282175541\n",
      "step: 3364, loss: 0.0022373043466359377\n",
      "step: 3365, loss: 0.05059123411774635\n",
      "step: 3366, loss: 0.009576004929840565\n",
      "step: 3367, loss: 0.09789025038480759\n",
      "step: 3368, loss: 0.004162115976214409\n",
      "step: 3369, loss: 0.050604477524757385\n",
      "step: 3370, loss: 0.0015921854646876454\n",
      "step: 3371, loss: 0.039624884724617004\n",
      "step: 3372, loss: 0.002370505128055811\n",
      "step: 3373, loss: 0.03753240406513214\n",
      "step: 3374, loss: 0.008444535546004772\n",
      "step: 3375, loss: 0.003021650016307831\n",
      "step: 3376, loss: 0.13431333005428314\n",
      "step: 3377, loss: 0.0019153169123455882\n",
      "step: 3378, loss: 0.04390093684196472\n",
      "step: 3379, loss: 0.004021850414574146\n",
      "step: 3380, loss: 0.006414211820811033\n",
      "step: 3381, loss: 0.010244361124932766\n",
      "step: 3382, loss: 0.001412751036696136\n",
      "step: 3383, loss: 0.04904918372631073\n",
      "step: 3384, loss: 0.011100647039711475\n",
      "step: 3385, loss: 0.0614907369017601\n",
      "step: 3386, loss: 0.016756268218159676\n",
      "step: 3387, loss: 0.05115625262260437\n",
      "step: 3388, loss: 0.03101271577179432\n",
      "step: 3389, loss: 0.0019677572418004274\n",
      "step: 3390, loss: 0.005578226875513792\n",
      "step: 3391, loss: 0.009367567487061024\n",
      "step: 3392, loss: 0.033433545380830765\n",
      "step: 3393, loss: 0.04888839274644852\n",
      "step: 3394, loss: 0.0022261859849095345\n",
      "step: 3395, loss: 0.03706979379057884\n",
      "step: 3396, loss: 0.046579159796237946\n",
      "step: 3397, loss: 0.008437134325504303\n",
      "step: 3398, loss: 0.003159415675327182\n",
      "step: 3399, loss: 0.007297660689800978\n",
      "step: 3400, loss: 0.004562171176075935\n",
      "step: 3401, loss: 0.04911143332719803\n",
      "step: 3402, loss: 0.08491383492946625\n",
      "step: 3403, loss: 0.007037386763840914\n",
      "step: 3404, loss: 0.003631897969171405\n",
      "step: 3405, loss: 0.21799248456954956\n",
      "step: 3406, loss: 0.0010947881964966655\n",
      "step: 3407, loss: 0.03429871425032616\n",
      "step: 3408, loss: 0.035754695534706116\n",
      "step: 3409, loss: 0.05387377366423607\n",
      "step: 3410, loss: 0.0034506097435951233\n",
      "step: 3411, loss: 0.0017188850324600935\n",
      "step: 3412, loss: 0.04478287324309349\n",
      "step: 3413, loss: 0.006618086248636246\n",
      "step: 3414, loss: 0.004368377383798361\n",
      "step: 3415, loss: 0.0670773833990097\n",
      "step: 3416, loss: 0.032532304525375366\n",
      "step: 3417, loss: 0.05163218453526497\n",
      "step: 3418, loss: 0.005693830084055662\n",
      "step: 3419, loss: 0.2057815045118332\n",
      "step: 3420, loss: 0.030645355582237244\n",
      "step: 3421, loss: 0.002708866260945797\n",
      "step: 3422, loss: 0.00809300784021616\n",
      "step: 3423, loss: 0.02363547869026661\n",
      "step: 3424, loss: 0.04915748909115791\n",
      "step: 3425, loss: 0.006295077037066221\n",
      "step: 3426, loss: 0.04485392943024635\n",
      "step: 3427, loss: 0.0013975792098790407\n",
      "step: 3428, loss: 0.05150064080953598\n",
      "step: 3429, loss: 0.008322740904986858\n",
      "step: 3430, loss: 0.046269044280052185\n",
      "step: 3431, loss: 0.003767036832869053\n",
      "step: 3432, loss: 0.21500062942504883\n",
      "step: 3433, loss: 0.05265676975250244\n",
      "step: 3434, loss: 0.003085117321461439\n",
      "step: 3435, loss: 0.059310704469680786\n",
      "step: 3436, loss: 0.011428789235651493\n",
      "step: 3437, loss: 0.0005294161383062601\n",
      "step: 3438, loss: 0.02924678474664688\n",
      "step: 3439, loss: 0.003976862877607346\n",
      "step: 3440, loss: 0.003051532432436943\n",
      "step: 3441, loss: 0.08015617728233337\n",
      "step: 3442, loss: 0.0032700381707400084\n",
      "step: 3443, loss: 0.0008139902492985129\n",
      "step: 3444, loss: 0.5033151507377625\n",
      "step: 3445, loss: 0.006361369043588638\n",
      "step: 3446, loss: 0.03949906677007675\n",
      "step: 3447, loss: 0.0033884907606989145\n",
      "step: 3448, loss: 0.002530871657654643\n",
      "step: 3449, loss: 0.058286748826503754\n",
      "step: 3450, loss: 0.0986982136964798\n",
      "step: 3451, loss: 0.0008276688167825341\n",
      "step: 3452, loss: 0.006702151149511337\n",
      "step: 3453, loss: 0.07452423125505447\n",
      "step: 3454, loss: 0.0010681173298507929\n",
      "step: 3455, loss: 0.016553442925214767\n",
      "step: 3456, loss: 0.0017015740741044283\n",
      "step: 3457, loss: 0.0060040997341275215\n",
      "step: 3458, loss: 0.06504779309034348\n",
      "step: 3459, loss: 0.006891621742397547\n",
      "step: 3460, loss: 0.049177899956703186\n",
      "step: 3461, loss: 0.011327903717756271\n",
      "step: 3462, loss: 0.008384594693779945\n",
      "step: 3463, loss: 0.004425819497555494\n",
      "step: 3464, loss: 0.03230365738272667\n",
      "step: 3465, loss: 0.05440788343548775\n",
      "step: 3466, loss: 0.05284912511706352\n",
      "step: 3467, loss: 0.005345812067389488\n",
      "step: 3468, loss: 0.006973522715270519\n",
      "step: 3469, loss: 0.03957603871822357\n",
      "step: 3470, loss: 0.0037614188622683287\n",
      "step: 3471, loss: 0.006621413864195347\n",
      "step: 3472, loss: 0.02967774122953415\n",
      "step: 3473, loss: 0.0008726003579795361\n",
      "step: 3474, loss: 0.007060745265334845\n",
      "step: 3475, loss: 0.00967559777200222\n",
      "step: 3476, loss: 0.07559385150671005\n",
      "step: 3477, loss: 0.04068731889128685\n",
      "step: 3478, loss: 0.006114262156188488\n",
      "step: 3479, loss: 0.0008806633995845914\n",
      "step: 3480, loss: 0.004238782450556755\n",
      "step: 3481, loss: 0.07752802968025208\n",
      "step: 3482, loss: 0.05071933567523956\n",
      "step: 3483, loss: 0.004357924219220877\n",
      "step: 3484, loss: 0.009677207097411156\n",
      "step: 3485, loss: 0.003630161052569747\n",
      "step: 3486, loss: 0.0007323872996494174\n",
      "step: 3487, loss: 0.004544079769402742\n",
      "step: 3488, loss: 0.1762811690568924\n",
      "step: 3489, loss: 0.002514961641281843\n",
      "step: 3490, loss: 0.03687487542629242\n",
      "step: 3491, loss: 0.01327600795775652\n",
      "step: 3492, loss: 0.0031967598479241133\n",
      "step: 3493, loss: 0.04067981615662575\n",
      "step: 3494, loss: 0.025631146505475044\n",
      "step: 3495, loss: 0.006265381351113319\n",
      "step: 3496, loss: 0.03811119496822357\n",
      "step: 3497, loss: 0.05283888801932335\n",
      "step: 3498, loss: 0.0023330405820161104\n",
      "step: 3499, loss: 0.0036126861814409494\n",
      "step: 3500, loss: 0.11160652339458466\n",
      "step: 3501, loss: 0.018111174926161766\n",
      "step: 3502, loss: 0.016802895814180374\n",
      "step: 3503, loss: 0.000445014942670241\n",
      "step: 3504, loss: 0.014305613934993744\n",
      "step: 3505, loss: 0.003903153585270047\n",
      "step: 3506, loss: 0.005318542942404747\n",
      "step: 3507, loss: 0.0037705490831285715\n",
      "step: 3508, loss: 0.09352218359708786\n",
      "step: 3509, loss: 0.04516138136386871\n",
      "step: 3510, loss: 0.01049613207578659\n",
      "step: 3511, loss: 0.020776262506842613\n",
      "step: 3512, loss: 0.03378435596823692\n",
      "step: 3513, loss: 0.04815693199634552\n",
      "step: 3514, loss: 0.017092110589146614\n",
      "step: 3515, loss: 0.010259808041155338\n",
      "step: 3516, loss: 0.005461464636027813\n",
      "step: 3517, loss: 0.007439760025590658\n",
      "step: 3518, loss: 0.007079036440700293\n",
      "step: 3519, loss: 0.0070718321949243546\n",
      "step: 3520, loss: 0.036778174340724945\n",
      "step: 3521, loss: 0.010623977519571781\n",
      "step: 3522, loss: 0.004325096495449543\n",
      "step: 3523, loss: 0.007306169252842665\n",
      "step: 3524, loss: 0.013577268458902836\n",
      "step: 3525, loss: 0.008084212429821491\n",
      "step: 3526, loss: 0.08725536614656448\n",
      "step: 3527, loss: 0.12378904223442078\n",
      "step: 3528, loss: 0.034829169511795044\n",
      "step: 3529, loss: 0.0377051942050457\n",
      "step: 3530, loss: 0.01276484690606594\n",
      "step: 3531, loss: 0.01975242607295513\n",
      "step: 3532, loss: 0.09314973652362823\n",
      "step: 3533, loss: 0.0062966253608465195\n",
      "step: 3534, loss: 0.004268981982022524\n",
      "step: 3535, loss: 0.0023198898416012526\n",
      "step: 3536, loss: 0.035739604383707047\n",
      "step: 3537, loss: 0.0019791321828961372\n",
      "step: 3538, loss: 0.012256531976163387\n",
      "step: 3539, loss: 0.046385422348976135\n",
      "step: 3540, loss: 0.002828099997714162\n",
      "step: 3541, loss: 0.07042718678712845\n",
      "step: 3542, loss: 0.03876945748925209\n",
      "step: 3543, loss: 0.0070915003307163715\n",
      "step: 3544, loss: 0.006826639175415039\n",
      "step: 3545, loss: 0.005090487189590931\n",
      "step: 3546, loss: 0.013382663950324059\n",
      "step: 3547, loss: 0.006048073060810566\n",
      "step: 3548, loss: 0.00419226149097085\n",
      "step: 3549, loss: 0.09107747673988342\n",
      "step: 3550, loss: 0.004431518726050854\n",
      "step: 3551, loss: 0.008119311183691025\n",
      "step: 3552, loss: 0.00828441884368658\n",
      "step: 3553, loss: 0.002846104558557272\n",
      "step: 3554, loss: 0.004539458081126213\n",
      "step: 3555, loss: 0.007466323208063841\n",
      "step: 3556, loss: 0.004851969890296459\n",
      "step: 3557, loss: 0.004010035656392574\n",
      "step: 3558, loss: 0.007562420330941677\n",
      "step: 3559, loss: 0.05575377121567726\n",
      "step: 3560, loss: 0.004693187773227692\n",
      "step: 3561, loss: 0.004399777390062809\n",
      "step: 3562, loss: 0.00025866206851787865\n",
      "step: 3563, loss: 0.059787239879369736\n",
      "step: 3564, loss: 0.003587470855563879\n",
      "step: 3565, loss: 0.009675842709839344\n",
      "step: 3566, loss: 0.0011093189241364598\n",
      "step: 3567, loss: 0.0033365548588335514\n",
      "step: 3568, loss: 0.010169344022870064\n",
      "step: 3569, loss: 0.032923873513936996\n",
      "step: 3570, loss: 0.061371736228466034\n",
      "step: 3571, loss: 0.009673631750047207\n",
      "step: 3572, loss: 0.17694127559661865\n",
      "step: 3573, loss: 0.022633908316493034\n",
      "step: 3574, loss: 0.015466196462512016\n",
      "step: 3575, loss: 0.0736774131655693\n",
      "step: 3576, loss: 0.009261353872716427\n",
      "step: 3577, loss: 0.007589050568640232\n",
      "step: 3578, loss: 0.003101175418123603\n",
      "step: 3579, loss: 0.09040042757987976\n",
      "step: 3580, loss: 0.00801512598991394\n",
      "step: 3581, loss: 0.051002029329538345\n",
      "step: 3582, loss: 0.0014499009121209383\n",
      "step: 3583, loss: 0.054237786680459976\n",
      "step: 3584, loss: 0.006210906896740198\n",
      "step: 3585, loss: 0.06495007127523422\n",
      "step: 3586, loss: 0.003967362456023693\n",
      "step: 3587, loss: 0.002465746598318219\n",
      "step: 3588, loss: 0.0050340439192950726\n",
      "step: 3589, loss: 0.06557362526655197\n",
      "step: 3590, loss: 0.04726674407720566\n",
      "step: 3591, loss: 0.0050466833636164665\n",
      "step: 3592, loss: 0.12232282757759094\n",
      "step: 3593, loss: 0.057327624410390854\n",
      "step: 3594, loss: 0.05913781747221947\n",
      "step: 3595, loss: 0.0028953985311090946\n",
      "step: 3596, loss: 0.09315600246191025\n",
      "step: 3597, loss: 0.0050373743288218975\n",
      "step: 3598, loss: 0.03548085689544678\n",
      "step: 3599, loss: 0.0034947360400110483\n",
      "step: 3600, loss: 0.0011182535672560334\n",
      "step: 3601, loss: 0.005769376177340746\n",
      "step: 3602, loss: 0.004580632317811251\n",
      "step: 3603, loss: 0.0017528960015624762\n",
      "step: 3604, loss: 0.06435269117355347\n",
      "step: 3605, loss: 0.004343392327427864\n",
      "step: 3606, loss: 0.1927923560142517\n",
      "step: 3607, loss: 0.03959658369421959\n",
      "step: 3608, loss: 0.007151991594582796\n",
      "step: 3609, loss: 0.11726056784391403\n",
      "step: 3610, loss: 0.16019916534423828\n",
      "step: 3611, loss: 0.05060490593314171\n",
      "step: 3612, loss: 0.03558260574936867\n",
      "step: 3613, loss: 0.007008448708802462\n",
      "step: 3614, loss: 0.008484483696520329\n",
      "step: 3615, loss: 0.006956841796636581\n",
      "step: 3616, loss: 0.002382871927693486\n",
      "step: 3617, loss: 0.02323787473142147\n",
      "step: 3618, loss: 0.13085512816905975\n",
      "step: 3619, loss: 0.0025322777219116688\n",
      "step: 3620, loss: 0.003227078355848789\n",
      "step: 3621, loss: 0.015505625866353512\n",
      "step: 3622, loss: 0.02019365131855011\n",
      "step: 3623, loss: 0.01866905763745308\n",
      "step: 3624, loss: 0.0033267668914049864\n",
      "step: 3625, loss: 0.09619320183992386\n",
      "step: 3626, loss: 0.044413741677999496\n",
      "step: 3627, loss: 0.001325633260421455\n",
      "step: 3628, loss: 0.04402188956737518\n",
      "step: 3629, loss: 0.016527915373444557\n",
      "step: 3630, loss: 0.022252235561609268\n",
      "step: 3631, loss: 0.0014689975650981069\n",
      "step: 3632, loss: 0.0038006803952157497\n",
      "step: 3633, loss: 0.01120856124907732\n",
      "step: 3634, loss: 0.00314926216378808\n",
      "step: 3635, loss: 0.0019400392193347216\n",
      "step: 3636, loss: 0.05266506224870682\n",
      "step: 3637, loss: 0.03858734294772148\n",
      "step: 3638, loss: 0.03489740565419197\n",
      "step: 3639, loss: 0.004365788772702217\n",
      "step: 3640, loss: 0.02628380060195923\n",
      "step: 3641, loss: 0.005751984193921089\n",
      "step: 3642, loss: 0.056315112859010696\n",
      "step: 3643, loss: 0.0031599849462509155\n",
      "step: 3644, loss: 0.004050862975418568\n",
      "step: 3645, loss: 0.0012682073283940554\n",
      "step: 3646, loss: 0.08659924566745758\n",
      "step: 3647, loss: 0.08592071384191513\n",
      "step: 3648, loss: 0.003601946635171771\n",
      "step: 3649, loss: 0.16009870171546936\n",
      "step: 3650, loss: 0.007127487566322088\n",
      "step: 3651, loss: 0.0008515212102793157\n",
      "step: 3652, loss: 0.0020625197794288397\n",
      "step: 3653, loss: 0.005982462782412767\n",
      "step: 3654, loss: 0.4714912474155426\n",
      "step: 3655, loss: 0.009282826445996761\n",
      "step: 3656, loss: 0.03911430761218071\n",
      "step: 3657, loss: 0.004388817120343447\n",
      "step: 3658, loss: 0.007825421169400215\n",
      "step: 3659, loss: 0.002259271452203393\n",
      "step: 3660, loss: 0.0038169221952557564\n",
      "step: 3661, loss: 0.04838094860315323\n",
      "step: 3662, loss: 0.02358946017920971\n",
      "step: 3663, loss: 0.12760263681411743\n",
      "step: 3664, loss: 0.010677010752260685\n",
      "step: 3665, loss: 0.006145387887954712\n",
      "step: 3666, loss: 0.07309506833553314\n",
      "step: 3667, loss: 0.004438446369022131\n",
      "step: 3668, loss: 0.0036518489941954613\n",
      "step: 3669, loss: 0.048780012875795364\n",
      "step: 3670, loss: 0.0013557375641539693\n",
      "step: 3671, loss: 0.0027745231054723263\n",
      "step: 3672, loss: 0.05218711495399475\n",
      "step: 3673, loss: 0.003299713134765625\n",
      "step: 3674, loss: 0.0035535376518964767\n",
      "step: 3675, loss: 0.003511986695230007\n",
      "step: 3676, loss: 0.0005592972156591713\n",
      "step: 3677, loss: 0.0011242714244872332\n",
      "step: 3678, loss: 0.03889176994562149\n",
      "step: 3679, loss: 0.055869754403829575\n",
      "step: 3680, loss: 0.002599765080958605\n",
      "step: 3681, loss: 0.05934932082891464\n",
      "step: 3682, loss: 0.0017094361828640103\n",
      "step: 3683, loss: 0.021220743656158447\n",
      "step: 3684, loss: 0.00042318750638514757\n",
      "step: 3685, loss: 0.0023889061994850636\n",
      "step: 3686, loss: 0.07393351942300797\n",
      "step: 3687, loss: 0.0372573547065258\n",
      "step: 3688, loss: 0.007011377718299627\n",
      "step: 3689, loss: 0.0038181489799171686\n",
      "step: 3690, loss: 0.005867550149559975\n",
      "step: 3691, loss: 0.0507853701710701\n",
      "step: 3692, loss: 0.0025092982687056065\n",
      "step: 3693, loss: 0.040542736649513245\n",
      "step: 3694, loss: 0.007827313616871834\n",
      "step: 3695, loss: 0.001875150017440319\n",
      "step: 3696, loss: 0.004260278772562742\n",
      "step: 3697, loss: 0.005623412784188986\n",
      "step: 3698, loss: 0.006878918036818504\n",
      "step: 3699, loss: 0.0013802218018099666\n",
      "step: 3700, loss: 0.005596887320280075\n",
      "step: 3701, loss: 0.009626940824091434\n",
      "step: 3702, loss: 0.10662954300642014\n",
      "step: 3703, loss: 0.0052456543780863285\n",
      "step: 3704, loss: 0.0100222397595644\n",
      "step: 3705, loss: 0.005342055112123489\n",
      "step: 3706, loss: 0.0023285155184566975\n",
      "step: 3707, loss: 0.02709052711725235\n",
      "step: 3708, loss: 0.002721192315220833\n",
      "step: 3709, loss: 0.0036910236813127995\n",
      "step: 3710, loss: 0.0029687026981264353\n",
      "step: 3711, loss: 0.004207522142678499\n",
      "step: 3712, loss: 0.053371861577034\n",
      "step: 3713, loss: 0.09285089373588562\n",
      "step: 3714, loss: 0.07425859570503235\n",
      "step: 3715, loss: 0.0026511019095778465\n",
      "step: 3716, loss: 0.0022229289170354605\n",
      "step: 3717, loss: 0.039309922605752945\n",
      "step: 3718, loss: 0.011329147964715958\n",
      "step: 3719, loss: 0.004741011653095484\n",
      "step: 3720, loss: 0.07060699164867401\n",
      "step: 3721, loss: 0.0478564128279686\n",
      "step: 3722, loss: 0.024915706366300583\n",
      "step: 3723, loss: 0.006762545555830002\n",
      "step: 3724, loss: 0.013588611967861652\n",
      "step: 3725, loss: 0.038168445229530334\n",
      "step: 3726, loss: 0.019088389351963997\n",
      "step: 3727, loss: 0.039013687521219254\n",
      "step: 3728, loss: 0.002841843292117119\n",
      "step: 3729, loss: 0.007116916589438915\n",
      "step: 3730, loss: 0.011776390485465527\n",
      "step: 3731, loss: 0.05279510095715523\n",
      "step: 3732, loss: 0.005384025629609823\n",
      "step: 3733, loss: 0.002177699003368616\n",
      "step: 3734, loss: 0.0020402916707098484\n",
      "step: 3735, loss: 0.005523027386516333\n",
      "step: 3736, loss: 0.0007441472262144089\n",
      "step: 3737, loss: 0.0462832935154438\n",
      "step: 3738, loss: 0.00791185162961483\n",
      "step: 3739, loss: 0.00477822357788682\n",
      "step: 3740, loss: 0.004120040684938431\n",
      "step: 3741, loss: 0.017365390434861183\n",
      "step: 3742, loss: 0.014037268236279488\n",
      "step: 3743, loss: 0.04754779860377312\n",
      "step: 3744, loss: 0.041861701756715775\n",
      "step: 3745, loss: 0.0032278967555612326\n",
      "step: 3746, loss: 0.025019127875566483\n",
      "step: 3747, loss: 0.005353985819965601\n",
      "step: 3748, loss: 0.0010047773830592632\n",
      "step: 3749, loss: 0.003394301515072584\n",
      "step: 3750, loss: 0.03554392233490944\n",
      "step: 3751, loss: 0.04465537145733833\n",
      "step: 3752, loss: 0.007357272319495678\n",
      "step: 3753, loss: 0.004525698721408844\n",
      "step: 3754, loss: 0.051261935383081436\n",
      "step: 3755, loss: 0.009860721416771412\n",
      "step: 3756, loss: 0.0027089801151305437\n",
      "step: 3757, loss: 0.008427286520600319\n",
      "step: 3758, loss: 0.032318420708179474\n",
      "step: 3759, loss: 0.07424597442150116\n",
      "step: 3760, loss: 0.2517009377479553\n",
      "step: 3761, loss: 0.03498269245028496\n",
      "step: 3762, loss: 0.07518523931503296\n",
      "step: 3763, loss: 0.01691477745771408\n",
      "step: 3764, loss: 0.009162664413452148\n",
      "step: 3765, loss: 0.02098509855568409\n",
      "step: 3766, loss: 0.006890337448567152\n",
      "step: 3767, loss: 0.011833026073873043\n",
      "step: 3768, loss: 0.04512688145041466\n",
      "step: 3769, loss: 0.1563892364501953\n",
      "step: 3770, loss: 0.0014182596933096647\n",
      "step: 3771, loss: 0.11653997749090195\n",
      "step: 3772, loss: 0.00381287420168519\n",
      "step: 3773, loss: 0.0042265779338777065\n",
      "step: 3774, loss: 0.0022681662812829018\n",
      "step: 3775, loss: 0.002727723214775324\n",
      "step: 3776, loss: 0.036964837461709976\n",
      "step: 3777, loss: 0.03545204550027847\n",
      "step: 3778, loss: 0.05510430783033371\n",
      "step: 3779, loss: 0.0036247223615646362\n",
      "step: 3780, loss: 0.0042321570217609406\n",
      "step: 3781, loss: 0.001874637440778315\n",
      "step: 3782, loss: 0.006724607199430466\n",
      "step: 3783, loss: 0.041715338826179504\n",
      "step: 3784, loss: 0.0033129220828413963\n",
      "step: 3785, loss: 0.008590703830122948\n",
      "step: 3786, loss: 0.004011425189673901\n",
      "step: 3787, loss: 0.006691234186291695\n",
      "step: 3788, loss: 0.0415845662355423\n",
      "step: 3789, loss: 0.002339293248951435\n",
      "step: 3790, loss: 0.003926537465304136\n",
      "step: 3791, loss: 0.12001173943281174\n",
      "step: 3792, loss: 0.004018235020339489\n",
      "step: 3793, loss: 0.005134713836014271\n",
      "step: 3794, loss: 0.00296601140871644\n",
      "step: 3795, loss: 0.0035729589872062206\n",
      "step: 3796, loss: 0.09776213765144348\n",
      "step: 3797, loss: 0.003726131282746792\n",
      "step: 3798, loss: 0.0026108897291123867\n",
      "step: 3799, loss: 0.05360354483127594\n",
      "step: 3800, loss: 0.046532317996025085\n",
      "step: 3801, loss: 0.038399361073970795\n",
      "step: 3802, loss: 0.02635553851723671\n",
      "step: 3803, loss: 0.1724177598953247\n",
      "step: 3804, loss: 0.25520870089530945\n",
      "step: 3805, loss: 0.0004922497319057584\n",
      "step: 3806, loss: 0.052612315863370895\n",
      "step: 3807, loss: 0.005564598366618156\n",
      "step: 3808, loss: 0.008000468835234642\n",
      "step: 3809, loss: 0.08703096210956573\n",
      "step: 3810, loss: 0.009105244651436806\n",
      "step: 3811, loss: 0.0557500459253788\n",
      "step: 3812, loss: 0.04358721897006035\n",
      "step: 3813, loss: 0.03126683086156845\n",
      "step: 3814, loss: 0.006893028039485216\n",
      "step: 3815, loss: 0.012602304108440876\n",
      "step: 3816, loss: 0.004451253451406956\n",
      "step: 3817, loss: 0.033942073583602905\n",
      "step: 3818, loss: 0.0383891835808754\n",
      "step: 3819, loss: 0.05815593898296356\n",
      "step: 3820, loss: 0.0014160290593281388\n",
      "step: 3821, loss: 0.04876350238919258\n",
      "step: 3822, loss: 0.006027958821505308\n",
      "step: 3823, loss: 0.011881418526172638\n",
      "step: 3824, loss: 0.006461238954216242\n",
      "step: 3825, loss: 0.15950044989585876\n",
      "step: 3826, loss: 0.002431633183732629\n",
      "step: 3827, loss: 0.0021027191542088985\n",
      "step: 3828, loss: 0.04642956703901291\n",
      "step: 3829, loss: 0.17709222435951233\n",
      "step: 3830, loss: 0.013307017274200916\n",
      "step: 3831, loss: 0.004366306588053703\n",
      "step: 3832, loss: 0.1136685460805893\n",
      "step: 3833, loss: 0.01393198687583208\n",
      "step: 3834, loss: 0.003987996838986874\n",
      "step: 3835, loss: 0.004967948887497187\n",
      "step: 3836, loss: 0.055306196212768555\n",
      "step: 3837, loss: 0.0072704278863966465\n",
      "step: 3838, loss: 0.0005543269799090922\n",
      "step: 3839, loss: 0.09490994364023209\n",
      "step: 3840, loss: 0.03996145352721214\n",
      "step: 3841, loss: 0.0019966948311775923\n",
      "step: 3842, loss: 0.046018023043870926\n",
      "step: 3843, loss: 0.0037198555655777454\n",
      "step: 3844, loss: 0.0004209970647934824\n",
      "step: 3845, loss: 0.0372924767434597\n",
      "step: 3846, loss: 0.012418866157531738\n",
      "step: 3847, loss: 0.00392573606222868\n",
      "step: 3848, loss: 0.00830672588199377\n",
      "step: 3849, loss: 0.0031022888142615557\n",
      "step: 3850, loss: 0.0009519249433651567\n",
      "step: 3851, loss: 0.09693428874015808\n",
      "step: 3852, loss: 0.012927250936627388\n",
      "step: 3853, loss: 0.007894163951277733\n",
      "step: 3854, loss: 0.050622109323740005\n",
      "step: 3855, loss: 0.0032842671498656273\n",
      "step: 3856, loss: 0.004934819880872965\n",
      "step: 3857, loss: 0.044163260608911514\n",
      "step: 3858, loss: 0.0030596673022955656\n",
      "step: 3859, loss: 0.060461532324552536\n",
      "step: 3860, loss: 0.03130510821938515\n",
      "step: 3861, loss: 0.0038057558704167604\n",
      "step: 3862, loss: 0.0021584643982350826\n",
      "step: 3863, loss: 0.001225620391778648\n",
      "step: 3864, loss: 0.1501326709985733\n",
      "step: 3865, loss: 0.147168830037117\n",
      "step: 3866, loss: 0.003953268751502037\n",
      "step: 3867, loss: 0.002285064198076725\n",
      "step: 3868, loss: 0.015532872639596462\n",
      "step: 3869, loss: 0.010877923108637333\n",
      "step: 3870, loss: 0.004562876187264919\n",
      "step: 3871, loss: 0.005192290060222149\n",
      "step: 3872, loss: 0.008277815766632557\n",
      "step: 3873, loss: 0.05776359885931015\n",
      "step: 3874, loss: 0.004315362311899662\n",
      "step: 3875, loss: 0.002873759251087904\n",
      "step: 3876, loss: 0.03715525195002556\n",
      "step: 3877, loss: 0.03576558083295822\n",
      "step: 3878, loss: 0.004761338699609041\n",
      "step: 3879, loss: 0.0031340650748461485\n",
      "step: 3880, loss: 0.0037236055359244347\n",
      "step: 3881, loss: 0.07809417694807053\n",
      "step: 3882, loss: 0.05842152237892151\n",
      "step: 3883, loss: 0.029256759211421013\n",
      "step: 3884, loss: 0.042245570570230484\n",
      "step: 3885, loss: 0.016034994274377823\n",
      "step: 3886, loss: 0.05215483158826828\n",
      "step: 3887, loss: 0.009919374249875546\n",
      "step: 3888, loss: 0.017889203503727913\n",
      "step: 3889, loss: 0.004522549454122782\n",
      "step: 3890, loss: 0.06015138328075409\n",
      "step: 3891, loss: 0.0075918822549283504\n",
      "step: 3892, loss: 0.0006634184974245727\n",
      "step: 3893, loss: 0.11065846681594849\n",
      "step: 3894, loss: 0.03536299243569374\n",
      "step: 3895, loss: 0.0653902143239975\n",
      "step: 3896, loss: 0.014943506568670273\n",
      "step: 3897, loss: 0.03562816232442856\n",
      "step: 3898, loss: 0.042573362588882446\n",
      "step: 3899, loss: 0.003951352089643478\n",
      "step: 3900, loss: 0.00365878501906991\n",
      "step: 3901, loss: 0.002201911760494113\n",
      "step: 3902, loss: 0.006894388701766729\n",
      "step: 3903, loss: 0.0066525740548968315\n",
      "step: 3904, loss: 0.0055042146705091\n",
      "step: 3905, loss: 0.006687886081635952\n",
      "step: 3906, loss: 0.00203900970518589\n",
      "step: 3907, loss: 0.0459396094083786\n",
      "step: 3908, loss: 0.008673766627907753\n",
      "step: 3909, loss: 0.013571981340646744\n",
      "step: 3910, loss: 0.0004970457521267235\n",
      "step: 3911, loss: 0.0009713084436953068\n",
      "step: 3912, loss: 0.017170466482639313\n",
      "step: 3913, loss: 0.002610538387671113\n",
      "step: 3914, loss: 0.04792889952659607\n",
      "step: 3915, loss: 0.04030144587159157\n",
      "step: 3916, loss: 0.005912674590945244\n",
      "step: 3917, loss: 0.0033162995241582394\n",
      "step: 3918, loss: 0.005288383923470974\n",
      "step: 3919, loss: 0.006010379642248154\n",
      "step: 3920, loss: 0.01224238146096468\n",
      "step: 3921, loss: 0.011950281448662281\n",
      "step: 3922, loss: 0.03329651057720184\n",
      "step: 3923, loss: 0.0026175749953836203\n",
      "step: 3924, loss: 0.04814181476831436\n",
      "step: 3925, loss: 0.0038338499143719673\n",
      "step: 3926, loss: 0.040265049785375595\n",
      "step: 3927, loss: 0.002335105324164033\n",
      "step: 3928, loss: 0.06284023076295853\n",
      "step: 3929, loss: 0.04583408683538437\n",
      "step: 3930, loss: 0.0036054200027137995\n",
      "step: 3931, loss: 0.002948108594864607\n",
      "step: 3932, loss: 0.003911921288818121\n",
      "step: 3933, loss: 0.002291297074407339\n",
      "step: 3934, loss: 0.005859819706529379\n",
      "step: 3935, loss: 0.10411259531974792\n",
      "step: 3936, loss: 0.0013174357591196895\n",
      "step: 3937, loss: 0.0009571783011779189\n",
      "step: 3938, loss: 0.0007032063440419734\n",
      "step: 3939, loss: 0.0629706159234047\n",
      "step: 3940, loss: 0.03943596035242081\n",
      "step: 3941, loss: 0.002961898222565651\n",
      "step: 3942, loss: 0.008171448484063148\n",
      "step: 3943, loss: 0.0161721371114254\n",
      "step: 3944, loss: 0.036874786019325256\n",
      "step: 3945, loss: 0.005785444751381874\n",
      "step: 3946, loss: 0.003493553726002574\n",
      "step: 3947, loss: 0.0027772770263254642\n",
      "step: 3948, loss: 0.009717635810375214\n",
      "step: 3949, loss: 0.05285349488258362\n",
      "step: 3950, loss: 0.03608623519539833\n",
      "step: 3951, loss: 0.003991736099123955\n",
      "step: 3952, loss: 0.021288588643074036\n",
      "step: 3953, loss: 0.004321420565247536\n",
      "step: 3954, loss: 0.004582787398248911\n",
      "step: 3955, loss: 0.005639818497002125\n",
      "step: 3956, loss: 0.010360154323279858\n",
      "step: 3957, loss: 0.004106689244508743\n",
      "step: 3958, loss: 0.019888075068593025\n",
      "step: 3959, loss: 0.03769470751285553\n",
      "step: 3960, loss: 0.02075481228530407\n",
      "step: 3961, loss: 0.15107591450214386\n",
      "step: 3962, loss: 0.051346007734537125\n",
      "step: 3963, loss: 0.0021102786995470524\n",
      "step: 3964, loss: 0.008407031185925007\n",
      "step: 3965, loss: 0.005733193829655647\n",
      "step: 3966, loss: 0.005621675401926041\n",
      "step: 3967, loss: 0.003178433747962117\n",
      "step: 3968, loss: 0.004097010940313339\n",
      "step: 3969, loss: 0.0006924241315573454\n",
      "step: 3970, loss: 0.01464117132127285\n",
      "step: 3971, loss: 0.028994042426347733\n",
      "step: 3972, loss: 0.044808387756347656\n",
      "step: 3973, loss: 0.005784008651971817\n",
      "step: 3974, loss: 0.010613861493766308\n",
      "step: 3975, loss: 0.03097631223499775\n",
      "step: 3976, loss: 0.10307648777961731\n",
      "step: 3977, loss: 0.026969842612743378\n",
      "step: 3978, loss: 0.0030423810239881277\n",
      "step: 3979, loss: 0.007585648447275162\n",
      "step: 3980, loss: 0.00447057792916894\n",
      "step: 3981, loss: 0.0038710981607437134\n",
      "step: 3982, loss: 0.004812316037714481\n",
      "step: 3983, loss: 0.009145831689238548\n",
      "step: 3984, loss: 0.00697943102568388\n",
      "step: 3985, loss: 0.003433678764849901\n",
      "step: 3986, loss: 0.006438720040023327\n",
      "step: 3987, loss: 0.000507038610521704\n",
      "step: 3988, loss: 0.008829486556351185\n",
      "step: 3989, loss: 0.008438103832304478\n",
      "step: 3990, loss: 0.0005045435973443091\n",
      "step: 3991, loss: 0.041277218610048294\n",
      "step: 3992, loss: 0.0004419964679982513\n",
      "step: 3993, loss: 0.05758349597454071\n",
      "step: 3994, loss: 0.053956810384988785\n",
      "step: 3995, loss: 0.05180726200342178\n",
      "step: 3996, loss: 0.005199317820370197\n",
      "step: 3997, loss: 0.042768917977809906\n",
      "step: 3998, loss: 0.024544697254896164\n",
      "step: 3999, loss: 0.03727273270487785\n",
      "step: 4000, loss: 0.02564561925828457\n",
      "step: 4001, loss: 0.003193157957866788\n",
      "step: 4002, loss: 0.0068854838609695435\n",
      "step: 4003, loss: 0.03045985847711563\n",
      "step: 4004, loss: 0.008333997800946236\n",
      "step: 4005, loss: 0.04400147125124931\n",
      "step: 4006, loss: 0.03864601254463196\n",
      "step: 4007, loss: 0.008399325422942638\n",
      "step: 4008, loss: 0.0060018147341907024\n",
      "step: 4009, loss: 0.0011708383681252599\n",
      "step: 4010, loss: 0.03360139951109886\n",
      "step: 4011, loss: 0.04206045717000961\n",
      "step: 4012, loss: 0.045562442392110825\n",
      "step: 4013, loss: 0.07671841979026794\n",
      "step: 4014, loss: 0.05008135363459587\n",
      "step: 4015, loss: 0.4329606592655182\n",
      "step: 4016, loss: 0.0061115240678191185\n",
      "step: 4017, loss: 0.00644198851659894\n",
      "step: 4018, loss: 0.003886731807142496\n",
      "step: 4019, loss: 0.0026596232783049345\n",
      "step: 4020, loss: 0.053181856870651245\n",
      "step: 4021, loss: 0.012758379802107811\n",
      "step: 4022, loss: 0.023728888481855392\n",
      "step: 4023, loss: 0.1794903725385666\n",
      "step: 4024, loss: 0.007680382113903761\n",
      "step: 4025, loss: 0.05860869586467743\n",
      "step: 4026, loss: 0.0006931865937076509\n",
      "step: 4027, loss: 0.005020171403884888\n",
      "step: 4028, loss: 0.0006228279089555144\n",
      "step: 4029, loss: 0.006902037188410759\n",
      "step: 4030, loss: 0.07904403656721115\n",
      "step: 4031, loss: 0.04938923195004463\n",
      "step: 4032, loss: 0.081544429063797\n",
      "step: 4033, loss: 0.0035528314765542746\n",
      "step: 4034, loss: 0.0005066230078227818\n",
      "step: 4035, loss: 0.07414096593856812\n",
      "step: 4036, loss: 0.0035991359036415815\n",
      "step: 4037, loss: 0.0033731511794030666\n",
      "step: 4038, loss: 0.014449812471866608\n",
      "step: 4039, loss: 0.04089261218905449\n",
      "step: 4040, loss: 0.003892364911735058\n",
      "step: 4041, loss: 0.01219017244875431\n",
      "step: 4042, loss: 0.004448580089956522\n",
      "step: 4043, loss: 0.05558145046234131\n",
      "step: 4044, loss: 0.026299551129341125\n",
      "step: 4045, loss: 0.02169547975063324\n",
      "step: 4046, loss: 0.015312720090150833\n",
      "step: 4047, loss: 0.353064626455307\n",
      "step: 4048, loss: 0.001349709928035736\n",
      "step: 4049, loss: 0.0045028747990727425\n",
      "step: 4050, loss: 0.02701711095869541\n",
      "step: 4051, loss: 0.08201174437999725\n",
      "step: 4052, loss: 0.0025311147328466177\n",
      "step: 4053, loss: 0.001221556100063026\n",
      "step: 4054, loss: 0.001631365972571075\n",
      "step: 4055, loss: 0.014478237368166447\n",
      "step: 4056, loss: 0.040378618985414505\n",
      "step: 4057, loss: 0.0013740244321525097\n",
      "step: 4058, loss: 0.02294996939599514\n",
      "step: 4059, loss: 0.006384734530001879\n",
      "step: 4060, loss: 0.011621861718595028\n",
      "step: 4061, loss: 0.0559379942715168\n",
      "step: 4062, loss: 0.004614034201949835\n",
      "step: 4063, loss: 0.008214766159653664\n",
      "step: 4064, loss: 0.0024141347967088223\n",
      "step: 4065, loss: 0.2846931219100952\n",
      "step: 4066, loss: 0.006151336245238781\n",
      "step: 4067, loss: 0.04969656467437744\n",
      "step: 4068, loss: 0.023565279319882393\n",
      "step: 4069, loss: 0.04125697910785675\n",
      "step: 4070, loss: 0.03905968740582466\n",
      "step: 4071, loss: 0.08483169227838516\n",
      "step: 4072, loss: 0.0075126164592802525\n",
      "step: 4073, loss: 0.0003524220082908869\n",
      "step: 4074, loss: 0.0008595390827395022\n",
      "step: 4075, loss: 0.0011963344877585769\n",
      "step: 4076, loss: 0.04283826798200607\n",
      "step: 4077, loss: 0.006282325834035873\n",
      "step: 4078, loss: 0.04048684239387512\n",
      "step: 4079, loss: 0.09801194071769714\n",
      "step: 4080, loss: 0.0015304558910429478\n",
      "step: 4081, loss: 0.0032815439626574516\n",
      "step: 4082, loss: 0.0008332407451234758\n",
      "step: 4083, loss: 0.053356874734163284\n",
      "step: 4084, loss: 0.0036354693584144115\n",
      "step: 4085, loss: 0.0064319525845348835\n",
      "step: 4086, loss: 0.0026752620469778776\n",
      "step: 4087, loss: 0.05713425949215889\n",
      "step: 4088, loss: 0.0005016663926653564\n",
      "step: 4089, loss: 0.002807503566145897\n",
      "step: 4090, loss: 0.00846962071955204\n",
      "step: 4091, loss: 0.003325010184198618\n",
      "step: 4092, loss: 0.0078017269261181355\n",
      "step: 4093, loss: 0.05448479577898979\n",
      "step: 4094, loss: 0.0027605807408690453\n",
      "step: 4095, loss: 0.005580848548561335\n",
      "step: 4096, loss: 0.04278203472495079\n",
      "step: 4097, loss: 0.016709284856915474\n",
      "step: 4098, loss: 0.00899692252278328\n",
      "step: 4099, loss: 0.04972570389509201\n",
      "step: 4100, loss: 0.009842447936534882\n",
      "step: 4101, loss: 0.0011233717668801546\n",
      "step: 4102, loss: 0.09251736849546432\n",
      "step: 4103, loss: 0.01071542501449585\n",
      "step: 4104, loss: 0.011213544756174088\n",
      "step: 4105, loss: 0.07860425114631653\n",
      "step: 4106, loss: 0.05925767868757248\n",
      "step: 4107, loss: 0.007425711024552584\n",
      "step: 4108, loss: 0.03545862436294556\n",
      "step: 4109, loss: 0.09070879220962524\n",
      "step: 4110, loss: 0.04742469638586044\n",
      "step: 4111, loss: 0.0017590643838047981\n",
      "step: 4112, loss: 0.00024023037985898554\n",
      "step: 4113, loss: 0.004693847615271807\n",
      "step: 4114, loss: 0.007911262102425098\n",
      "step: 4115, loss: 0.0016001320909708738\n",
      "step: 4116, loss: 0.004929980728775263\n",
      "step: 4117, loss: 0.06032397225499153\n",
      "step: 4118, loss: 0.053156089037656784\n",
      "step: 4119, loss: 0.0010164475534111261\n",
      "step: 4120, loss: 0.0017800518544390798\n",
      "step: 4121, loss: 0.00031901372130960226\n",
      "step: 4122, loss: 0.004358506761491299\n",
      "step: 4123, loss: 0.03660663217306137\n",
      "step: 4124, loss: 0.02711200900375843\n",
      "step: 4125, loss: 0.005328549072146416\n",
      "step: 4126, loss: 0.041299279779195786\n",
      "step: 4127, loss: 0.05716296285390854\n",
      "step: 4128, loss: 0.07716494798660278\n",
      "step: 4129, loss: 0.007928968407213688\n",
      "step: 4130, loss: 0.1853797733783722\n",
      "step: 4131, loss: 0.0043943519704043865\n",
      "step: 4132, loss: 0.05485658347606659\n",
      "step: 4133, loss: 0.006102508865296841\n",
      "step: 4134, loss: 0.01331354957073927\n",
      "step: 4135, loss: 0.008315658196806908\n",
      "step: 4136, loss: 0.003561680903658271\n",
      "step: 4137, loss: 0.0041584414429962635\n",
      "step: 4138, loss: 0.06062697991728783\n",
      "step: 4139, loss: 0.001377770327962935\n",
      "step: 4140, loss: 0.04407427832484245\n",
      "step: 4141, loss: 0.0009569252142682672\n",
      "step: 4142, loss: 0.0025342917069792747\n",
      "step: 4143, loss: 0.006661995779722929\n",
      "step: 4144, loss: 0.02457367442548275\n",
      "step: 4145, loss: 0.001914641004987061\n",
      "step: 4146, loss: 0.015488876961171627\n",
      "step: 4147, loss: 0.07207050174474716\n",
      "step: 4148, loss: 0.04792988300323486\n",
      "step: 4149, loss: 0.007273663766682148\n",
      "step: 4150, loss: 0.007673682179301977\n",
      "step: 4151, loss: 0.0014695822028443217\n",
      "step: 4152, loss: 0.08037120848894119\n",
      "step: 4153, loss: 0.004279430024325848\n",
      "step: 4154, loss: 0.003482509171590209\n",
      "step: 4155, loss: 0.6945778131484985\n",
      "step: 4156, loss: 0.0010578937362879515\n",
      "step: 4157, loss: 0.04916655644774437\n",
      "step: 4158, loss: 0.05626176297664642\n",
      "step: 4159, loss: 0.002111442619934678\n",
      "step: 4160, loss: 0.2354688048362732\n",
      "step: 4161, loss: 0.008454146794974804\n",
      "step: 4162, loss: 0.0009345997823402286\n",
      "step: 4163, loss: 0.023940546438097954\n",
      "step: 4164, loss: 0.0033676819875836372\n",
      "step: 4165, loss: 0.003382260911166668\n",
      "step: 4166, loss: 0.004244477488100529\n",
      "step: 4167, loss: 0.04200252518057823\n",
      "step: 4168, loss: 0.035311684012413025\n",
      "step: 4169, loss: 0.004037007223814726\n",
      "step: 4170, loss: 0.002501117065548897\n",
      "step: 4171, loss: 0.003591185435652733\n",
      "step: 4172, loss: 0.09010706096887589\n",
      "step: 4173, loss: 0.005092949140816927\n",
      "step: 4174, loss: 0.036517661064863205\n",
      "step: 4175, loss: 0.004412390757352114\n",
      "step: 4176, loss: 0.009626250714063644\n",
      "step: 4177, loss: 0.0008895269129425287\n",
      "step: 4178, loss: 0.008678126148879528\n",
      "step: 4179, loss: 0.015118956565856934\n",
      "step: 4180, loss: 0.0049131945706903934\n",
      "step: 4181, loss: 0.004390965681523085\n",
      "step: 4182, loss: 0.0001466546527808532\n",
      "step: 4183, loss: 0.0034034994896501303\n",
      "step: 4184, loss: 0.007406546268612146\n",
      "step: 4185, loss: 0.0021822103299200535\n",
      "step: 4186, loss: 0.003415909130126238\n",
      "step: 4187, loss: 0.0015106197679415345\n",
      "step: 4188, loss: 0.15579211711883545\n",
      "step: 4189, loss: 0.00474372087046504\n",
      "step: 4190, loss: 0.0006167743122205138\n",
      "step: 4191, loss: 0.007305464707314968\n",
      "step: 4192, loss: 0.005069034639745951\n",
      "step: 4193, loss: 0.017475854605436325\n",
      "step: 4194, loss: 0.0008621843298897147\n",
      "step: 4195, loss: 0.00608383072540164\n",
      "step: 4196, loss: 0.009394287131726742\n",
      "step: 4197, loss: 0.005850759334862232\n",
      "step: 4198, loss: 0.004767546895891428\n",
      "step: 4199, loss: 0.02943943440914154\n",
      "step: 4200, loss: 0.002183087868615985\n",
      "step: 4201, loss: 0.01638719253242016\n",
      "step: 4202, loss: 0.11722990870475769\n",
      "step: 4203, loss: 0.007329287938773632\n",
      "step: 4204, loss: 0.0008771437569521368\n",
      "step: 4205, loss: 0.041731420904397964\n",
      "step: 4206, loss: 0.007525063585489988\n",
      "step: 4207, loss: 0.0033420484978705645\n",
      "step: 4208, loss: 0.041370563209056854\n",
      "step: 4209, loss: 0.05216128006577492\n",
      "step: 4210, loss: 0.0014769211411476135\n",
      "step: 4211, loss: 0.005066676065325737\n",
      "step: 4212, loss: 0.09621374309062958\n",
      "step: 4213, loss: 0.005929586943238974\n",
      "step: 4214, loss: 0.0027466220781207085\n",
      "step: 4215, loss: 0.0021111462265253067\n",
      "step: 4216, loss: 0.004746127873659134\n",
      "step: 4217, loss: 0.01609550230205059\n",
      "step: 4218, loss: 0.0010015236912295222\n",
      "step: 4219, loss: 0.05704852566123009\n",
      "step: 4220, loss: 0.0686318427324295\n",
      "step: 4221, loss: 0.0012944628251716495\n",
      "step: 4222, loss: 0.0006804203731007874\n",
      "step: 4223, loss: 0.01875334605574608\n",
      "step: 4224, loss: 0.039678432047367096\n",
      "step: 4225, loss: 0.0009985120268538594\n",
      "step: 4226, loss: 0.05810437723994255\n",
      "step: 4227, loss: 0.00898987427353859\n",
      "step: 4228, loss: 0.0010245698504149914\n",
      "step: 4229, loss: 0.01371712889522314\n",
      "step: 4230, loss: 0.007287509273737669\n",
      "step: 4231, loss: 0.049708396196365356\n",
      "step: 4232, loss: 0.019764825701713562\n",
      "step: 4233, loss: 0.009060979820787907\n",
      "step: 4234, loss: 0.000490132428240031\n",
      "step: 4235, loss: 0.08292607963085175\n",
      "step: 4236, loss: 0.01666855812072754\n",
      "step: 4237, loss: 0.005404553841799498\n",
      "step: 4238, loss: 0.009551004506647587\n",
      "step: 4239, loss: 0.009613540023565292\n",
      "step: 4240, loss: 0.0007296802359633148\n",
      "step: 4241, loss: 0.003664277261123061\n",
      "step: 4242, loss: 0.009601365774869919\n",
      "step: 4243, loss: 0.19419579207897186\n",
      "step: 4244, loss: 0.005627410486340523\n",
      "step: 4245, loss: 0.06410546600818634\n",
      "step: 4246, loss: 0.010297125205397606\n",
      "step: 4247, loss: 0.006052443757653236\n",
      "step: 4248, loss: 0.0021246648393571377\n",
      "step: 4249, loss: 0.0018085393821820617\n",
      "step: 4250, loss: 0.011970923282206059\n",
      "step: 4251, loss: 0.05278503894805908\n",
      "step: 4252, loss: 0.04481612145900726\n",
      "step: 4253, loss: 0.05330391228199005\n",
      "step: 4254, loss: 0.001091566402465105\n",
      "step: 4255, loss: 0.061027031391859055\n",
      "step: 4256, loss: 0.03696705400943756\n",
      "step: 4257, loss: 0.04709792137145996\n",
      "step: 4258, loss: 0.006981475278735161\n",
      "step: 4259, loss: 0.007758250925689936\n",
      "step: 4260, loss: 0.005926794372498989\n",
      "step: 4261, loss: 0.010965858586132526\n",
      "step: 4262, loss: 0.03664861246943474\n",
      "step: 4263, loss: 0.004873847588896751\n",
      "step: 4264, loss: 0.00040561548667028546\n",
      "step: 4265, loss: 0.0012160964542999864\n",
      "step: 4266, loss: 0.020379895344376564\n",
      "step: 4267, loss: 0.1717974692583084\n",
      "step: 4268, loss: 0.002591070020571351\n",
      "step: 4269, loss: 0.017509564757347107\n",
      "step: 4270, loss: 0.12140056490898132\n",
      "step: 4271, loss: 0.008285422809422016\n",
      "step: 4272, loss: 0.03519362583756447\n",
      "step: 4273, loss: 0.03049750067293644\n",
      "step: 4274, loss: 0.006603431887924671\n",
      "step: 4275, loss: 0.01862385682761669\n",
      "step: 4276, loss: 0.566321849822998\n",
      "step: 4277, loss: 0.07026687264442444\n",
      "step: 4278, loss: 0.02022795006632805\n",
      "step: 4279, loss: 0.04446226730942726\n",
      "step: 4280, loss: 0.0071583036333322525\n",
      "step: 4281, loss: 0.0032623165752738714\n",
      "step: 4282, loss: 0.006407581735402346\n",
      "step: 4283, loss: 0.0013696221867576241\n",
      "step: 4284, loss: 0.0038627469912171364\n",
      "step: 4285, loss: 0.013687843456864357\n",
      "step: 4286, loss: 0.06989946216344833\n",
      "step: 4287, loss: 0.0006229195860214531\n",
      "step: 4288, loss: 0.009674645029008389\n",
      "step: 4289, loss: 0.003576724324375391\n",
      "step: 4290, loss: 0.006544745527207851\n",
      "step: 4291, loss: 0.01976337470114231\n",
      "step: 4292, loss: 0.006133483722805977\n",
      "step: 4293, loss: 0.002104002982378006\n",
      "step: 4294, loss: 0.0014520501717925072\n",
      "step: 4295, loss: 0.00779089517891407\n",
      "step: 4296, loss: 0.5543115735054016\n",
      "step: 4297, loss: 0.008717981167137623\n",
      "step: 4298, loss: 0.0011917628580704331\n",
      "step: 4299, loss: 0.0054695867002010345\n",
      "step: 4300, loss: 0.0019628829322755337\n",
      "step: 4301, loss: 0.008431106805801392\n",
      "step: 4302, loss: 0.0037670519668608904\n",
      "step: 4303, loss: 0.0054083336144685745\n",
      "step: 4304, loss: 0.0009382205316796899\n",
      "step: 4305, loss: 0.10537345707416534\n",
      "step: 4306, loss: 0.0036440526600927114\n",
      "step: 4307, loss: 0.030992362648248672\n",
      "step: 4308, loss: 0.006422764156013727\n",
      "step: 4309, loss: 0.05251014232635498\n",
      "step: 4310, loss: 0.03504129499197006\n",
      "step: 4311, loss: 0.005436663515865803\n",
      "step: 4312, loss: 0.046149302273988724\n",
      "step: 4313, loss: 0.001193513860926032\n",
      "step: 4314, loss: 0.05061092972755432\n",
      "step: 4315, loss: 0.0018570981919765472\n",
      "step: 4316, loss: 0.043087124824523926\n",
      "step: 4317, loss: 0.006603081710636616\n",
      "step: 4318, loss: 0.09759268164634705\n",
      "step: 4319, loss: 0.0032618017867207527\n",
      "step: 4320, loss: 0.042155418545007706\n",
      "step: 4321, loss: 0.09907033294439316\n",
      "step: 4322, loss: 0.06076579540967941\n",
      "step: 4323, loss: 0.0022580083459615707\n",
      "step: 4324, loss: 0.06214354187250137\n",
      "step: 4325, loss: 0.004645390436053276\n",
      "step: 4326, loss: 0.0038619220722466707\n",
      "step: 4327, loss: 0.005234241485595703\n",
      "step: 4328, loss: 0.02751733362674713\n",
      "step: 4329, loss: 0.0018063683528453112\n",
      "step: 4330, loss: 0.00333581306040287\n",
      "step: 4331, loss: 0.0025399774312973022\n",
      "step: 4332, loss: 0.005332011729478836\n",
      "step: 4333, loss: 0.02663475275039673\n",
      "step: 4334, loss: 0.00180021021515131\n",
      "step: 4335, loss: 0.001672666985541582\n",
      "step: 4336, loss: 0.073983334004879\n",
      "step: 4337, loss: 0.010827131569385529\n",
      "step: 4338, loss: 0.008212774991989136\n",
      "step: 4339, loss: 0.0672764852643013\n",
      "step: 4340, loss: 0.02690057083964348\n",
      "step: 4341, loss: 0.04622369632124901\n",
      "step: 4342, loss: 0.04522991180419922\n",
      "step: 4343, loss: 0.04893328994512558\n",
      "step: 4344, loss: 0.005654364824295044\n",
      "step: 4345, loss: 0.03913768380880356\n",
      "step: 4346, loss: 0.037528518587350845\n",
      "step: 4347, loss: 0.004783209413290024\n",
      "step: 4348, loss: 0.0006737496005371213\n",
      "step: 4349, loss: 0.080075204372406\n",
      "step: 4350, loss: 0.004755128640681505\n",
      "step: 4351, loss: 0.002564000431448221\n",
      "step: 4352, loss: 0.0014484222047030926\n",
      "step: 4353, loss: 0.0007606020662933588\n",
      "step: 4354, loss: 0.013319825753569603\n",
      "step: 4355, loss: 0.0685727521777153\n",
      "step: 4356, loss: 0.0016191948670893908\n",
      "step: 4357, loss: 0.0394420400261879\n",
      "step: 4358, loss: 0.0004988879663869739\n",
      "step: 4359, loss: 0.01068907417356968\n",
      "step: 4360, loss: 0.03474145755171776\n",
      "step: 4361, loss: 0.0009854196105152369\n",
      "step: 4362, loss: 0.003982347436249256\n",
      "step: 4363, loss: 0.006530086509883404\n",
      "step: 4364, loss: 0.06179501488804817\n",
      "step: 4365, loss: 0.0034887194633483887\n",
      "step: 4366, loss: 0.05010885372757912\n",
      "step: 4367, loss: 0.0012673139572143555\n",
      "step: 4368, loss: 0.0021843817085027695\n",
      "step: 4369, loss: 0.04431254416704178\n",
      "step: 4370, loss: 0.03812570869922638\n",
      "step: 4371, loss: 0.08929646760225296\n",
      "step: 4372, loss: 0.0022674344945698977\n",
      "step: 4373, loss: 0.012586577795445919\n",
      "step: 4374, loss: 0.006124160252511501\n",
      "step: 4375, loss: 0.011955685913562775\n",
      "step: 4376, loss: 0.0022358663845807314\n",
      "step: 4377, loss: 0.0015371500048786402\n",
      "step: 4378, loss: 0.0029392517171800137\n",
      "step: 4379, loss: 0.0032390402629971504\n",
      "step: 4380, loss: 0.0029783379286527634\n",
      "step: 4381, loss: 0.001057142042554915\n",
      "step: 4382, loss: 0.012454035691916943\n",
      "step: 4383, loss: 0.0014683273620903492\n",
      "step: 4384, loss: 0.07094739377498627\n",
      "step: 4385, loss: 0.04895934462547302\n",
      "step: 4386, loss: 0.000998521107248962\n",
      "step: 4387, loss: 0.011312606744468212\n",
      "step: 4388, loss: 0.0351191870868206\n",
      "step: 4389, loss: 0.0009191703284159303\n",
      "step: 4390, loss: 0.10011056810617447\n",
      "step: 4391, loss: 0.0114028574898839\n",
      "step: 4392, loss: 0.00354865868575871\n",
      "step: 4393, loss: 0.03388240933418274\n",
      "step: 4394, loss: 0.04553655534982681\n",
      "step: 4395, loss: 0.007045790087431669\n",
      "step: 4396, loss: 0.004577321000397205\n",
      "step: 4397, loss: 0.09798751026391983\n",
      "step: 4398, loss: 0.04935196787118912\n",
      "step: 4399, loss: 0.00502747343853116\n",
      "step: 4400, loss: 0.011246348731219769\n",
      "step: 4401, loss: 0.0012296921340748668\n",
      "step: 4402, loss: 0.040646638721227646\n",
      "step: 4403, loss: 0.002188073704019189\n",
      "step: 4404, loss: 0.008353055454790592\n",
      "step: 4405, loss: 0.038323722779750824\n",
      "step: 4406, loss: 0.004407946951687336\n",
      "step: 4407, loss: 0.001057999674230814\n",
      "step: 4408, loss: 0.005934527143836021\n",
      "step: 4409, loss: 0.09757719188928604\n",
      "step: 4410, loss: 0.004142602905631065\n",
      "step: 4411, loss: 0.04981666058301926\n",
      "step: 4412, loss: 0.009747259318828583\n",
      "step: 4413, loss: 0.002402667421847582\n",
      "step: 4414, loss: 0.0008697378798387945\n",
      "step: 4415, loss: 0.002596513833850622\n",
      "step: 4416, loss: 0.0028923789504915476\n",
      "step: 4417, loss: 0.05142475292086601\n",
      "step: 4418, loss: 0.00969927292317152\n",
      "step: 4419, loss: 0.03238694369792938\n",
      "step: 4420, loss: 0.00427235197275877\n",
      "step: 4421, loss: 0.008460534736514091\n",
      "step: 4422, loss: 0.12249623239040375\n",
      "step: 4423, loss: 0.012539613991975784\n",
      "step: 4424, loss: 0.08110019564628601\n",
      "step: 4425, loss: 0.01170273870229721\n",
      "step: 4426, loss: 0.0055184257216751575\n",
      "step: 4427, loss: 0.07700297236442566\n",
      "step: 4428, loss: 0.016692467033863068\n",
      "step: 4429, loss: 0.07020736485719681\n",
      "step: 4430, loss: 0.002913885749876499\n",
      "step: 4431, loss: 0.01051643118262291\n",
      "step: 4432, loss: 0.00902093667536974\n",
      "step: 4433, loss: 0.007260424084961414\n",
      "step: 4434, loss: 0.0025440251920372248\n",
      "step: 4435, loss: 0.005795614793896675\n",
      "step: 4436, loss: 0.0007265630993060768\n",
      "step: 4437, loss: 0.0011656214483082294\n",
      "step: 4438, loss: 0.013606306165456772\n",
      "step: 4439, loss: 0.17762626707553864\n",
      "step: 4440, loss: 0.03850436583161354\n",
      "step: 4441, loss: 0.04510387405753136\n",
      "step: 4442, loss: 0.03137168288230896\n",
      "step: 4443, loss: 0.0138577651232481\n",
      "step: 4444, loss: 0.0010542491218075156\n",
      "step: 4445, loss: 0.010220080614089966\n",
      "step: 4446, loss: 0.06767208129167557\n",
      "step: 4447, loss: 0.00571620324626565\n",
      "step: 4448, loss: 0.015748023986816406\n",
      "step: 4449, loss: 0.04897838085889816\n",
      "step: 4450, loss: 0.03909469023346901\n",
      "step: 4451, loss: 0.08463568985462189\n",
      "step: 4452, loss: 0.039150170981884\n",
      "step: 4453, loss: 0.0034998932387679815\n",
      "step: 4454, loss: 0.014298951253294945\n",
      "step: 4455, loss: 0.002019505249336362\n",
      "step: 4456, loss: 0.01720360480248928\n",
      "step: 4457, loss: 0.01014761719852686\n",
      "step: 4458, loss: 0.0062085106037557125\n",
      "step: 4459, loss: 0.05214326083660126\n",
      "step: 4460, loss: 0.0023053414188325405\n",
      "step: 4461, loss: 0.007086481899023056\n",
      "step: 4462, loss: 0.0030593383125960827\n",
      "step: 4463, loss: 0.0019025754882022738\n",
      "step: 4464, loss: 0.0018757048528641462\n",
      "step: 4465, loss: 0.04811761900782585\n",
      "step: 4466, loss: 0.0013913927832618356\n",
      "step: 4467, loss: 0.0006296599167399108\n",
      "step: 4468, loss: 0.004895137157291174\n",
      "step: 4469, loss: 0.5454027652740479\n",
      "step: 4470, loss: 0.012956300750374794\n",
      "step: 4471, loss: 0.011011222377419472\n",
      "step: 4472, loss: 0.011306031607091427\n",
      "step: 4473, loss: 0.011442945338785648\n",
      "step: 4474, loss: 0.00034560635685920715\n",
      "step: 4475, loss: 0.12890440225601196\n",
      "step: 4476, loss: 0.00959106720983982\n",
      "step: 4477, loss: 0.011029761284589767\n",
      "step: 4478, loss: 0.003634148510172963\n",
      "step: 4479, loss: 0.056261301040649414\n",
      "step: 4480, loss: 0.04502303898334503\n",
      "step: 4481, loss: 0.0009782824199646711\n",
      "step: 4482, loss: 0.003126900875940919\n",
      "step: 4483, loss: 0.003136304672807455\n",
      "step: 4484, loss: 0.0026090017054229975\n",
      "step: 4485, loss: 0.010399634018540382\n",
      "step: 4486, loss: 0.06640832126140594\n",
      "step: 4487, loss: 0.02553260326385498\n",
      "step: 4488, loss: 0.08423681557178497\n",
      "step: 4489, loss: 0.012923589907586575\n",
      "step: 4490, loss: 0.0018585043726488948\n",
      "step: 4491, loss: 0.005225813947618008\n",
      "step: 4492, loss: 0.002317257458344102\n",
      "step: 4493, loss: 0.006881827022880316\n",
      "step: 4494, loss: 0.08574918657541275\n",
      "step: 4495, loss: 0.03759637102484703\n",
      "step: 4496, loss: 0.05204959213733673\n",
      "step: 4497, loss: 0.0025868071243166924\n",
      "step: 4498, loss: 0.0007521984516642988\n",
      "step: 4499, loss: 0.043910641223192215\n",
      "step: 4500, loss: 0.010204843245446682\n",
      "step: 4501, loss: 0.005773989949375391\n",
      "step: 4502, loss: 0.002730334410443902\n",
      "step: 4503, loss: 0.0015937648713588715\n",
      "step: 4504, loss: 0.002515884581953287\n",
      "step: 4505, loss: 0.0015944800106808543\n",
      "step: 4506, loss: 0.0051351869478821754\n",
      "step: 4507, loss: 0.010610231198370457\n",
      "step: 4508, loss: 0.008041951805353165\n",
      "step: 4509, loss: 0.001959738787263632\n",
      "step: 4510, loss: 0.003443099558353424\n",
      "step: 4511, loss: 0.010337485931813717\n",
      "step: 4512, loss: 0.03509384021162987\n",
      "step: 4513, loss: 0.010041122324764729\n",
      "step: 4514, loss: 0.006302597466856241\n",
      "step: 4515, loss: 0.004432628862559795\n",
      "step: 4516, loss: 0.03006012737751007\n",
      "step: 4517, loss: 0.05140120908617973\n",
      "step: 4518, loss: 0.0007321432931348681\n",
      "step: 4519, loss: 0.19305510818958282\n",
      "step: 4520, loss: 0.005505087785422802\n",
      "step: 4521, loss: 0.09276127070188522\n",
      "step: 4522, loss: 0.13359671831130981\n",
      "step: 4523, loss: 0.00140489696059376\n",
      "step: 4524, loss: 0.06129671633243561\n",
      "step: 4525, loss: 0.03960306942462921\n",
      "step: 4526, loss: 0.04893779754638672\n",
      "step: 4527, loss: 0.0008560364949516952\n",
      "step: 4528, loss: 0.006555395666509867\n",
      "step: 4529, loss: 0.0036853980273008347\n",
      "step: 4530, loss: 0.0008619677973911166\n",
      "step: 4531, loss: 0.00129821861628443\n",
      "step: 4532, loss: 0.009518301114439964\n",
      "step: 4533, loss: 0.0048913490027189255\n",
      "step: 4534, loss: 0.0007270486676134169\n",
      "step: 4535, loss: 0.00039082751027308404\n",
      "step: 4536, loss: 0.0003345469885971397\n",
      "step: 4537, loss: 0.0008829568978399038\n",
      "step: 4538, loss: 0.04987665265798569\n",
      "step: 4539, loss: 0.041586313396692276\n",
      "step: 4540, loss: 0.014845483005046844\n",
      "step: 4541, loss: 0.033591996878385544\n",
      "step: 4542, loss: 0.007789934054017067\n",
      "step: 4543, loss: 0.0005069082835689187\n",
      "step: 4544, loss: 0.009192049503326416\n",
      "step: 4545, loss: 0.0005263058701530099\n",
      "step: 4546, loss: 0.04650432616472244\n",
      "step: 4547, loss: 0.0004735993279609829\n",
      "step: 4548, loss: 0.0058988602831959724\n",
      "step: 4549, loss: 0.04761304706335068\n",
      "step: 4550, loss: 0.004281232599169016\n",
      "step: 4551, loss: 0.04440595954656601\n",
      "step: 4552, loss: 0.004142978694289923\n",
      "step: 4553, loss: 0.008839395828545094\n",
      "step: 4554, loss: 0.5676718354225159\n",
      "step: 4555, loss: 0.011562692001461983\n",
      "step: 4556, loss: 0.036433614790439606\n",
      "step: 4557, loss: 0.009653708897531033\n",
      "step: 4558, loss: 0.05241280049085617\n",
      "step: 4559, loss: 0.10401034355163574\n",
      "step: 4560, loss: 0.0404006764292717\n",
      "step: 4561, loss: 0.021172504872083664\n",
      "step: 4562, loss: 0.003369864309206605\n",
      "step: 4563, loss: 0.054526448249816895\n",
      "step: 4564, loss: 0.051678773015737534\n",
      "step: 4565, loss: 0.0028779057320207357\n",
      "step: 4566, loss: 0.0063304603099823\n",
      "step: 4567, loss: 0.00804714486002922\n",
      "step: 4568, loss: 0.004114113748073578\n",
      "step: 4569, loss: 0.0048598661087453365\n",
      "step: 4570, loss: 0.0015393852954730392\n",
      "step: 4571, loss: 0.00167229981161654\n",
      "step: 4572, loss: 0.040116213262081146\n",
      "step: 4573, loss: 0.0061281160451471806\n",
      "step: 4574, loss: 0.0030976973939687014\n",
      "step: 4575, loss: 0.0595049113035202\n",
      "step: 4576, loss: 0.009833463467657566\n",
      "step: 4577, loss: 0.008147121407091618\n",
      "step: 4578, loss: 0.00884639099240303\n",
      "step: 4579, loss: 0.011016309261322021\n",
      "step: 4580, loss: 0.010409360751509666\n",
      "step: 4581, loss: 0.04921969398856163\n",
      "step: 4582, loss: 0.001587606850080192\n",
      "step: 4583, loss: 0.004131809342652559\n",
      "step: 4584, loss: 0.0025868667289614677\n",
      "step: 4585, loss: 0.09557557106018066\n",
      "step: 4586, loss: 0.09877923876047134\n",
      "step: 4587, loss: 0.002218340989202261\n",
      "step: 4588, loss: 0.010744921863079071\n",
      "step: 4589, loss: 0.004811326041817665\n",
      "step: 4590, loss: 0.0007035236340016127\n",
      "step: 4591, loss: 0.005854528397321701\n",
      "step: 4592, loss: 0.009726602584123611\n",
      "step: 4593, loss: 0.02194579318165779\n",
      "step: 4594, loss: 0.022320332005620003\n",
      "step: 4595, loss: 0.028287336230278015\n",
      "step: 4596, loss: 0.0031034424901008606\n",
      "step: 4597, loss: 0.010112177580595016\n",
      "step: 4598, loss: 0.00540499622002244\n",
      "step: 4599, loss: 0.04438518360257149\n",
      "step: 4600, loss: 0.0018758709775283933\n",
      "step: 4601, loss: 0.0014024290721863508\n",
      "step: 4602, loss: 0.0006052225362509489\n",
      "step: 4603, loss: 0.051265910267829895\n",
      "step: 4604, loss: 0.0005069239414297044\n",
      "step: 4605, loss: 0.0044414023868739605\n",
      "step: 4606, loss: 0.012929137796163559\n",
      "step: 4607, loss: 0.11253717541694641\n",
      "step: 4608, loss: 0.026031985878944397\n",
      "step: 4609, loss: 0.007119262591004372\n",
      "step: 4610, loss: 0.0006596525199711323\n",
      "step: 4611, loss: 0.009061461314558983\n",
      "step: 4612, loss: 0.00035590826882980764\n",
      "step: 4613, loss: 0.0007238113903440535\n",
      "step: 4614, loss: 0.05372923985123634\n",
      "step: 4615, loss: 0.04742151126265526\n",
      "step: 4616, loss: 0.0007058905903249979\n",
      "step: 4617, loss: 0.039904963225126266\n",
      "step: 4618, loss: 0.06295983493328094\n",
      "step: 4619, loss: 0.005944826640188694\n",
      "step: 4620, loss: 0.04227602109313011\n",
      "step: 4621, loss: 0.01127341017127037\n",
      "step: 4622, loss: 0.0003871306835208088\n",
      "step: 4623, loss: 0.0511513315141201\n",
      "step: 4624, loss: 0.009317498654127121\n",
      "step: 4625, loss: 0.06885416805744171\n",
      "step: 4626, loss: 0.02524944767355919\n",
      "step: 4627, loss: 0.0014872081810608506\n",
      "step: 4628, loss: 0.1550968736410141\n",
      "step: 4629, loss: 0.005467207636684179\n",
      "step: 4630, loss: 0.0217193104326725\n",
      "step: 4631, loss: 0.007367223501205444\n",
      "step: 4632, loss: 0.0059899939224123955\n",
      "step: 4633, loss: 0.0023747358936816454\n",
      "step: 4634, loss: 0.0013232443016022444\n",
      "step: 4635, loss: 0.002129353815689683\n",
      "step: 4636, loss: 0.05443495139479637\n",
      "step: 4637, loss: 0.00570035632699728\n",
      "step: 4638, loss: 0.000739271636120975\n",
      "step: 4639, loss: 0.00531786447390914\n",
      "step: 4640, loss: 0.005104562267661095\n",
      "step: 4641, loss: 0.005519786383956671\n",
      "step: 4642, loss: 0.0007441690540872514\n",
      "step: 4643, loss: 0.0026804273948073387\n",
      "step: 4644, loss: 0.009841268882155418\n",
      "step: 4645, loss: 0.025223715230822563\n",
      "step: 4646, loss: 0.045650314539670944\n",
      "step: 4647, loss: 0.023384684696793556\n",
      "step: 4648, loss: 0.004614782519638538\n",
      "step: 4649, loss: 0.004601096734404564\n",
      "step: 4650, loss: 0.0036516007967293262\n",
      "step: 4651, loss: 0.004645974840968847\n",
      "step: 4652, loss: 0.0036983818281441927\n",
      "step: 4653, loss: 0.031692445278167725\n",
      "step: 4654, loss: 0.009040039032697678\n",
      "step: 4655, loss: 0.029193997383117676\n",
      "step: 4656, loss: 0.0997198075056076\n",
      "step: 4657, loss: 0.006441382225602865\n",
      "step: 4658, loss: 0.01270184013992548\n",
      "step: 4659, loss: 0.008193491958081722\n",
      "step: 4660, loss: 0.00036265404196456075\n",
      "step: 4661, loss: 0.0008739476907067001\n",
      "step: 4662, loss: 0.05444535240530968\n",
      "step: 4663, loss: 0.045977167785167694\n",
      "step: 4664, loss: 0.07089564204216003\n",
      "step: 4665, loss: 0.043159566819667816\n",
      "step: 4666, loss: 0.0005982560105621815\n",
      "step: 4667, loss: 0.001315559260547161\n",
      "step: 4668, loss: 0.008492914028465748\n",
      "step: 4669, loss: 0.004549578297883272\n",
      "step: 4670, loss: 0.18574124574661255\n",
      "step: 4671, loss: 0.002218559617176652\n",
      "step: 4672, loss: 0.0015627266839146614\n",
      "step: 4673, loss: 0.002631643321365118\n",
      "step: 4674, loss: 0.0015483720926567912\n",
      "step: 4675, loss: 0.0003266629937570542\n",
      "step: 4676, loss: 0.013459202833473682\n",
      "step: 4677, loss: 0.007226626854389906\n",
      "step: 4678, loss: 0.0005526590975932777\n",
      "step: 4679, loss: 0.004925742279738188\n",
      "step: 4680, loss: 0.00933180470019579\n",
      "step: 4681, loss: 0.0014024130068719387\n",
      "step: 4682, loss: 0.053420644253492355\n",
      "step: 4683, loss: 0.0012325942516326904\n",
      "step: 4684, loss: 0.005616589915007353\n",
      "step: 4685, loss: 0.003800616366788745\n",
      "step: 4686, loss: 0.1200980693101883\n",
      "step: 4687, loss: 0.000462608237285167\n",
      "step: 4688, loss: 0.0016897926107048988\n",
      "step: 4689, loss: 0.040348831564188004\n",
      "step: 4690, loss: 0.004671391099691391\n",
      "step: 4691, loss: 0.004028096329420805\n",
      "step: 4692, loss: 0.04015303775668144\n",
      "step: 4693, loss: 0.0006447794148698449\n",
      "step: 4694, loss: 0.05002438649535179\n",
      "step: 4695, loss: 0.003976350650191307\n",
      "step: 4696, loss: 0.03364143148064613\n",
      "step: 4697, loss: 0.0011091226479038596\n",
      "step: 4698, loss: 0.03857589513063431\n",
      "step: 4699, loss: 0.017648916691541672\n",
      "step: 4700, loss: 0.010348106734454632\n",
      "step: 4701, loss: 0.0021055603865534067\n",
      "step: 4702, loss: 0.039025403559207916\n",
      "step: 4703, loss: 0.0089949369430542\n",
      "step: 4704, loss: 0.02179562859237194\n",
      "step: 4705, loss: 0.13002079725265503\n",
      "step: 4706, loss: 0.0007429181132465601\n",
      "step: 4707, loss: 0.013237887993454933\n",
      "step: 4708, loss: 0.0016778348945081234\n",
      "step: 4709, loss: 0.06491941213607788\n",
      "step: 4710, loss: 0.0010094144381582737\n",
      "step: 4711, loss: 0.007389700971543789\n",
      "step: 4712, loss: 0.0007474242011085153\n",
      "step: 4713, loss: 0.006009739823639393\n",
      "step: 4714, loss: 0.0053125047124922276\n",
      "step: 4715, loss: 0.036228395998477936\n",
      "step: 4716, loss: 0.015309845097362995\n",
      "step: 4717, loss: 0.03231506794691086\n",
      "step: 4718, loss: 0.04987592622637749\n",
      "step: 4719, loss: 0.0024905456230044365\n",
      "step: 4720, loss: 0.027751754969358444\n",
      "step: 4721, loss: 0.006566288415342569\n",
      "step: 4722, loss: 0.013676810078322887\n",
      "step: 4723, loss: 0.04827425628900528\n",
      "step: 4724, loss: 0.025041867047548294\n",
      "step: 4725, loss: 0.00712679885327816\n",
      "step: 4726, loss: 0.04714890569448471\n",
      "step: 4727, loss: 0.03360763564705849\n",
      "step: 4728, loss: 0.046438999474048615\n",
      "step: 4729, loss: 0.00428100535646081\n",
      "step: 4730, loss: 0.09330248832702637\n",
      "step: 4731, loss: 0.01623455248773098\n",
      "step: 4732, loss: 0.006175920832902193\n",
      "step: 4733, loss: 0.5370703339576721\n",
      "step: 4734, loss: 0.006565954070538282\n",
      "step: 4735, loss: 0.0011726616648957133\n",
      "step: 4736, loss: 0.0009633973822928965\n",
      "step: 4737, loss: 0.006172314751893282\n",
      "step: 4738, loss: 0.014765096828341484\n",
      "step: 4739, loss: 0.12185505032539368\n",
      "step: 4740, loss: 0.011523689143359661\n",
      "step: 4741, loss: 0.039858199656009674\n",
      "step: 4742, loss: 0.03298470377922058\n",
      "step: 4743, loss: 0.0073584155179560184\n",
      "step: 4744, loss: 0.05273895338177681\n",
      "step: 4745, loss: 0.08983969688415527\n",
      "step: 4746, loss: 0.04101630300283432\n",
      "step: 4747, loss: 0.022573715075850487\n",
      "step: 4748, loss: 0.05321826413273811\n",
      "step: 4749, loss: 0.0008244812488555908\n",
      "step: 4750, loss: 0.01230289414525032\n",
      "step: 4751, loss: 0.08646083623170853\n",
      "step: 4752, loss: 0.008290407247841358\n",
      "step: 4753, loss: 0.006824076641350985\n",
      "step: 4754, loss: 0.008027789182960987\n",
      "step: 4755, loss: 0.0014555747620761395\n",
      "step: 4756, loss: 0.00561382994055748\n",
      "step: 4757, loss: 0.04638978838920593\n",
      "step: 4758, loss: 0.017253447324037552\n",
      "step: 4759, loss: 0.007405870594084263\n",
      "step: 4760, loss: 0.02471200004220009\n",
      "step: 4761, loss: 0.023463701829314232\n",
      "step: 4762, loss: 0.045444730669260025\n",
      "step: 4763, loss: 0.0974186435341835\n",
      "step: 4764, loss: 0.0007055233581922948\n",
      "step: 4765, loss: 0.005954619497060776\n",
      "step: 4766, loss: 0.0003572501882445067\n",
      "step: 4767, loss: 0.05641530454158783\n",
      "step: 4768, loss: 0.013755100779235363\n",
      "step: 4769, loss: 0.05285697802901268\n",
      "step: 4770, loss: 0.00022645443095825613\n",
      "step: 4771, loss: 0.007310471031814814\n",
      "step: 4772, loss: 0.5384352207183838\n",
      "step: 4773, loss: 0.046173349022865295\n",
      "step: 4774, loss: 0.05662497133016586\n",
      "step: 4775, loss: 0.022744588553905487\n",
      "step: 4776, loss: 0.006203827913850546\n",
      "step: 4777, loss: 0.04295526072382927\n",
      "step: 4778, loss: 0.0011080263648182154\n",
      "step: 4779, loss: 0.0025007417425513268\n",
      "step: 4780, loss: 0.002246500225737691\n",
      "step: 4781, loss: 0.0010575507767498493\n",
      "step: 4782, loss: 0.01285911351442337\n",
      "step: 4783, loss: 0.00297034764662385\n",
      "step: 4784, loss: 0.00789350364357233\n",
      "step: 4785, loss: 0.03545723855495453\n",
      "step: 4786, loss: 0.0061776284128427505\n",
      "step: 4787, loss: 0.006108411122113466\n",
      "step: 4788, loss: 0.007824818603694439\n",
      "step: 4789, loss: 0.002486331155523658\n",
      "step: 4790, loss: 0.0012541142059490085\n",
      "step: 4791, loss: 0.042833805084228516\n",
      "step: 4792, loss: 0.009850988164544106\n",
      "step: 4793, loss: 0.04311390966176987\n",
      "step: 4794, loss: 0.005883242003619671\n",
      "step: 4795, loss: 0.0016307185869663954\n",
      "step: 4796, loss: 0.023214425891637802\n",
      "step: 4797, loss: 0.0004439530603121966\n",
      "step: 4798, loss: 0.0019194926135241985\n",
      "step: 4799, loss: 0.013362406753003597\n",
      "step: 4800, loss: 0.0062092505395412445\n",
      "step: 4801, loss: 0.001092218910343945\n",
      "step: 4802, loss: 0.05268445983529091\n",
      "step: 4803, loss: 0.034513045102357864\n",
      "step: 4804, loss: 0.011372820474207401\n",
      "step: 4805, loss: 0.001155531732365489\n",
      "step: 4806, loss: 0.0011030706809833646\n",
      "step: 4807, loss: 0.0060706231743097305\n",
      "step: 4808, loss: 0.012111779302358627\n",
      "step: 4809, loss: 0.00926237553358078\n",
      "step: 4810, loss: 0.004285629838705063\n",
      "step: 4811, loss: 0.05021841824054718\n",
      "step: 4812, loss: 0.0018619297770783305\n",
      "step: 4813, loss: 0.005360377952456474\n",
      "step: 4814, loss: 0.0012585510266944766\n",
      "step: 4815, loss: 0.05260300636291504\n",
      "step: 4816, loss: 0.0016110960859805346\n",
      "step: 4817, loss: 0.0016997107304632664\n",
      "step: 4818, loss: 0.005265176761895418\n",
      "step: 4819, loss: 0.006906017195433378\n",
      "step: 4820, loss: 0.009458422660827637\n",
      "step: 4821, loss: 0.04025910794734955\n",
      "step: 4822, loss: 0.0012951934477314353\n",
      "step: 4823, loss: 0.0014060118701308966\n",
      "step: 4824, loss: 0.04663523659110069\n",
      "step: 4825, loss: 0.0077631245367228985\n",
      "step: 4826, loss: 0.001780834747478366\n",
      "step: 4827, loss: 0.002471181098371744\n",
      "step: 4828, loss: 0.005768860690295696\n",
      "step: 4829, loss: 0.013853787444531918\n",
      "step: 4830, loss: 0.017321722581982613\n",
      "step: 4831, loss: 0.020147385075688362\n",
      "step: 4832, loss: 0.021070009097456932\n",
      "step: 4833, loss: 0.0013340696459636092\n",
      "step: 4834, loss: 0.007223375607281923\n",
      "step: 4835, loss: 0.004124974831938744\n",
      "step: 4836, loss: 0.00044256477849557996\n",
      "step: 4837, loss: 0.007739555090665817\n",
      "step: 4838, loss: 0.0013581395614892244\n",
      "step: 4839, loss: 0.08751474320888519\n",
      "step: 4840, loss: 0.011502284556627274\n",
      "step: 4841, loss: 0.005137456115335226\n",
      "step: 4842, loss: 0.0031618089415133\n",
      "step: 4843, loss: 0.032221175730228424\n",
      "step: 4844, loss: 0.015464824624359608\n",
      "step: 4845, loss: 0.010982178151607513\n",
      "step: 4846, loss: 0.003346536075696349\n",
      "step: 4847, loss: 0.09534095972776413\n",
      "step: 4848, loss: 0.027489131316542625\n",
      "step: 4849, loss: 0.06364516168832779\n",
      "step: 4850, loss: 0.010293100029230118\n",
      "step: 4851, loss: 0.18445049226284027\n",
      "step: 4852, loss: 0.0019270707853138447\n",
      "step: 4853, loss: 0.039228860288858414\n",
      "step: 4854, loss: 0.0005435326602309942\n",
      "step: 4855, loss: 0.024289652705192566\n",
      "step: 4856, loss: 0.05076999589800835\n",
      "step: 4857, loss: 0.04522039368748665\n",
      "step: 4858, loss: 0.00030641694320365787\n",
      "step: 4859, loss: 0.0009816401870921254\n",
      "step: 4860, loss: 0.04845765605568886\n",
      "step: 4861, loss: 0.0004539162328001112\n",
      "step: 4862, loss: 0.009196735918521881\n",
      "step: 4863, loss: 0.0032587882596999407\n",
      "step: 4864, loss: 0.009770295582711697\n",
      "step: 4865, loss: 0.0010757815325632691\n",
      "step: 4866, loss: 0.00309319281950593\n",
      "step: 4867, loss: 0.07549579441547394\n",
      "step: 4868, loss: 0.010800923220813274\n",
      "step: 4869, loss: 0.07721783965826035\n",
      "step: 4870, loss: 0.0009979917667806149\n",
      "step: 4871, loss: 0.004763117525726557\n",
      "step: 4872, loss: 0.015802206471562386\n",
      "step: 4873, loss: 0.03567581996321678\n",
      "step: 4874, loss: 0.004867454059422016\n",
      "step: 4875, loss: 0.015098950825631618\n",
      "step: 4876, loss: 0.003041097428649664\n",
      "step: 4877, loss: 0.08170217275619507\n",
      "step: 4878, loss: 0.004476355388760567\n",
      "step: 4879, loss: 0.00122215342707932\n",
      "step: 4880, loss: 0.0010417208541184664\n",
      "step: 4881, loss: 0.049543529748916626\n",
      "step: 4882, loss: 0.003169723553583026\n",
      "step: 4883, loss: 0.0207060556858778\n",
      "step: 4884, loss: 0.03902857005596161\n",
      "step: 4885, loss: 0.042280446738004684\n",
      "step: 4886, loss: 0.006331582088023424\n",
      "step: 4887, loss: 0.004843436647206545\n",
      "step: 4888, loss: 0.06407294422388077\n",
      "step: 4889, loss: 0.006288822274655104\n",
      "step: 4890, loss: 0.002866046968847513\n",
      "step: 4891, loss: 0.005413679871708155\n",
      "step: 4892, loss: 0.0002869754680432379\n",
      "step: 4893, loss: 0.09605646133422852\n",
      "step: 4894, loss: 0.04257490485906601\n",
      "step: 4895, loss: 0.0006144093931652606\n",
      "step: 4896, loss: 0.001212036469951272\n",
      "step: 4897, loss: 0.008113613352179527\n",
      "step: 4898, loss: 0.004232433624565601\n",
      "step: 4899, loss: 0.01034619566053152\n",
      "step: 4900, loss: 0.09689205884933472\n",
      "step: 4901, loss: 0.001803771243430674\n",
      "step: 4902, loss: 0.0003286862629465759\n",
      "step: 4903, loss: 0.013808945193886757\n",
      "step: 4904, loss: 0.006904061418026686\n",
      "step: 4905, loss: 0.0701477974653244\n",
      "step: 4906, loss: 0.003036614740267396\n",
      "step: 4907, loss: 0.01717115193605423\n",
      "step: 4908, loss: 0.010631815530359745\n",
      "step: 4909, loss: 0.2624676823616028\n",
      "step: 4910, loss: 0.010747944004833698\n",
      "step: 4911, loss: 0.004567216616123915\n",
      "step: 4912, loss: 0.0004986864514648914\n",
      "step: 4913, loss: 0.005667686928063631\n",
      "step: 4914, loss: 0.006104757077991962\n",
      "step: 4915, loss: 0.00358393182978034\n",
      "step: 4916, loss: 0.007814865559339523\n",
      "step: 4917, loss: 0.058049172163009644\n",
      "step: 4918, loss: 0.0008273786515928805\n",
      "step: 4919, loss: 0.00047750846715644\n",
      "step: 4920, loss: 0.009563996456563473\n",
      "step: 4921, loss: 0.0010769651271402836\n",
      "step: 4922, loss: 0.002513828221708536\n",
      "step: 4923, loss: 0.0011932478519156575\n",
      "step: 4924, loss: 0.0027382627595216036\n",
      "step: 4925, loss: 0.018932489678263664\n",
      "step: 4926, loss: 0.01095790509134531\n",
      "step: 4927, loss: 0.002208143472671509\n",
      "step: 4928, loss: 0.013716998510062695\n",
      "step: 4929, loss: 0.007320626173168421\n",
      "step: 4930, loss: 0.0012451583752408624\n",
      "step: 4931, loss: 0.0028539260383695364\n",
      "step: 4932, loss: 0.0008176857372745872\n",
      "step: 4933, loss: 0.001841185730881989\n",
      "step: 4934, loss: 0.0068659246899187565\n",
      "step: 4935, loss: 0.0005995008978061378\n",
      "step: 4936, loss: 0.006865832954645157\n",
      "step: 4937, loss: 0.010739823803305626\n",
      "step: 4938, loss: 0.19380168616771698\n",
      "step: 4939, loss: 0.03408503159880638\n",
      "step: 4940, loss: 0.034253064543008804\n",
      "step: 4941, loss: 0.0047825053334236145\n",
      "step: 4942, loss: 0.000778589746914804\n",
      "step: 4943, loss: 0.1911657154560089\n",
      "step: 4944, loss: 0.05965595319867134\n",
      "step: 4945, loss: 0.08993373811244965\n",
      "step: 4946, loss: 0.0069936709478497505\n",
      "step: 4947, loss: 0.04173269122838974\n",
      "step: 4948, loss: 0.029205594211816788\n",
      "step: 4949, loss: 0.029845478013157845\n",
      "step: 4950, loss: 0.04996483400464058\n",
      "step: 4951, loss: 0.0025110566057264805\n",
      "step: 4952, loss: 0.010861844755709171\n",
      "step: 4953, loss: 0.00028874195413663983\n",
      "step: 4954, loss: 0.001115106511861086\n",
      "step: 4955, loss: 0.013809079304337502\n",
      "step: 4956, loss: 0.0010299044661223888\n",
      "step: 4957, loss: 0.0029255333356559277\n",
      "step: 4958, loss: 0.0067192246206104755\n",
      "step: 4959, loss: 0.09891035407781601\n",
      "step: 4960, loss: 0.016238713636994362\n",
      "step: 4961, loss: 0.011634054593741894\n",
      "step: 4962, loss: 0.005847686901688576\n",
      "step: 4963, loss: 0.055970486253499985\n",
      "step: 4964, loss: 0.014045605435967445\n",
      "step: 4965, loss: 0.004100461024791002\n",
      "step: 4966, loss: 0.00046608003322035074\n",
      "step: 4967, loss: 0.010348506271839142\n",
      "step: 4968, loss: 0.03753071650862694\n",
      "step: 4969, loss: 0.004543249029666185\n",
      "step: 4970, loss: 0.005656654015183449\n",
      "step: 4971, loss: 0.00529909273609519\n",
      "step: 4972, loss: 0.004486657213419676\n",
      "step: 4973, loss: 0.0023011150769889355\n",
      "step: 4974, loss: 0.0015361565165221691\n",
      "step: 4975, loss: 0.001349615165963769\n",
      "step: 4976, loss: 0.024948477745056152\n",
      "step: 4977, loss: 0.0007741943700239062\n",
      "step: 4978, loss: 0.005692580249160528\n",
      "step: 4979, loss: 0.009861089289188385\n",
      "step: 4980, loss: 0.0009567796951159835\n",
      "step: 4981, loss: 0.006755161099135876\n",
      "step: 4982, loss: 0.0032401294447481632\n",
      "step: 4983, loss: 0.0038336538709700108\n",
      "step: 4984, loss: 0.009550448507070541\n",
      "step: 4985, loss: 0.006209685932844877\n",
      "step: 4986, loss: 0.0009552424889989197\n",
      "step: 4987, loss: 0.005174047313630581\n",
      "step: 4988, loss: 0.006471924018114805\n",
      "step: 4989, loss: 0.0019629255402833223\n",
      "step: 4990, loss: 0.006447956431657076\n",
      "step: 4991, loss: 0.0006704734405502677\n",
      "step: 4992, loss: 0.019011221826076508\n",
      "step: 4993, loss: 0.014929970726370811\n",
      "step: 4994, loss: 0.024109236896038055\n",
      "step: 4995, loss: 0.06318149715662003\n",
      "step: 4996, loss: 0.002480862196534872\n",
      "step: 4997, loss: 0.0013360135490074754\n",
      "step: 4998, loss: 0.001272510620765388\n",
      "step: 4999, loss: 0.009846844710409641\n",
      "step: 5000, loss: 0.006027224473655224\n",
      "step: 5001, loss: 0.011828943155705929\n",
      "step: 5002, loss: 0.0014745600055903196\n",
      "step: 5003, loss: 0.0007123372633941472\n",
      "step: 5004, loss: 0.006990373134613037\n",
      "step: 5005, loss: 0.007989286445081234\n",
      "step: 5006, loss: 0.011746554635465145\n",
      "step: 5007, loss: 0.01134984940290451\n",
      "step: 5008, loss: 0.007080345880240202\n",
      "step: 5009, loss: 0.057943008840084076\n",
      "step: 5010, loss: 0.0010919140186160803\n",
      "step: 5011, loss: 0.05508573353290558\n",
      "step: 5012, loss: 0.00873749703168869\n",
      "step: 5013, loss: 0.0010638443054631352\n",
      "step: 5014, loss: 0.001581587246619165\n",
      "step: 5015, loss: 0.0005894509959034622\n",
      "step: 5016, loss: 0.05260215699672699\n",
      "step: 5017, loss: 0.00041746252099983394\n",
      "step: 5018, loss: 0.000359829660737887\n",
      "step: 5019, loss: 0.0003969478711951524\n",
      "step: 5020, loss: 0.03517327085137367\n",
      "step: 5021, loss: 0.0066065192222595215\n",
      "step: 5022, loss: 0.0065614706836640835\n",
      "step: 5023, loss: 0.0009988390374928713\n",
      "step: 5024, loss: 0.046647995710372925\n",
      "step: 5025, loss: 0.0021515395492315292\n",
      "step: 5026, loss: 0.011454053223133087\n",
      "step: 5027, loss: 0.0015677489573135972\n",
      "step: 5028, loss: 0.0012682861415669322\n",
      "step: 5029, loss: 0.010808681137859821\n",
      "step: 5030, loss: 0.22677099704742432\n",
      "step: 5031, loss: 0.008695429190993309\n",
      "step: 5032, loss: 0.01033969409763813\n",
      "step: 5033, loss: 0.039940670132637024\n",
      "step: 5034, loss: 0.0050204163417220116\n",
      "step: 5035, loss: 0.00444337772205472\n",
      "step: 5036, loss: 0.0032862997613847256\n",
      "step: 5037, loss: 0.10754115134477615\n",
      "step: 5038, loss: 0.03301424905657768\n",
      "step: 5039, loss: 0.05517919361591339\n",
      "step: 5040, loss: 0.0663481056690216\n",
      "step: 5041, loss: 0.014461538754403591\n",
      "step: 5042, loss: 0.0013534632744267583\n",
      "step: 5043, loss: 0.008585129864513874\n",
      "step: 5044, loss: 0.011200444772839546\n",
      "step: 5045, loss: 0.0032571267802268267\n",
      "step: 5046, loss: 0.0037457041908055544\n",
      "step: 5047, loss: 0.0007831130642443895\n",
      "step: 5048, loss: 0.05290423333644867\n",
      "step: 5049, loss: 0.013621967285871506\n",
      "step: 5050, loss: 0.0011294602882117033\n",
      "step: 5051, loss: 0.0010733680101111531\n",
      "step: 5052, loss: 0.0006406259490177035\n",
      "step: 5053, loss: 0.006688420195132494\n",
      "step: 5054, loss: 0.3386974334716797\n",
      "step: 5055, loss: 0.006881364621222019\n",
      "step: 5056, loss: 0.011411133222281933\n",
      "step: 5057, loss: 0.010805334895849228\n",
      "step: 5058, loss: 0.0054676043801009655\n",
      "step: 5059, loss: 0.002789585618302226\n",
      "step: 5060, loss: 0.051765576004981995\n",
      "step: 5061, loss: 0.003130495548248291\n",
      "step: 5062, loss: 0.015454387292265892\n",
      "step: 5063, loss: 0.05948236957192421\n",
      "step: 5064, loss: 0.049937013536691666\n",
      "step: 5065, loss: 0.043459855020046234\n",
      "step: 5066, loss: 0.008794868364930153\n",
      "step: 5067, loss: 0.005021199584007263\n",
      "step: 5068, loss: 0.0004264075541868806\n",
      "step: 5069, loss: 0.03601239249110222\n",
      "step: 5070, loss: 0.009119194000959396\n",
      "step: 5071, loss: 0.0010423207422718406\n",
      "step: 5072, loss: 0.033289406448602676\n",
      "step: 5073, loss: 0.003936314955353737\n",
      "step: 5074, loss: 0.0036624926142394543\n",
      "step: 5075, loss: 0.00016048044199123979\n",
      "step: 5076, loss: 0.0002832228783518076\n",
      "step: 5077, loss: 0.05865443870425224\n",
      "step: 5078, loss: 0.012262122705578804\n",
      "step: 5079, loss: 0.0671851858496666\n",
      "step: 5080, loss: 0.0013167079305276275\n",
      "step: 5081, loss: 0.003484351560473442\n",
      "step: 5082, loss: 0.057288702577352524\n",
      "step: 5083, loss: 0.0007823932101018727\n",
      "step: 5084, loss: 0.0036086528562009335\n",
      "step: 5085, loss: 0.0001908168924273923\n",
      "step: 5086, loss: 0.0010145272826775908\n",
      "step: 5087, loss: 0.047340814024209976\n",
      "step: 5088, loss: 0.010091005824506283\n",
      "step: 5089, loss: 0.013013374991714954\n",
      "step: 5090, loss: 0.04259370267391205\n",
      "step: 5091, loss: 0.046861667186021805\n",
      "step: 5092, loss: 0.001113681006245315\n",
      "step: 5093, loss: 0.0017039855010807514\n",
      "step: 5094, loss: 0.0021805937867611647\n",
      "step: 5095, loss: 0.006865011993795633\n",
      "step: 5096, loss: 0.000927494780626148\n",
      "step: 5097, loss: 0.033480580896139145\n",
      "step: 5098, loss: 0.0027627029921859503\n",
      "step: 5099, loss: 0.0009335878421552479\n",
      "step: 5100, loss: 0.002928159199655056\n",
      "step: 5101, loss: 0.0004933218588121235\n",
      "step: 5102, loss: 0.0010318749118596315\n",
      "step: 5103, loss: 0.004185325000435114\n",
      "step: 5104, loss: 0.003641907824203372\n",
      "step: 5105, loss: 0.00369430729188025\n",
      "step: 5106, loss: 0.04605394974350929\n",
      "step: 5107, loss: 0.0013199022505432367\n",
      "step: 5108, loss: 0.0092020807787776\n",
      "step: 5109, loss: 0.003294127993285656\n",
      "step: 5110, loss: 0.048587799072265625\n",
      "step: 5111, loss: 0.009563498198986053\n",
      "step: 5112, loss: 0.0018663197988644242\n",
      "step: 5113, loss: 0.0004509941500145942\n",
      "step: 5114, loss: 0.001010676845908165\n",
      "step: 5115, loss: 0.04242958500981331\n",
      "step: 5116, loss: 0.033066969364881516\n",
      "step: 5117, loss: 0.09063472598791122\n",
      "step: 5118, loss: 0.001570328720845282\n",
      "step: 5119, loss: 0.006842591799795628\n",
      "step: 5120, loss: 0.0016678214306011796\n",
      "step: 5121, loss: 0.0007582726539112628\n",
      "step: 5122, loss: 0.014083128422498703\n",
      "step: 5123, loss: 0.0015734672779217362\n",
      "step: 5124, loss: 0.026093965396285057\n",
      "step: 5125, loss: 0.00035567476879805326\n",
      "step: 5126, loss: 0.01024045143276453\n",
      "step: 5127, loss: 0.00012084597983630374\n",
      "step: 5128, loss: 0.001887032063677907\n",
      "step: 5129, loss: 0.001532441470772028\n",
      "step: 5130, loss: 0.0011560347629711032\n",
      "step: 5131, loss: 0.006893259938806295\n",
      "step: 5132, loss: 0.07079260051250458\n",
      "step: 5133, loss: 0.007412736304104328\n",
      "step: 5134, loss: 0.0007436613668687642\n",
      "step: 5135, loss: 0.003927184268832207\n",
      "step: 5136, loss: 0.00124122214037925\n",
      "step: 5137, loss: 0.01076886523514986\n",
      "step: 5138, loss: 0.04188802093267441\n",
      "step: 5139, loss: 0.0009811902418732643\n",
      "step: 5140, loss: 0.0489811934530735\n",
      "step: 5141, loss: 0.03404732421040535\n",
      "step: 5142, loss: 0.006291073746979237\n",
      "step: 5143, loss: 0.003233330324292183\n",
      "step: 5144, loss: 0.01234359759837389\n",
      "step: 5145, loss: 0.09012996405363083\n",
      "step: 5146, loss: 0.003702891990542412\n",
      "step: 5147, loss: 0.0006910552619956434\n",
      "step: 5148, loss: 0.0032549703028053045\n",
      "step: 5149, loss: 0.1941230744123459\n",
      "step: 5150, loss: 0.0465274378657341\n",
      "step: 5151, loss: 0.00039351999294012785\n",
      "step: 5152, loss: 0.04374736174941063\n",
      "step: 5153, loss: 0.0022507032845169306\n",
      "step: 5154, loss: 0.0009424947784282267\n",
      "step: 5155, loss: 0.0011932267807424068\n",
      "step: 5156, loss: 0.08413755148649216\n",
      "step: 5157, loss: 0.005913080181926489\n",
      "step: 5158, loss: 0.008991992101073265\n",
      "step: 5159, loss: 0.0007011459092609584\n",
      "step: 5160, loss: 0.004584439564496279\n",
      "step: 5161, loss: 0.012487923726439476\n",
      "step: 5162, loss: 0.0013180577661842108\n",
      "step: 5163, loss: 0.09600498527288437\n",
      "step: 5164, loss: 0.06489313393831253\n",
      "step: 5165, loss: 0.0012208230327814817\n",
      "step: 5166, loss: 0.025101857259869576\n",
      "step: 5167, loss: 0.0034044829662889242\n",
      "step: 5168, loss: 0.005818154662847519\n",
      "step: 5169, loss: 0.003370141377672553\n",
      "step: 5170, loss: 0.007236279081553221\n",
      "step: 5171, loss: 0.009592344984412193\n",
      "step: 5172, loss: 0.008893790654838085\n",
      "step: 5173, loss: 0.004900111351162195\n",
      "step: 5174, loss: 0.0011092377826571465\n",
      "step: 5175, loss: 0.007299118675291538\n",
      "step: 5176, loss: 0.05426515266299248\n",
      "step: 5177, loss: 0.5303756594657898\n",
      "step: 5178, loss: 0.09089231491088867\n",
      "step: 5179, loss: 0.0008280155016109347\n",
      "step: 5180, loss: 0.0020453985780477524\n",
      "step: 5181, loss: 0.0011915466748178005\n",
      "step: 5182, loss: 0.05943543463945389\n",
      "step: 5183, loss: 0.12680669128894806\n",
      "step: 5184, loss: 0.05322541669011116\n",
      "step: 5185, loss: 0.044556427747011185\n",
      "step: 5186, loss: 0.007028091698884964\n",
      "step: 5187, loss: 0.08808659762144089\n",
      "step: 5188, loss: 0.00040338904364034534\n",
      "step: 5189, loss: 0.00046660174848511815\n",
      "step: 5190, loss: 0.0343930758535862\n",
      "step: 5191, loss: 0.05069698020815849\n",
      "step: 5192, loss: 0.0073112603276968\n",
      "step: 5193, loss: 0.00401659682393074\n",
      "step: 5194, loss: 0.01057960744947195\n",
      "step: 5195, loss: 0.004975495859980583\n",
      "step: 5196, loss: 0.0005976727697998285\n",
      "step: 5197, loss: 0.0335470475256443\n",
      "step: 5198, loss: 0.03084234520792961\n",
      "step: 5199, loss: 0.0025550746358931065\n",
      "step: 5200, loss: 0.012119000777602196\n",
      "step: 5201, loss: 0.008510499261319637\n",
      "step: 5202, loss: 0.051950111985206604\n",
      "step: 5203, loss: 0.1261187344789505\n",
      "step: 5204, loss: 0.003440156811848283\n",
      "step: 5205, loss: 0.04718925431370735\n",
      "step: 5206, loss: 0.0011550657218322158\n",
      "step: 5207, loss: 0.004842524416744709\n",
      "step: 5208, loss: 0.0013963786186650395\n",
      "step: 5209, loss: 0.012523757293820381\n",
      "step: 5210, loss: 0.0020580170676112175\n",
      "step: 5211, loss: 0.00222858227789402\n",
      "step: 5212, loss: 0.0005348618724383414\n",
      "step: 5213, loss: 0.005227029789239168\n",
      "step: 5214, loss: 0.0011675335699692369\n",
      "step: 5215, loss: 0.002535395324230194\n",
      "step: 5216, loss: 0.042870841920375824\n",
      "step: 5217, loss: 0.00902308989316225\n",
      "step: 5218, loss: 0.0007785051129758358\n",
      "step: 5219, loss: 0.003583152312785387\n",
      "step: 5220, loss: 0.001570888445712626\n",
      "step: 5221, loss: 0.00939102005213499\n",
      "step: 5222, loss: 0.00545335840433836\n",
      "step: 5223, loss: 0.0005174450343474746\n",
      "step: 5224, loss: 0.05149693042039871\n",
      "step: 5225, loss: 0.0007493097800761461\n",
      "step: 5226, loss: 0.07196713238954544\n",
      "step: 5227, loss: 0.0031851923558861017\n",
      "step: 5228, loss: 0.008679482154548168\n",
      "step: 5229, loss: 0.0024807637091726065\n",
      "step: 5230, loss: 0.001652217353694141\n",
      "step: 5231, loss: 0.05179194360971451\n",
      "step: 5232, loss: 0.05385340750217438\n",
      "step: 5233, loss: 0.046119142323732376\n",
      "step: 5234, loss: 0.001265184604562819\n",
      "step: 5235, loss: 0.0007175013306550682\n",
      "step: 5236, loss: 0.0025921848136931658\n",
      "step: 5237, loss: 0.0002541316789574921\n",
      "step: 5238, loss: 0.08874517679214478\n",
      "step: 5239, loss: 0.0949530377984047\n",
      "step: 5240, loss: 0.0013197117950767279\n",
      "step: 5241, loss: 0.003031703643500805\n",
      "step: 5242, loss: 0.002702576108276844\n",
      "step: 5243, loss: 0.06843370944261551\n",
      "step: 5244, loss: 0.00040980815538205206\n",
      "step: 5245, loss: 0.008270896971225739\n",
      "step: 5246, loss: 0.5472269058227539\n",
      "step: 5247, loss: 0.003256281604990363\n",
      "step: 5248, loss: 0.00682310713455081\n",
      "step: 5249, loss: 0.00753532350063324\n",
      "step: 5250, loss: 0.009858246892690659\n",
      "step: 5251, loss: 0.05270885303616524\n",
      "step: 5252, loss: 0.0011834469623863697\n",
      "step: 5253, loss: 0.04551425948739052\n",
      "step: 5254, loss: 0.003049616701900959\n",
      "step: 5255, loss: 0.0007453460711985826\n",
      "step: 5256, loss: 0.001616701134480536\n",
      "step: 5257, loss: 0.0046721454709768295\n",
      "step: 5258, loss: 0.05976652726531029\n",
      "step: 5259, loss: 0.002054607030004263\n",
      "step: 5260, loss: 0.00346567714586854\n",
      "step: 5261, loss: 0.0023204826284199953\n",
      "step: 5262, loss: 0.008226802572607994\n",
      "step: 5263, loss: 0.0010041787754744291\n",
      "step: 5264, loss: 0.08850254118442535\n",
      "step: 5265, loss: 0.0076706199906766415\n",
      "step: 5266, loss: 0.0006975047872401774\n",
      "step: 5267, loss: 0.005842724349349737\n",
      "step: 5268, loss: 0.012750644236803055\n",
      "step: 5269, loss: 0.0005514557706192136\n",
      "step: 5270, loss: 0.04811366647481918\n",
      "step: 5271, loss: 0.04601771757006645\n",
      "step: 5272, loss: 0.1132567971944809\n",
      "step: 5273, loss: 0.01494319923222065\n",
      "step: 5274, loss: 0.007917743176221848\n",
      "step: 5275, loss: 0.0044492571614682674\n",
      "step: 5276, loss: 0.059402160346508026\n",
      "step: 5277, loss: 0.0009586780797690153\n",
      "step: 5278, loss: 0.012861566618084908\n",
      "step: 5279, loss: 0.001317142858169973\n",
      "step: 5280, loss: 0.003417984349653125\n",
      "step: 5281, loss: 0.04767341539263725\n",
      "step: 5282, loss: 0.0028758225962519646\n",
      "step: 5283, loss: 0.008087444119155407\n",
      "step: 5284, loss: 0.0003727406729012728\n",
      "step: 5285, loss: 0.0017364881932735443\n",
      "step: 5286, loss: 0.029418807476758957\n",
      "step: 5287, loss: 0.0019279717234894633\n",
      "step: 5288, loss: 0.00302669289521873\n",
      "step: 5289, loss: 0.023409230634570122\n",
      "step: 5290, loss: 0.04152839630842209\n",
      "step: 5291, loss: 0.037285953760147095\n",
      "step: 5292, loss: 0.032783035188913345\n",
      "step: 5293, loss: 0.0021602504421025515\n",
      "step: 5294, loss: 0.001504611107520759\n",
      "step: 5295, loss: 0.007342123426496983\n",
      "step: 5296, loss: 0.0033522218000143766\n",
      "step: 5297, loss: 0.048618607223033905\n",
      "step: 5298, loss: 0.0015291282907128334\n",
      "step: 5299, loss: 0.22480052709579468\n",
      "step: 5300, loss: 0.0003535800497047603\n",
      "step: 5301, loss: 0.009698472917079926\n",
      "step: 5302, loss: 0.002783565316349268\n",
      "step: 5303, loss: 0.006541012320667505\n",
      "step: 5304, loss: 0.003067021956667304\n",
      "step: 5305, loss: 0.008464408107101917\n",
      "step: 5306, loss: 0.007849636487662792\n",
      "step: 5307, loss: 0.002540609799325466\n",
      "step: 5308, loss: 0.000982856028713286\n",
      "step: 5309, loss: 0.0004702759615611285\n",
      "step: 5310, loss: 0.012872209772467613\n",
      "step: 5311, loss: 0.007839214988052845\n",
      "step: 5312, loss: 0.0008596915868110955\n",
      "step: 5313, loss: 0.007745082955807447\n",
      "step: 5314, loss: 0.0015978919109329581\n",
      "step: 5315, loss: 0.050240516662597656\n",
      "step: 5316, loss: 0.0213351771235466\n",
      "step: 5317, loss: 0.011952369473874569\n",
      "step: 5318, loss: 0.036072343587875366\n",
      "step: 5319, loss: 0.011978746391832829\n",
      "step: 5320, loss: 0.002042877720668912\n",
      "step: 5321, loss: 0.03419863432645798\n",
      "step: 5322, loss: 0.0006170374108478427\n",
      "step: 5323, loss: 0.03674107417464256\n",
      "step: 5324, loss: 0.0054079825058579445\n",
      "step: 5325, loss: 0.0014180317521095276\n",
      "step: 5326, loss: 0.007596408948302269\n",
      "step: 5327, loss: 0.006236540153622627\n",
      "step: 5328, loss: 0.0006217718473635614\n",
      "step: 5329, loss: 0.04791001230478287\n",
      "step: 5330, loss: 0.004995402880012989\n",
      "step: 5331, loss: 0.004265602212399244\n",
      "step: 5332, loss: 0.00755782425403595\n",
      "step: 5333, loss: 0.00964287482202053\n",
      "step: 5334, loss: 0.0035346313379704952\n",
      "step: 5335, loss: 0.008727464824914932\n",
      "step: 5336, loss: 0.0013018144527450204\n",
      "step: 5337, loss: 0.0009104401688091457\n",
      "step: 5338, loss: 0.046950411051511765\n",
      "step: 5339, loss: 0.009283478371798992\n",
      "step: 5340, loss: 0.002412678673863411\n",
      "step: 5341, loss: 0.019001655280590057\n",
      "step: 5342, loss: 0.00033330038422718644\n",
      "step: 5343, loss: 0.009619603864848614\n",
      "step: 5344, loss: 0.04283030331134796\n",
      "step: 5345, loss: 0.0008126041502691805\n",
      "step: 5346, loss: 0.04644886404275894\n",
      "step: 5347, loss: 0.0519905723631382\n",
      "step: 5348, loss: 0.03834180906414986\n",
      "step: 5349, loss: 0.006827585864812136\n",
      "step: 5350, loss: 0.03579391911625862\n",
      "step: 5351, loss: 0.005325423553586006\n",
      "step: 5352, loss: 0.0014808005653321743\n",
      "step: 5353, loss: 0.009050666354596615\n",
      "step: 5354, loss: 0.010654625482857227\n",
      "step: 5355, loss: 0.0017045396380126476\n",
      "step: 5356, loss: 0.024509675800800323\n",
      "step: 5357, loss: 0.0014946843730285764\n",
      "step: 5358, loss: 0.000998307135887444\n",
      "step: 5359, loss: 0.0012337012449279428\n",
      "step: 5360, loss: 0.003552141832187772\n",
      "step: 5361, loss: 0.0008723221253603697\n",
      "step: 5362, loss: 0.045354198664426804\n",
      "step: 5363, loss: 0.013151035644114017\n",
      "step: 5364, loss: 0.0010987130226567388\n",
      "step: 5365, loss: 0.006221905816346407\n",
      "step: 5366, loss: 0.0017768708057701588\n",
      "step: 5367, loss: 0.0053094057366251945\n",
      "step: 5368, loss: 0.009511751122772694\n",
      "step: 5369, loss: 0.000626219785772264\n",
      "step: 5370, loss: 0.0005425417330116034\n",
      "step: 5371, loss: 0.0005109995836392045\n",
      "step: 5372, loss: 0.04437952861189842\n",
      "step: 5373, loss: 0.05741468444466591\n",
      "step: 5374, loss: 0.0008967470494098961\n",
      "step: 5375, loss: 0.0018386038718745112\n",
      "step: 5376, loss: 0.03270506113767624\n",
      "step: 5377, loss: 0.002256023697555065\n",
      "step: 5378, loss: 0.004795889835804701\n",
      "step: 5379, loss: 0.09617897868156433\n",
      "step: 5380, loss: 0.0010029840050265193\n",
      "step: 5381, loss: 0.0012443613959476352\n",
      "step: 5382, loss: 0.001390761462971568\n",
      "step: 5383, loss: 0.0011335777817294002\n",
      "step: 5384, loss: 0.000696352100931108\n",
      "step: 5385, loss: 0.0011007484281435609\n",
      "step: 5386, loss: 0.05406249687075615\n",
      "step: 5387, loss: 0.00765368202701211\n",
      "step: 5388, loss: 0.003506057197228074\n",
      "step: 5389, loss: 0.01411544531583786\n",
      "step: 5390, loss: 0.0004966926062479615\n",
      "step: 5391, loss: 0.054825831204652786\n",
      "step: 5392, loss: 0.0017295405268669128\n",
      "step: 5393, loss: 0.05288362130522728\n",
      "step: 5394, loss: 0.006873522885143757\n",
      "step: 5395, loss: 0.0033336374908685684\n",
      "step: 5396, loss: 0.007512856274843216\n",
      "step: 5397, loss: 0.0013821171596646309\n",
      "step: 5398, loss: 0.01271736342459917\n",
      "step: 5399, loss: 0.046640168875455856\n",
      "step: 5400, loss: 0.0023695454001426697\n",
      "step: 5401, loss: 0.031129924580454826\n",
      "step: 5402, loss: 0.08859574049711227\n",
      "step: 5403, loss: 0.0032248166389763355\n",
      "step: 5404, loss: 0.01313053723424673\n",
      "step: 5405, loss: 0.0016637329244986176\n",
      "step: 5406, loss: 0.0029946903232485056\n",
      "step: 5407, loss: 0.005923372693359852\n",
      "step: 5408, loss: 0.003852616297081113\n",
      "step: 5409, loss: 0.0006972490227781236\n",
      "step: 5410, loss: 0.0012426483444869518\n",
      "step: 5411, loss: 0.00029549558530561626\n",
      "step: 5412, loss: 0.0005293561262078583\n",
      "step: 5413, loss: 0.006339590530842543\n",
      "step: 5414, loss: 0.0016095993341878057\n",
      "step: 5415, loss: 0.011589991860091686\n",
      "step: 5416, loss: 0.0017811645520851016\n",
      "step: 5417, loss: 0.0012041674926877022\n",
      "step: 5418, loss: 0.00250208075158298\n",
      "step: 5419, loss: 0.02884642407298088\n",
      "step: 5420, loss: 0.001524245017208159\n",
      "step: 5421, loss: 0.08692087978124619\n",
      "step: 5422, loss: 0.05598368123173714\n",
      "step: 5423, loss: 0.04246040806174278\n",
      "step: 5424, loss: 0.006567845121026039\n",
      "step: 5425, loss: 0.0005259346798993647\n",
      "step: 5426, loss: 0.0011880084639415145\n",
      "step: 5427, loss: 0.007409880869090557\n",
      "step: 5428, loss: 0.0012545414501801133\n",
      "step: 5429, loss: 0.0019367868080735207\n",
      "step: 5430, loss: 0.0059348964132368565\n",
      "step: 5431, loss: 0.004512527957558632\n",
      "step: 5432, loss: 0.03206299617886543\n",
      "step: 5433, loss: 0.000378646538592875\n",
      "step: 5434, loss: 0.0023385011591017246\n",
      "step: 5435, loss: 0.03871363773941994\n",
      "step: 5436, loss: 0.0073580206371843815\n",
      "step: 5437, loss: 0.00839504599571228\n",
      "step: 5438, loss: 0.03551340475678444\n",
      "step: 5439, loss: 0.0018467134796082973\n",
      "step: 5440, loss: 0.006933761760592461\n",
      "step: 5441, loss: 0.03425365313887596\n",
      "step: 5442, loss: 0.00031490944093093276\n",
      "step: 5443, loss: 0.0032484657131135464\n",
      "step: 5444, loss: 0.004243934992700815\n",
      "step: 5445, loss: 0.007548687979578972\n",
      "step: 5446, loss: 0.0007616271614097059\n",
      "step: 5447, loss: 0.538096010684967\n",
      "step: 5448, loss: 0.006366982590407133\n",
      "step: 5449, loss: 0.05434618890285492\n",
      "step: 5450, loss: 0.0003897049755323678\n",
      "step: 5451, loss: 0.0019621048122644424\n",
      "step: 5452, loss: 0.005829010624438524\n",
      "step: 5453, loss: 0.02546905353665352\n",
      "step: 5454, loss: 0.008303727023303509\n",
      "step: 5455, loss: 0.0007398945745080709\n",
      "step: 5456, loss: 0.002572560915723443\n",
      "step: 5457, loss: 0.0033513798844069242\n",
      "step: 5458, loss: 0.0297755915671587\n",
      "step: 5459, loss: 0.000813174236100167\n",
      "step: 5460, loss: 0.007927763275802135\n",
      "step: 5461, loss: 0.001535826944746077\n",
      "step: 5462, loss: 0.0023082089610397816\n",
      "step: 5463, loss: 0.00045163201866671443\n",
      "step: 5464, loss: 0.009008314460515976\n",
      "step: 5465, loss: 0.0004456921305973083\n",
      "step: 5466, loss: 0.0020541001576930285\n",
      "step: 5467, loss: 0.002182531636208296\n",
      "step: 5468, loss: 0.0006076450226828456\n",
      "step: 5469, loss: 0.04802351072430611\n",
      "step: 5470, loss: 0.0028046139050275087\n",
      "step: 5471, loss: 0.09283731132745743\n",
      "step: 5472, loss: 0.0005628970102407038\n",
      "step: 5473, loss: 0.0013640153920277953\n",
      "step: 5474, loss: 9.694210166344419e-05\n",
      "step: 5475, loss: 0.002342559862881899\n",
      "step: 5476, loss: 0.035142894834280014\n",
      "step: 5477, loss: 0.000777924491558224\n",
      "step: 5478, loss: 0.00040351797360926867\n",
      "step: 5479, loss: 0.01686614193022251\n",
      "step: 5480, loss: 0.006817651446908712\n",
      "step: 5481, loss: 0.012117446400225163\n",
      "step: 5482, loss: 0.002098378026857972\n",
      "step: 5483, loss: 0.0013647248269990087\n",
      "step: 5484, loss: 0.0025655562058091164\n",
      "step: 5485, loss: 0.014625315554440022\n",
      "step: 5486, loss: 0.0793510228395462\n",
      "step: 5487, loss: 0.002633614931255579\n",
      "step: 5488, loss: 0.04910654574632645\n",
      "step: 5489, loss: 0.002343008993193507\n",
      "step: 5490, loss: 0.0071024177595973015\n",
      "step: 5491, loss: 0.007938755676150322\n",
      "step: 5492, loss: 0.003788188798353076\n",
      "step: 5493, loss: 0.0210981797426939\n",
      "step: 5494, loss: 0.0026169568300247192\n",
      "step: 5495, loss: 0.0008264604839496315\n",
      "step: 5496, loss: 0.015026253648102283\n",
      "step: 5497, loss: 0.0024626264348626137\n",
      "step: 5498, loss: 0.08636000752449036\n",
      "step: 5499, loss: 0.13631676137447357\n",
      "step: 5500, loss: 0.04246904328465462\n",
      "step: 5501, loss: 0.0013807201758027077\n",
      "step: 5502, loss: 0.0015685842372477055\n",
      "step: 5503, loss: 0.002028333256021142\n",
      "step: 5504, loss: 0.05603118985891342\n",
      "step: 5505, loss: 0.052909187972545624\n",
      "step: 5506, loss: 0.0007230157498270273\n",
      "step: 5507, loss: 0.0016070851124823093\n",
      "step: 5508, loss: 0.10169705748558044\n",
      "step: 5509, loss: 0.002573734614998102\n",
      "step: 5510, loss: 0.002643642947077751\n",
      "step: 5511, loss: 0.08970130234956741\n",
      "step: 5512, loss: 0.0039449287578463554\n",
      "step: 5513, loss: 0.029636375606060028\n",
      "step: 5514, loss: 0.0012409300543367863\n",
      "step: 5515, loss: 0.0061122034676373005\n",
      "step: 5516, loss: 0.0008624896872788668\n",
      "step: 5517, loss: 0.0038218218833208084\n",
      "step: 5518, loss: 0.0024228410329669714\n",
      "step: 5519, loss: 0.05268323794007301\n",
      "step: 5520, loss: 0.0027826682198792696\n",
      "step: 5521, loss: 0.026466690003871918\n",
      "step: 5522, loss: 0.07871490716934204\n",
      "step: 5523, loss: 0.0014872683677822351\n",
      "step: 5524, loss: 0.013779916800558567\n",
      "step: 5525, loss: 0.00028500446933321655\n",
      "step: 5526, loss: 0.0002659976598806679\n",
      "step: 5527, loss: 0.0024327151477336884\n",
      "step: 5528, loss: 0.013776618987321854\n",
      "step: 5529, loss: 0.006013127043843269\n",
      "step: 5530, loss: 0.02127581648528576\n",
      "step: 5531, loss: 0.4264601767063141\n",
      "step: 5532, loss: 0.0026813691947609186\n",
      "step: 5533, loss: 0.0026982324197888374\n",
      "step: 5534, loss: 0.0009219906642101705\n",
      "step: 5535, loss: 0.09658357501029968\n",
      "step: 5536, loss: 0.0016101301880553365\n",
      "step: 5537, loss: 0.019434625282883644\n",
      "step: 5538, loss: 0.01519401092082262\n",
      "step: 5539, loss: 0.009554442018270493\n",
      "step: 5540, loss: 0.0029131590854376554\n",
      "step: 5541, loss: 0.006700498517602682\n",
      "step: 5542, loss: 0.010689778253436089\n",
      "step: 5543, loss: 0.0001743648899719119\n",
      "step: 5544, loss: 0.030250104144215584\n",
      "step: 5545, loss: 0.0937742069363594\n",
      "step: 5546, loss: 0.0631658136844635\n",
      "step: 5547, loss: 0.0020053735934197903\n",
      "step: 5548, loss: 0.0008013160550035536\n",
      "step: 5549, loss: 0.000478754926007241\n",
      "step: 5550, loss: 0.05039618909358978\n",
      "step: 5551, loss: 0.007331343367695808\n",
      "step: 5552, loss: 0.0023955705109983683\n",
      "step: 5553, loss: 0.015211238525807858\n",
      "step: 5554, loss: 0.0007942571537569165\n",
      "step: 5555, loss: 0.01582663506269455\n",
      "step: 5556, loss: 0.0007848338573239744\n",
      "step: 5557, loss: 0.006565643008798361\n",
      "step: 5558, loss: 0.02008199691772461\n",
      "step: 5559, loss: 0.0029703346081078053\n",
      "step: 5560, loss: 0.001929309917613864\n",
      "step: 5561, loss: 0.0007676343084312975\n",
      "step: 5562, loss: 0.05200016126036644\n",
      "step: 5563, loss: 0.0026092559564858675\n",
      "step: 5564, loss: 0.007573165465146303\n",
      "step: 5565, loss: 0.016627822071313858\n",
      "step: 5566, loss: 0.058596543967723846\n",
      "step: 5567, loss: 0.03936053439974785\n",
      "step: 5568, loss: 0.0006932378746569157\n",
      "step: 5569, loss: 0.030877595767378807\n",
      "step: 5570, loss: 4.842276393901557e-05\n",
      "step: 5571, loss: 0.04430942237377167\n",
      "step: 5572, loss: 0.0004970931331627071\n",
      "step: 5573, loss: 0.0059843240305781364\n",
      "step: 5574, loss: 0.040347423404455185\n",
      "step: 5575, loss: 0.10349978506565094\n",
      "step: 5576, loss: 0.010546701960265636\n",
      "step: 5577, loss: 0.0015329024754464626\n",
      "step: 5578, loss: 0.05215461552143097\n",
      "step: 5579, loss: 0.002827326999977231\n",
      "step: 5580, loss: 0.002173728309571743\n",
      "step: 5581, loss: 0.18947337567806244\n",
      "step: 5582, loss: 0.06169692426919937\n",
      "step: 5583, loss: 0.007586585357785225\n",
      "step: 5584, loss: 0.005190828815102577\n",
      "step: 5585, loss: 0.002287850948050618\n",
      "step: 5586, loss: 0.0006936126155778766\n",
      "step: 5587, loss: 0.000812946876976639\n",
      "step: 5588, loss: 0.007604239042848349\n",
      "step: 5589, loss: 0.0040360367856919765\n",
      "step: 5590, loss: 0.0008890127646736801\n",
      "step: 5591, loss: 0.002853463403880596\n",
      "step: 5592, loss: 0.04984334856271744\n",
      "step: 5593, loss: 0.00854950025677681\n",
      "step: 5594, loss: 0.05601758509874344\n",
      "step: 5595, loss: 0.004367274697870016\n",
      "step: 5596, loss: 0.029604602605104446\n",
      "step: 5597, loss: 0.0005215757992118597\n",
      "step: 5598, loss: 0.0049839382991194725\n",
      "step: 5599, loss: 0.002224902156740427\n",
      "step: 5600, loss: 0.0030251136049628258\n",
      "step: 5601, loss: 0.0679282546043396\n",
      "step: 5602, loss: 0.057778846472501755\n",
      "step: 5603, loss: 0.015744177624583244\n",
      "step: 5604, loss: 0.010973499156534672\n",
      "step: 5605, loss: 0.006114038173109293\n",
      "step: 5606, loss: 0.0015079538570716977\n",
      "step: 5607, loss: 0.0008776874165050685\n",
      "step: 5608, loss: 0.05790539085865021\n",
      "step: 5609, loss: 0.0068117608316242695\n",
      "step: 5610, loss: 0.009631851688027382\n",
      "step: 5611, loss: 0.0014612979721277952\n",
      "step: 5612, loss: 0.0005945917801000178\n",
      "step: 5613, loss: 0.0009366151061840355\n",
      "step: 5614, loss: 0.03936496004462242\n",
      "step: 5615, loss: 0.0020738826133310795\n",
      "step: 5616, loss: 0.004007821902632713\n",
      "step: 5617, loss: 0.04309621453285217\n",
      "step: 5618, loss: 0.010274520143866539\n",
      "step: 5619, loss: 0.0006272605969570577\n",
      "step: 5620, loss: 0.001021654694341123\n",
      "step: 5621, loss: 0.04728098213672638\n",
      "step: 5622, loss: 0.010744242928922176\n",
      "step: 5623, loss: 0.0016522338846698403\n",
      "step: 5624, loss: 0.006133421789854765\n",
      "step: 5625, loss: 0.03081154264509678\n",
      "step: 5626, loss: 0.0008246441138908267\n",
      "step: 5627, loss: 0.0027263849042356014\n",
      "step: 5628, loss: 0.02686220407485962\n",
      "step: 5629, loss: 0.013423124328255653\n",
      "step: 5630, loss: 0.008994572795927525\n",
      "step: 5631, loss: 0.00033010035986080766\n",
      "step: 5632, loss: 0.01013635378330946\n",
      "step: 5633, loss: 0.002242924179881811\n",
      "step: 5634, loss: 0.0016773698152974248\n",
      "step: 5635, loss: 0.0016286666505038738\n",
      "step: 5636, loss: 0.020699478685855865\n",
      "step: 5637, loss: 0.002908692928031087\n",
      "step: 5638, loss: 0.005591770634055138\n",
      "step: 5639, loss: 0.0009280936210416257\n",
      "step: 5640, loss: 0.03618781641125679\n",
      "step: 5641, loss: 0.22472117841243744\n",
      "step: 5642, loss: 0.148843914270401\n",
      "step: 5643, loss: 0.0006886620540171862\n",
      "step: 5644, loss: 0.005273115821182728\n",
      "step: 5645, loss: 0.03411421179771423\n",
      "step: 5646, loss: 0.04522278904914856\n",
      "step: 5647, loss: 0.003579631680622697\n",
      "step: 5648, loss: 0.0009935983689501882\n",
      "step: 5649, loss: 0.004125040490180254\n",
      "step: 5650, loss: 0.4436924457550049\n",
      "step: 5651, loss: 0.0005276298034004867\n",
      "step: 5652, loss: 0.0012786600273102522\n",
      "step: 5653, loss: 0.09997572749853134\n",
      "step: 5654, loss: 0.0008469611639156938\n",
      "step: 5655, loss: 0.0019334581447765231\n",
      "step: 5656, loss: 0.05471698194742203\n",
      "step: 5657, loss: 0.0005464137066155672\n",
      "step: 5658, loss: 0.005514694377779961\n",
      "step: 5659, loss: 0.0037339322734624147\n",
      "step: 5660, loss: 0.00319668254815042\n",
      "step: 5661, loss: 0.026273835450410843\n",
      "step: 5662, loss: 0.0006528185331262648\n",
      "step: 5663, loss: 0.0001980127563001588\n",
      "step: 5664, loss: 0.004330461844801903\n",
      "step: 5665, loss: 0.004864363465458155\n",
      "step: 5666, loss: 0.04837577044963837\n",
      "step: 5667, loss: 0.001291708555072546\n",
      "step: 5668, loss: 0.0010296822292730212\n",
      "step: 5669, loss: 0.007585080806165934\n",
      "step: 5670, loss: 0.014186982996761799\n",
      "step: 5671, loss: 0.0010720177087932825\n",
      "step: 5672, loss: 0.00038538698572665453\n",
      "step: 5673, loss: 0.04178304225206375\n",
      "step: 5674, loss: 0.05999965965747833\n",
      "step: 5675, loss: 0.0015055197291076183\n",
      "step: 5676, loss: 0.001626057201065123\n",
      "step: 5677, loss: 0.010847869329154491\n",
      "step: 5678, loss: 0.052847038954496384\n",
      "step: 5679, loss: 0.0008856990025378764\n",
      "step: 5680, loss: 0.0022533293813467026\n",
      "step: 5681, loss: 0.00270296772941947\n",
      "step: 5682, loss: 0.0023903113324195147\n",
      "step: 5683, loss: 0.03829441964626312\n",
      "step: 5684, loss: 0.011460351757705212\n",
      "step: 5685, loss: 0.004187111742794514\n",
      "step: 5686, loss: 0.0028510859701782465\n",
      "step: 5687, loss: 0.0008133837836794555\n",
      "step: 5688, loss: 0.0010747432243078947\n",
      "step: 5689, loss: 0.0604625828564167\n",
      "step: 5690, loss: 0.002331419847905636\n",
      "step: 5691, loss: 0.0012630891287699342\n",
      "step: 5692, loss: 0.03921134024858475\n",
      "step: 5693, loss: 0.018936175853013992\n",
      "step: 5694, loss: 0.044251590967178345\n",
      "step: 5695, loss: 0.002714636968448758\n",
      "step: 5696, loss: 0.008604529313743114\n",
      "step: 5697, loss: 0.012316236272454262\n",
      "step: 5698, loss: 0.02444133535027504\n",
      "step: 5699, loss: 0.0028818557038903236\n",
      "step: 5700, loss: 0.0007960256771184504\n",
      "step: 5701, loss: 0.0004864604852627963\n",
      "step: 5702, loss: 0.14719921350479126\n",
      "step: 5703, loss: 0.0496537908911705\n",
      "step: 5704, loss: 0.0034839087165892124\n",
      "step: 5705, loss: 0.004674792289733887\n",
      "step: 5706, loss: 0.04643459618091583\n",
      "step: 5707, loss: 0.0025240532122552395\n",
      "step: 5708, loss: 0.008412168361246586\n",
      "step: 5709, loss: 0.00297941779717803\n",
      "step: 5710, loss: 0.08084338903427124\n",
      "step: 5711, loss: 0.0027332082390785217\n",
      "step: 5712, loss: 0.00046773263602517545\n",
      "step: 5713, loss: 0.008134633302688599\n",
      "step: 5714, loss: 0.0138041190803051\n",
      "step: 5715, loss: 0.0008812568848952651\n",
      "step: 5716, loss: 0.0021132808178663254\n",
      "step: 5717, loss: 0.0016503583174198866\n",
      "step: 5718, loss: 0.0007936736801639199\n",
      "step: 5719, loss: 0.002561707515269518\n",
      "step: 5720, loss: 0.0011568786576390266\n",
      "step: 5721, loss: 0.0033420163672417402\n",
      "step: 5722, loss: 0.02688593603670597\n",
      "step: 5723, loss: 0.0013329044450074434\n",
      "step: 5724, loss: 0.0035256736446172\n",
      "step: 5725, loss: 0.01741122640669346\n",
      "step: 5726, loss: 0.0026537023950368166\n",
      "step: 5727, loss: 0.0024807918816804886\n",
      "step: 5728, loss: 0.0007262192666530609\n",
      "step: 5729, loss: 0.0023018093779683113\n",
      "step: 5730, loss: 0.012236096896231174\n",
      "step: 5731, loss: 0.003748317714780569\n",
      "step: 5732, loss: 0.0062564341351389885\n",
      "step: 5733, loss: 0.0008485251455567777\n",
      "step: 5734, loss: 0.0008054646314121783\n",
      "step: 5735, loss: 0.00043021529563702643\n",
      "step: 5736, loss: 0.0012353942729532719\n",
      "step: 5737, loss: 0.0011725289514288306\n",
      "step: 5738, loss: 0.009251661598682404\n",
      "step: 5739, loss: 0.05140931159257889\n",
      "step: 5740, loss: 0.05462704598903656\n",
      "step: 5741, loss: 0.0554959774017334\n",
      "step: 5742, loss: 0.008665073662996292\n",
      "step: 5743, loss: 0.0013301296858116984\n",
      "step: 5744, loss: 0.0015148371458053589\n",
      "step: 5745, loss: 0.0015686765545979142\n",
      "step: 5746, loss: 0.001264659222215414\n",
      "step: 5747, loss: 0.05111655592918396\n",
      "step: 5748, loss: 0.012465324252843857\n",
      "step: 5749, loss: 0.008294694125652313\n",
      "step: 5750, loss: 0.0018734568729996681\n",
      "step: 5751, loss: 0.011385173536837101\n",
      "step: 5752, loss: 0.01075656432658434\n",
      "step: 5753, loss: 0.00026116525987163186\n",
      "step: 5754, loss: 0.04081454873085022\n",
      "step: 5755, loss: 0.007202520966529846\n",
      "step: 5756, loss: 0.01319933496415615\n",
      "step: 5757, loss: 0.0672755017876625\n",
      "step: 5758, loss: 0.0425625704228878\n",
      "step: 5759, loss: 0.009730811230838299\n",
      "step: 5760, loss: 0.00213955738581717\n",
      "step: 5761, loss: 0.03723406046628952\n",
      "step: 5762, loss: 0.0006348953465931118\n",
      "step: 5763, loss: 0.014746922999620438\n",
      "step: 5764, loss: 0.0004253967199474573\n",
      "step: 5765, loss: 0.006903736386448145\n",
      "step: 5766, loss: 0.026698598638176918\n",
      "step: 5767, loss: 0.0012249562423676252\n",
      "step: 5768, loss: 0.005135055631399155\n",
      "step: 5769, loss: 0.03653703257441521\n",
      "step: 5770, loss: 0.0020884184632450342\n",
      "step: 5771, loss: 0.0036555370315909386\n",
      "step: 5772, loss: 0.0008443731348961592\n",
      "step: 5773, loss: 0.0035329300444573164\n",
      "step: 5774, loss: 0.37533554434776306\n",
      "step: 5775, loss: 0.0016750420909374952\n",
      "step: 5776, loss: 0.0006088993977755308\n",
      "step: 5777, loss: 0.0007402215851470828\n",
      "step: 5778, loss: 0.0013397153234109282\n",
      "step: 5779, loss: 0.059935376048088074\n",
      "step: 5780, loss: 0.0016786050982773304\n",
      "step: 5781, loss: 0.0061599635519087315\n",
      "step: 5782, loss: 0.001271965797059238\n",
      "step: 5783, loss: 0.049241796135902405\n",
      "step: 5784, loss: 0.019742725417017937\n",
      "step: 5785, loss: 0.0006688747089356184\n",
      "step: 5786, loss: 0.010360338725149632\n",
      "step: 5787, loss: 0.03210967406630516\n",
      "step: 5788, loss: 0.002481835661455989\n",
      "step: 5789, loss: 0.008947794325649738\n",
      "step: 5790, loss: 0.04893781244754791\n",
      "step: 5791, loss: 0.001990808406844735\n",
      "step: 5792, loss: 0.000886728463228792\n",
      "step: 5793, loss: 0.0008041572291404009\n",
      "step: 5794, loss: 0.006486884318292141\n",
      "step: 5795, loss: 0.03783755004405975\n",
      "step: 5796, loss: 0.0019546938128769398\n",
      "step: 5797, loss: 0.025973401963710785\n",
      "step: 5798, loss: 0.12591372430324554\n",
      "step: 5799, loss: 0.04264010488986969\n",
      "step: 5800, loss: 0.007050202693790197\n",
      "step: 5801, loss: 0.04394237697124481\n",
      "step: 5802, loss: 0.00048377952771261334\n",
      "step: 5803, loss: 0.009752814657986164\n",
      "step: 5804, loss: 0.043526314198970795\n",
      "step: 5805, loss: 0.05182337760925293\n",
      "step: 5806, loss: 0.0008964784210547805\n",
      "step: 5807, loss: 0.000891299219802022\n",
      "step: 5808, loss: 0.05555777996778488\n",
      "step: 5809, loss: 0.0012128346133977175\n",
      "step: 5810, loss: 0.00258736964315176\n",
      "step: 5811, loss: 0.0041452194564044476\n",
      "step: 5812, loss: 0.0004920546780340374\n",
      "step: 5813, loss: 0.0007229328039102256\n",
      "step: 5814, loss: 0.03474719449877739\n",
      "step: 5815, loss: 0.05230886489152908\n",
      "step: 5816, loss: 0.0012089743977412581\n",
      "step: 5817, loss: 0.0574561208486557\n",
      "step: 5818, loss: 0.019798967987298965\n",
      "step: 5819, loss: 0.009906147606670856\n",
      "step: 5820, loss: 0.0028994346503168344\n",
      "step: 5821, loss: 0.000450446386821568\n",
      "step: 5822, loss: 0.002226474229246378\n",
      "step: 5823, loss: 0.07907573878765106\n",
      "step: 5824, loss: 0.027275603264570236\n",
      "step: 5825, loss: 0.009305641986429691\n",
      "step: 5826, loss: 0.010405106469988823\n",
      "step: 5827, loss: 0.023338336497545242\n",
      "step: 5828, loss: 0.04235924407839775\n",
      "step: 5829, loss: 0.05872900038957596\n",
      "step: 5830, loss: 0.0007592155016027391\n",
      "step: 5831, loss: 0.003417535685002804\n",
      "step: 5832, loss: 0.02639455534517765\n",
      "step: 5833, loss: 0.01984594389796257\n",
      "step: 5834, loss: 0.0024374390486627817\n",
      "step: 5835, loss: 0.002949028043076396\n",
      "step: 5836, loss: 0.013959881849586964\n",
      "step: 5837, loss: 0.007703672628849745\n",
      "step: 5838, loss: 0.05785925313830376\n",
      "step: 5839, loss: 0.002374758943915367\n",
      "step: 5840, loss: 0.002952906535938382\n",
      "step: 5841, loss: 0.049205757677555084\n",
      "step: 5842, loss: 0.0015092610847204924\n",
      "step: 5843, loss: 0.0018062448361888528\n",
      "step: 5844, loss: 0.09218750894069672\n",
      "step: 5845, loss: 0.007399858441203833\n",
      "step: 5846, loss: 0.0006122834165580571\n",
      "step: 5847, loss: 0.0031165056861937046\n",
      "step: 5848, loss: 0.0006910406518727541\n",
      "step: 5849, loss: 0.006785571109503508\n",
      "step: 5850, loss: 0.04551013559103012\n",
      "step: 5851, loss: 0.15497492253780365\n",
      "step: 5852, loss: 0.0017021404346451163\n",
      "step: 5853, loss: 0.06335241347551346\n",
      "step: 5854, loss: 0.005275200121104717\n",
      "step: 5855, loss: 0.0166181568056345\n",
      "step: 5856, loss: 0.0009382995194755495\n",
      "step: 5857, loss: 0.001282844808883965\n",
      "step: 5858, loss: 0.000515247811563313\n",
      "step: 5859, loss: 0.008878003805875778\n",
      "step: 5860, loss: 0.0010718568228185177\n",
      "step: 5861, loss: 0.006397572346031666\n",
      "step: 5862, loss: 0.006037428509443998\n",
      "step: 5863, loss: 0.00272556534036994\n",
      "step: 5864, loss: 0.04595841467380524\n",
      "step: 5865, loss: 0.002971435198560357\n",
      "step: 5866, loss: 0.058608345687389374\n",
      "step: 5867, loss: 0.06813055276870728\n",
      "step: 5868, loss: 0.0007672730134800076\n",
      "step: 5869, loss: 0.00044588075252249837\n",
      "step: 5870, loss: 0.0010794441914185882\n",
      "step: 5871, loss: 0.016316816210746765\n",
      "step: 5872, loss: 0.005833320785313845\n",
      "step: 5873, loss: 0.012507421895861626\n",
      "step: 5874, loss: 0.0048399316146969795\n",
      "step: 5875, loss: 0.051513444632291794\n",
      "step: 5876, loss: 0.0264351274818182\n",
      "step: 5877, loss: 0.0010518302442505956\n",
      "step: 5878, loss: 0.06855475157499313\n",
      "step: 5879, loss: 0.0011078461539000273\n",
      "step: 5880, loss: 0.0015447383048012853\n",
      "step: 5881, loss: 0.00489830831065774\n",
      "step: 5882, loss: 0.033147264271974564\n",
      "step: 5883, loss: 0.001957576721906662\n",
      "step: 5884, loss: 0.002505182521417737\n",
      "step: 5885, loss: 0.010453591123223305\n",
      "step: 5886, loss: 0.0012683847453445196\n",
      "step: 5887, loss: 0.04103998839855194\n",
      "step: 5888, loss: 0.19182296097278595\n",
      "step: 5889, loss: 0.02145332843065262\n",
      "step: 5890, loss: 0.0029059331864118576\n",
      "step: 5891, loss: 0.042390771210193634\n",
      "step: 5892, loss: 0.03468332439661026\n",
      "step: 5893, loss: 0.011692201718688011\n",
      "step: 5894, loss: 0.0025657820515334606\n",
      "step: 5895, loss: 0.0015455282991752028\n",
      "step: 5896, loss: 0.000916166405659169\n",
      "step: 5897, loss: 0.05540616437792778\n",
      "step: 5898, loss: 0.005747570190578699\n",
      "step: 5899, loss: 0.019941335543990135\n",
      "step: 5900, loss: 0.0004143908154219389\n",
      "step: 5901, loss: 0.003126707626506686\n",
      "step: 5902, loss: 0.0052851345390081406\n",
      "step: 5903, loss: 0.0030315308831632137\n",
      "step: 5904, loss: 0.029934005811810493\n",
      "step: 5905, loss: 0.0013492980506271124\n",
      "step: 5906, loss: 0.0017465688288211823\n",
      "step: 5907, loss: 0.04850177466869354\n",
      "step: 5908, loss: 0.018305260688066483\n",
      "step: 5909, loss: 0.0021991475950926542\n",
      "step: 5910, loss: 0.000580995692871511\n",
      "step: 5911, loss: 0.05744331702589989\n",
      "step: 5912, loss: 0.022728806361556053\n",
      "step: 5913, loss: 0.006971316412091255\n",
      "step: 5914, loss: 0.004982520826160908\n",
      "step: 5915, loss: 0.004713648930191994\n",
      "step: 5916, loss: 0.000805868417955935\n",
      "step: 5917, loss: 0.00359937222674489\n",
      "step: 5918, loss: 0.029753683134913445\n",
      "step: 5919, loss: 0.005153145641088486\n",
      "step: 5920, loss: 0.13573960959911346\n",
      "step: 5921, loss: 0.0006783799617551267\n",
      "step: 5922, loss: 0.0057266950607299805\n",
      "step: 5923, loss: 0.006009365897625685\n",
      "step: 5924, loss: 0.0008366233669221401\n",
      "step: 5925, loss: 0.005375874228775501\n",
      "step: 5926, loss: 0.01621878892183304\n",
      "step: 5927, loss: 0.003380072768777609\n",
      "step: 5928, loss: 0.09590177237987518\n",
      "step: 5929, loss: 0.033802423626184464\n",
      "step: 5930, loss: 0.005519600585103035\n",
      "step: 5931, loss: 0.0022758913692086935\n",
      "step: 5932, loss: 0.026405081152915955\n",
      "step: 5933, loss: 0.013857394456863403\n",
      "step: 5934, loss: 0.035570524632930756\n",
      "step: 5935, loss: 0.12800665199756622\n",
      "step: 5936, loss: 0.002446137834340334\n",
      "step: 5937, loss: 0.0007283185259439051\n",
      "step: 5938, loss: 0.002811507787555456\n",
      "step: 5939, loss: 0.003687503980472684\n",
      "step: 5940, loss: 0.0008749344269745052\n",
      "step: 5941, loss: 0.0006051359232515097\n",
      "step: 5942, loss: 0.05057043209671974\n",
      "step: 5943, loss: 0.0006315785576589406\n",
      "step: 5944, loss: 0.03955976292490959\n",
      "step: 5945, loss: 0.0037815666291862726\n",
      "step: 5946, loss: 0.0019123952370136976\n",
      "step: 5947, loss: 0.015691695734858513\n",
      "step: 5948, loss: 0.059200700372457504\n",
      "step: 5949, loss: 0.034887030720710754\n",
      "step: 5950, loss: 0.0034880333114415407\n",
      "step: 5951, loss: 0.007486662827432156\n",
      "step: 5952, loss: 0.007133712060749531\n",
      "step: 5953, loss: 0.05139923095703125\n",
      "step: 5954, loss: 0.0006554176798090339\n",
      "step: 5955, loss: 0.005223926156759262\n",
      "step: 5956, loss: 0.10544757544994354\n",
      "step: 5957, loss: 0.0005954087246209383\n",
      "step: 5958, loss: 0.007757715415209532\n",
      "step: 5959, loss: 0.001220662030391395\n",
      "step: 5960, loss: 0.007999950088560581\n",
      "step: 5961, loss: 0.0328255258500576\n",
      "step: 5962, loss: 0.006440669763833284\n",
      "step: 5963, loss: 0.04741327837109566\n",
      "step: 5964, loss: 0.00028130083228461444\n",
      "step: 5965, loss: 0.0007864608778618276\n",
      "step: 5966, loss: 0.0027206363156437874\n",
      "step: 5967, loss: 0.001209292677231133\n",
      "step: 5968, loss: 0.0016638151137158275\n",
      "step: 5969, loss: 0.010496864095330238\n",
      "step: 5970, loss: 0.001091189798898995\n",
      "step: 5971, loss: 0.04854949563741684\n",
      "step: 5972, loss: 0.3199518918991089\n",
      "step: 5973, loss: 0.03488091006875038\n",
      "step: 5974, loss: 0.0011406997218728065\n",
      "step: 5975, loss: 0.004215551540255547\n",
      "step: 5976, loss: 0.04241359233856201\n",
      "step: 5977, loss: 0.008842067793011665\n",
      "step: 5978, loss: 0.0014048052253201604\n",
      "step: 5979, loss: 0.05723850801587105\n",
      "step: 5980, loss: 0.0025365338660776615\n",
      "step: 5981, loss: 0.000796815671492368\n",
      "step: 5982, loss: 0.05048079416155815\n",
      "step: 5983, loss: 0.0008578194538131356\n",
      "step: 5984, loss: 0.0014491673791781068\n",
      "step: 5985, loss: 0.08289894461631775\n",
      "step: 5986, loss: 0.0008015704224817455\n",
      "step: 5987, loss: 0.03393808752298355\n",
      "step: 5988, loss: 0.00019062288629356772\n",
      "step: 5989, loss: 0.034469787031412125\n",
      "step: 5990, loss: 0.004182640463113785\n",
      "step: 5991, loss: 0.0015061821322888136\n",
      "step: 5992, loss: 0.007988850586116314\n",
      "step: 5993, loss: 0.0010667955502867699\n",
      "step: 5994, loss: 0.00045848757144995034\n",
      "step: 5995, loss: 0.0012634310405701399\n",
      "step: 5996, loss: 0.05301655828952789\n",
      "step: 5997, loss: 0.0009407478501088917\n",
      "step: 5998, loss: 0.0028966828249394894\n",
      "step: 5999, loss: 0.04576534777879715\n",
      "step: 6000, loss: 0.005273823160678148\n",
      "step: 6001, loss: 0.0010150800226256251\n",
      "step: 6002, loss: 0.03692037984728813\n",
      "step: 6003, loss: 0.0012676826445385814\n",
      "step: 6004, loss: 0.0020598387345671654\n",
      "step: 6005, loss: 0.05413972958922386\n",
      "step: 6006, loss: 0.0015667015686631203\n",
      "step: 6007, loss: 0.0030994010157883167\n",
      "step: 6008, loss: 0.04824695736169815\n",
      "step: 6009, loss: 0.00816164631396532\n",
      "step: 6010, loss: 0.005090564489364624\n",
      "step: 6011, loss: 0.0020380918867886066\n",
      "step: 6012, loss: 0.0013725849566981196\n",
      "step: 6013, loss: 0.00309921195730567\n",
      "step: 6014, loss: 0.01585998386144638\n",
      "step: 6015, loss: 0.0013590737944468856\n",
      "step: 6016, loss: 0.0036819693632423878\n",
      "step: 6017, loss: 0.009096814319491386\n",
      "step: 6018, loss: 0.0010878794128075242\n",
      "step: 6019, loss: 0.07002546638250351\n",
      "step: 6020, loss: 0.001814858173020184\n",
      "step: 6021, loss: 0.002027004724368453\n",
      "step: 6022, loss: 0.004723593592643738\n",
      "step: 6023, loss: 0.0027100134175270796\n",
      "step: 6024, loss: 0.0011775908060371876\n",
      "step: 6025, loss: 0.04670291393995285\n",
      "step: 6026, loss: 0.0022124273236840963\n",
      "step: 6027, loss: 0.008546878583729267\n",
      "step: 6028, loss: 0.0010575555497780442\n",
      "step: 6029, loss: 0.010849343612790108\n",
      "step: 6030, loss: 0.0008928464958444238\n",
      "step: 6031, loss: 0.009540135972201824\n",
      "step: 6032, loss: 8.714795694686472e-05\n",
      "step: 6033, loss: 0.01217957865446806\n",
      "step: 6034, loss: 0.018377888947725296\n",
      "step: 6035, loss: 0.0008735109586268663\n",
      "step: 6036, loss: 0.0006324511487036943\n",
      "step: 6037, loss: 0.0029308011289685965\n",
      "step: 6038, loss: 0.015566535294055939\n",
      "step: 6039, loss: 0.009457595646381378\n",
      "step: 6040, loss: 0.05485911667346954\n",
      "step: 6041, loss: 0.04609842225909233\n",
      "step: 6042, loss: 0.05871427059173584\n",
      "step: 6043, loss: 0.0026617341209203005\n",
      "step: 6044, loss: 0.0011796612525358796\n",
      "step: 6045, loss: 0.001467880909331143\n",
      "step: 6046, loss: 0.06734341382980347\n",
      "step: 6047, loss: 0.010485862381756306\n",
      "step: 6048, loss: 0.0019012864213436842\n",
      "step: 6049, loss: 0.008424084633588791\n",
      "step: 6050, loss: 0.003362230258062482\n",
      "step: 6051, loss: 0.011209403164684772\n",
      "step: 6052, loss: 0.0014301189221441746\n",
      "step: 6053, loss: 0.0019793808460235596\n",
      "step: 6054, loss: 0.0008697426528669894\n",
      "step: 6055, loss: 0.04996366426348686\n",
      "step: 6056, loss: 0.0010853413259610534\n",
      "step: 6057, loss: 0.0009954838315024972\n",
      "step: 6058, loss: 0.0013645152794197202\n",
      "step: 6059, loss: 0.0008429361623711884\n",
      "step: 6060, loss: 0.00068896688753739\n",
      "step: 6061, loss: 0.00376300816424191\n",
      "step: 6062, loss: 0.009097017347812653\n",
      "step: 6063, loss: 0.057867370545864105\n",
      "step: 6064, loss: 0.00955558754503727\n",
      "step: 6065, loss: 0.0029555251821875572\n",
      "step: 6066, loss: 0.0004206907469779253\n",
      "step: 6067, loss: 0.0007187615265138447\n",
      "step: 6068, loss: 0.001031243009492755\n",
      "step: 6069, loss: 0.0028313794173300266\n",
      "step: 6070, loss: 0.004005769267678261\n",
      "step: 6071, loss: 0.05123560503125191\n",
      "step: 6072, loss: 0.03662191703915596\n",
      "step: 6073, loss: 0.10164037346839905\n",
      "step: 6074, loss: 0.5389026403427124\n",
      "step: 6075, loss: 0.22977697849273682\n",
      "step: 6076, loss: 0.0008218821021728218\n",
      "step: 6077, loss: 0.0028434281703084707\n",
      "step: 6078, loss: 0.0023138218093663454\n",
      "step: 6079, loss: 0.007347093429416418\n",
      "step: 6080, loss: 0.002429352141916752\n",
      "step: 6081, loss: 0.0014836520422250032\n",
      "step: 6082, loss: 0.0014732340350747108\n",
      "step: 6083, loss: 0.006519543472677469\n",
      "step: 6084, loss: 0.0024552256800234318\n",
      "step: 6085, loss: 0.0023421214427798986\n",
      "step: 6086, loss: 0.002373603405430913\n",
      "step: 6087, loss: 0.007538275793194771\n",
      "step: 6088, loss: 0.00888926349580288\n",
      "step: 6089, loss: 0.018811827525496483\n",
      "step: 6090, loss: 0.002625469584017992\n",
      "step: 6091, loss: 0.0029310849495232105\n",
      "step: 6092, loss: 0.010871521197259426\n",
      "step: 6093, loss: 0.009457997046411037\n",
      "step: 6094, loss: 0.0021165127400308847\n",
      "step: 6095, loss: 0.0600278377532959\n",
      "step: 6096, loss: 0.0029448384884744883\n",
      "step: 6097, loss: 0.0009036748670041561\n",
      "step: 6098, loss: 0.0006388325127772987\n",
      "step: 6099, loss: 0.04742702469229698\n",
      "step: 6100, loss: 0.0030336861964315176\n",
      "step: 6101, loss: 0.0015230693388730288\n",
      "step: 6102, loss: 0.045576099306344986\n",
      "step: 6103, loss: 0.0031270370818674564\n",
      "step: 6104, loss: 0.0003782169660553336\n",
      "step: 6105, loss: 0.008464370854198933\n",
      "step: 6106, loss: 0.051227353513240814\n",
      "step: 6107, loss: 0.0003810433263424784\n",
      "step: 6108, loss: 0.0010023987852036953\n",
      "step: 6109, loss: 0.0013521219370886683\n",
      "step: 6110, loss: 0.0015146189834922552\n",
      "step: 6111, loss: 0.000843685120344162\n",
      "step: 6112, loss: 0.0013482406502589583\n",
      "step: 6113, loss: 0.048356715589761734\n",
      "step: 6114, loss: 0.010842213407158852\n",
      "step: 6115, loss: 0.004080734681338072\n",
      "step: 6116, loss: 0.000418502080719918\n",
      "step: 6117, loss: 0.06184246018528938\n",
      "step: 6118, loss: 0.004586602561175823\n",
      "step: 6119, loss: 0.0022923641372472048\n",
      "step: 6120, loss: 0.03582989051938057\n",
      "step: 6121, loss: 0.0029327827505767345\n",
      "step: 6122, loss: 0.0031430593226104975\n",
      "step: 6123, loss: 0.0010125774424523115\n",
      "step: 6124, loss: 5.7244684285251424e-05\n",
      "step: 6125, loss: 0.08163619786500931\n",
      "step: 6126, loss: 0.0012358096428215504\n",
      "step: 6127, loss: 0.0013117530616000295\n",
      "step: 6128, loss: 0.5230600237846375\n",
      "step: 6129, loss: 0.00012138915917603299\n",
      "step: 6130, loss: 0.04880267754197121\n",
      "step: 6131, loss: 0.014777038246393204\n",
      "step: 6132, loss: 0.06628638505935669\n",
      "step: 6133, loss: 0.00292632426135242\n",
      "step: 6134, loss: 0.0005886654835194349\n",
      "step: 6135, loss: 0.0035745184868574142\n",
      "step: 6136, loss: 0.0001594192290212959\n",
      "step: 6137, loss: 0.021114367991685867\n",
      "step: 6138, loss: 0.05238831415772438\n",
      "step: 6139, loss: 0.06340550631284714\n",
      "step: 6140, loss: 0.010756919160485268\n",
      "step: 6141, loss: 0.019951194524765015\n",
      "step: 6142, loss: 0.0016570836305618286\n",
      "step: 6143, loss: 0.010476794093847275\n",
      "step: 6144, loss: 0.0005465256981551647\n",
      "step: 6145, loss: 0.06226501986384392\n",
      "step: 6146, loss: 0.007048787549138069\n",
      "step: 6147, loss: 0.009945272468030453\n",
      "step: 6148, loss: 0.001162195112556219\n",
      "step: 6149, loss: 0.0016547420527786016\n",
      "step: 6150, loss: 0.04900618642568588\n",
      "step: 6151, loss: 0.011967981234192848\n",
      "step: 6152, loss: 0.0009457293199375272\n",
      "step: 6153, loss: 0.005869507789611816\n",
      "step: 6154, loss: 0.0015157890738919377\n",
      "step: 6155, loss: 0.012206447310745716\n",
      "step: 6156, loss: 0.021651525050401688\n",
      "step: 6157, loss: 0.0010083329398185015\n",
      "step: 6158, loss: 0.00041949376463890076\n",
      "step: 6159, loss: 0.051150862127542496\n",
      "step: 6160, loss: 0.14199750125408173\n",
      "step: 6161, loss: 0.0001800332247512415\n",
      "step: 6162, loss: 0.010411001741886139\n",
      "step: 6163, loss: 0.001378721441142261\n",
      "step: 6164, loss: 0.01642138883471489\n",
      "step: 6165, loss: 0.01779871992766857\n",
      "step: 6166, loss: 0.0010798638686537743\n",
      "step: 6167, loss: 0.00023597963445354253\n",
      "step: 6168, loss: 0.0013620886020362377\n",
      "step: 6169, loss: 0.00011559559789020568\n",
      "step: 6170, loss: 0.0011678035371005535\n",
      "step: 6171, loss: 0.0031689011957496405\n",
      "step: 6172, loss: 0.0004874849983025342\n",
      "step: 6173, loss: 0.0009234377648681402\n",
      "step: 6174, loss: 0.0008746262174099684\n",
      "step: 6175, loss: 0.0333554707467556\n",
      "step: 6176, loss: 0.0007616895600222051\n",
      "step: 6177, loss: 0.10420680046081543\n",
      "step: 6178, loss: 0.00029044729308225214\n",
      "step: 6179, loss: 0.002111123874783516\n",
      "step: 6180, loss: 0.01572069711983204\n",
      "step: 6181, loss: 0.05813300982117653\n",
      "step: 6182, loss: 0.0006980212056078017\n",
      "step: 6183, loss: 0.001176979742012918\n",
      "step: 6184, loss: 0.038219135254621506\n",
      "step: 6185, loss: 0.0021584813948720694\n",
      "step: 6186, loss: 0.0024590736720710993\n",
      "step: 6187, loss: 0.04760637879371643\n",
      "step: 6188, loss: 0.002295328304171562\n",
      "step: 6189, loss: 0.047754786908626556\n",
      "step: 6190, loss: 0.0016233468195423484\n",
      "step: 6191, loss: 0.04662806913256645\n",
      "step: 6192, loss: 0.02502233348786831\n",
      "step: 6193, loss: 0.008095014840364456\n",
      "step: 6194, loss: 0.000646946660708636\n",
      "step: 6195, loss: 0.022354135289788246\n",
      "step: 6196, loss: 0.032503701746463776\n",
      "step: 6197, loss: 0.0033828511368483305\n",
      "step: 6198, loss: 0.001764835324138403\n",
      "step: 6199, loss: 0.023693952709436417\n",
      "step: 6200, loss: 0.0013569805305451155\n",
      "step: 6201, loss: 0.022998256608843803\n",
      "step: 6202, loss: 0.03394827991724014\n",
      "step: 6203, loss: 0.0024013621732592583\n",
      "step: 6204, loss: 0.0918026715517044\n",
      "step: 6205, loss: 0.0012689699651673436\n",
      "step: 6206, loss: 0.0002597998536657542\n",
      "step: 6207, loss: 0.011641805991530418\n",
      "step: 6208, loss: 0.0012502748286351562\n",
      "step: 6209, loss: 5.761452121078037e-05\n",
      "step: 6210, loss: 0.0007596337236464024\n",
      "step: 6211, loss: 0.0021887090988457203\n",
      "step: 6212, loss: 0.051337432116270065\n",
      "step: 6213, loss: 0.051602743566036224\n",
      "step: 6214, loss: 0.008059396408498287\n",
      "step: 6215, loss: 0.0010403201449662447\n",
      "step: 6216, loss: 0.020305225625634193\n",
      "step: 6217, loss: 0.010942362248897552\n",
      "step: 6218, loss: 0.0236202422529459\n",
      "step: 6219, loss: 0.0006041454034857452\n",
      "step: 6220, loss: 0.0012478010030463338\n",
      "step: 6221, loss: 0.009661940857768059\n",
      "step: 6222, loss: 0.31688061356544495\n",
      "step: 6223, loss: 0.0033476436510682106\n",
      "step: 6224, loss: 0.0014060166431590915\n",
      "step: 6225, loss: 0.03818623721599579\n",
      "step: 6226, loss: 0.00043627392733469605\n",
      "step: 6227, loss: 0.14909769594669342\n",
      "step: 6228, loss: 0.055478814989328384\n",
      "step: 6229, loss: 0.012392200529575348\n",
      "step: 6230, loss: 0.002269824966788292\n",
      "step: 6231, loss: 0.0008988922345452011\n",
      "step: 6232, loss: 0.0023819440975785255\n",
      "step: 6233, loss: 0.00028513860888779163\n",
      "step: 6234, loss: 5.9567868447629735e-05\n",
      "step: 6235, loss: 0.004070086404681206\n",
      "step: 6236, loss: 0.0008044971618801355\n",
      "step: 6237, loss: 0.0135216498747468\n",
      "step: 6238, loss: 0.0016063397051766515\n",
      "step: 6239, loss: 0.0015844786539673805\n",
      "step: 6240, loss: 0.0008784483652561903\n",
      "step: 6241, loss: 0.00976339727640152\n",
      "step: 6242, loss: 0.0029647881165146828\n",
      "step: 6243, loss: 0.08722613006830215\n",
      "step: 6244, loss: 0.018070951104164124\n",
      "step: 6245, loss: 0.0035983440466225147\n",
      "step: 6246, loss: 0.00337805668823421\n",
      "step: 6247, loss: 0.0007742908783257008\n",
      "step: 6248, loss: 0.007392592262476683\n",
      "step: 6249, loss: 0.0017591253854334354\n",
      "step: 6250, loss: 0.011304352432489395\n",
      "step: 6251, loss: 0.02586742490530014\n",
      "step: 6252, loss: 0.0006307133007794619\n",
      "step: 6253, loss: 0.013230850920081139\n",
      "step: 6254, loss: 0.009802395477890968\n",
      "step: 6255, loss: 0.0012276467168703675\n",
      "step: 6256, loss: 0.002149837324395776\n",
      "step: 6257, loss: 0.021748848259449005\n",
      "step: 6258, loss: 0.015979884192347527\n",
      "step: 6259, loss: 0.0029186394531279802\n",
      "step: 6260, loss: 0.013806012459099293\n",
      "step: 6261, loss: 0.0003208366397302598\n",
      "step: 6262, loss: 0.0011202141176909208\n",
      "step: 6263, loss: 0.0014217427233234048\n",
      "step: 6264, loss: 0.0011227827053517103\n",
      "step: 6265, loss: 0.0829847902059555\n",
      "step: 6266, loss: 0.0020825639367103577\n",
      "step: 6267, loss: 0.007506717462092638\n",
      "step: 6268, loss: 0.0013784742914140224\n",
      "step: 6269, loss: 0.030660640448331833\n",
      "step: 6270, loss: 0.0033045869786292315\n",
      "step: 6271, loss: 0.05213238298892975\n",
      "step: 6272, loss: 0.0009767379378899932\n",
      "step: 6273, loss: 0.002643094165250659\n",
      "step: 6274, loss: 0.0034680122043937445\n",
      "step: 6275, loss: 0.004026760347187519\n",
      "step: 6276, loss: 0.007426002994179726\n",
      "step: 6277, loss: 0.04965456947684288\n",
      "step: 6278, loss: 0.0026174604427069426\n",
      "step: 6279, loss: 0.008973876014351845\n",
      "step: 6280, loss: 0.003628843231126666\n",
      "step: 6281, loss: 0.0028802675660699606\n",
      "step: 6282, loss: 0.02233024686574936\n",
      "step: 6283, loss: 0.016593314707279205\n",
      "step: 6284, loss: 0.012017534114420414\n",
      "step: 6285, loss: 0.0024479907006025314\n",
      "step: 6286, loss: 0.0013791891979053617\n",
      "step: 6287, loss: 0.0025826662313193083\n",
      "step: 6288, loss: 0.013751648366451263\n",
      "step: 6289, loss: 0.004078407306224108\n",
      "step: 6290, loss: 0.0036029471084475517\n",
      "step: 6291, loss: 0.0017611761577427387\n",
      "step: 6292, loss: 0.0025267975870519876\n",
      "step: 6293, loss: 0.018312370404601097\n",
      "step: 6294, loss: 0.001151087461039424\n",
      "step: 6295, loss: 0.0017601394793018699\n",
      "step: 6296, loss: 0.0012378640240058303\n",
      "step: 6297, loss: 0.04845208674669266\n",
      "step: 6298, loss: 0.08642386645078659\n",
      "step: 6299, loss: 0.0004846572410315275\n",
      "step: 6300, loss: 0.00863394420593977\n",
      "step: 6301, loss: 0.002802430186420679\n",
      "step: 6302, loss: 0.011440729722380638\n",
      "step: 6303, loss: 0.080750972032547\n",
      "step: 6304, loss: 0.0697568953037262\n",
      "step: 6305, loss: 0.03897827863693237\n",
      "step: 6306, loss: 0.0008377899066545069\n",
      "step: 6307, loss: 0.11050888895988464\n",
      "step: 6308, loss: 0.01165751926600933\n",
      "step: 6309, loss: 0.014860481955111027\n",
      "step: 6310, loss: 0.0024958185385912657\n",
      "step: 6311, loss: 0.009893609210848808\n",
      "step: 6312, loss: 0.0355137437582016\n",
      "step: 6313, loss: 0.057152632623910904\n",
      "step: 6314, loss: 0.023618686944246292\n",
      "step: 6315, loss: 0.0005186364287510514\n",
      "step: 6316, loss: 0.0009387877071276307\n",
      "step: 6317, loss: 0.010547191835939884\n",
      "step: 6318, loss: 0.03389575332403183\n",
      "step: 6319, loss: 0.04098277911543846\n",
      "step: 6320, loss: 0.002039906568825245\n",
      "step: 6321, loss: 0.003127817530184984\n",
      "step: 6322, loss: 0.00114035839214921\n",
      "step: 6323, loss: 0.016736265271902084\n",
      "step: 6324, loss: 0.001375694526359439\n",
      "step: 6325, loss: 0.05002495273947716\n",
      "step: 6326, loss: 0.004210196901112795\n",
      "step: 6327, loss: 0.03840909153223038\n",
      "step: 6328, loss: 0.0021851188503205776\n",
      "step: 6329, loss: 0.001762524712830782\n",
      "step: 6330, loss: 0.004003112204372883\n",
      "step: 6331, loss: 0.030046945437788963\n",
      "step: 6332, loss: 0.0232761949300766\n",
      "step: 6333, loss: 0.0037762534338980913\n",
      "step: 6334, loss: 0.0007955917972140014\n",
      "step: 6335, loss: 0.016923535615205765\n",
      "step: 6336, loss: 0.009084392338991165\n",
      "step: 6337, loss: 0.0096899988129735\n",
      "step: 6338, loss: 0.0004985283594578505\n",
      "step: 6339, loss: 0.000939244928304106\n",
      "step: 6340, loss: 0.00054460036335513\n",
      "step: 6341, loss: 0.0015742849791422486\n",
      "step: 6342, loss: 0.0016196408541873097\n",
      "step: 6343, loss: 0.0009249435388483107\n",
      "step: 6344, loss: 0.008901768364012241\n",
      "step: 6345, loss: 0.013723056763410568\n",
      "step: 6346, loss: 0.04838433861732483\n",
      "step: 6347, loss: 0.0010883144568651915\n",
      "step: 6348, loss: 0.0013903172221034765\n",
      "step: 6349, loss: 0.06365411728620529\n",
      "step: 6350, loss: 0.013582472689449787\n",
      "step: 6351, loss: 0.01114944089204073\n",
      "step: 6352, loss: 0.060577888041734695\n",
      "step: 6353, loss: 0.024500269442796707\n",
      "step: 6354, loss: 0.0008273650892078876\n",
      "step: 6355, loss: 0.0009375373483635485\n",
      "step: 6356, loss: 0.04575042799115181\n",
      "step: 6357, loss: 0.003278097603470087\n",
      "step: 6358, loss: 0.0016472340794280171\n",
      "step: 6359, loss: 0.0059768809005618095\n",
      "step: 6360, loss: 0.001092703314498067\n",
      "step: 6361, loss: 0.0012149685062468052\n",
      "step: 6362, loss: 0.04435945674777031\n",
      "step: 6363, loss: 0.0005685688811354339\n",
      "step: 6364, loss: 0.0030294088646769524\n",
      "step: 6365, loss: 0.0026806422974914312\n",
      "step: 6366, loss: 0.001793809817172587\n",
      "step: 6367, loss: 0.0007052755099721253\n",
      "step: 6368, loss: 0.011920887976884842\n",
      "step: 6369, loss: 0.002353862626478076\n",
      "step: 6370, loss: 0.03736982122063637\n",
      "step: 6371, loss: 0.04870054870843887\n",
      "step: 6372, loss: 0.012669241055846214\n",
      "step: 6373, loss: 0.04998137801885605\n",
      "step: 6374, loss: 0.007955149747431278\n",
      "step: 6375, loss: 0.005269048269838095\n",
      "step: 6376, loss: 0.05501040816307068\n",
      "step: 6377, loss: 0.0022440108004957438\n",
      "step: 6378, loss: 0.0011859978549182415\n",
      "step: 6379, loss: 0.09387385100126266\n",
      "step: 6380, loss: 0.0007176175713539124\n",
      "step: 6381, loss: 0.06392239034175873\n",
      "step: 6382, loss: 0.02488265186548233\n",
      "step: 6383, loss: 0.0013100509531795979\n",
      "step: 6384, loss: 0.03916934132575989\n",
      "step: 6385, loss: 0.0013509165728464723\n",
      "step: 6386, loss: 0.061522092670202255\n",
      "step: 6387, loss: 0.0020250685047358274\n",
      "step: 6388, loss: 0.0644935742020607\n",
      "step: 6389, loss: 0.031979188323020935\n",
      "step: 6390, loss: 0.04824313148856163\n",
      "step: 6391, loss: 0.0013544656103476882\n",
      "step: 6392, loss: 0.051221877336502075\n",
      "step: 6393, loss: 0.059077244251966476\n",
      "step: 6394, loss: 0.0020125785376876593\n",
      "step: 6395, loss: 0.0028894487768411636\n",
      "step: 6396, loss: 0.003741539316251874\n",
      "step: 6397, loss: 0.010143599472939968\n",
      "step: 6398, loss: 0.0016023281496018171\n",
      "step: 6399, loss: 0.0008793786400929093\n",
      "step: 6400, loss: 0.012667945586144924\n",
      "step: 6401, loss: 0.004488295875489712\n",
      "step: 6402, loss: 0.050227392464876175\n",
      "step: 6403, loss: 0.0019286904716864228\n",
      "step: 6404, loss: 0.0031031996477395296\n",
      "step: 6405, loss: 0.0015027648769319057\n",
      "step: 6406, loss: 0.0023445698898285627\n",
      "step: 6407, loss: 0.0005744523368775845\n",
      "step: 6408, loss: 0.005127036012709141\n",
      "step: 6409, loss: 0.048951636999845505\n",
      "step: 6410, loss: 0.00218524900265038\n",
      "step: 6411, loss: 0.04623968154191971\n",
      "step: 6412, loss: 0.0007687211036682129\n",
      "step: 6413, loss: 0.0006808058824390173\n",
      "step: 6414, loss: 0.00959183368831873\n",
      "step: 6415, loss: 0.00023413141025230289\n",
      "step: 6416, loss: 0.012146622873842716\n",
      "step: 6417, loss: 0.0006318952073343098\n",
      "step: 6418, loss: 0.0009334628120996058\n",
      "step: 6419, loss: 0.004570264834910631\n",
      "step: 6420, loss: 0.04246809333562851\n",
      "step: 6421, loss: 0.003098514163866639\n",
      "step: 6422, loss: 0.0002541644498705864\n",
      "step: 6423, loss: 0.0011784499511122704\n",
      "step: 6424, loss: 0.08828955888748169\n",
      "step: 6425, loss: 0.04897082969546318\n",
      "step: 6426, loss: 0.049931157380342484\n",
      "step: 6427, loss: 0.008353786543011665\n",
      "step: 6428, loss: 0.004734438844025135\n",
      "step: 6429, loss: 0.0006972465198487043\n",
      "step: 6430, loss: 0.0012801013654097915\n",
      "step: 6431, loss: 0.04939333349466324\n",
      "step: 6432, loss: 0.018939023837447166\n",
      "step: 6433, loss: 0.04871787875890732\n",
      "step: 6434, loss: 0.04614625126123428\n",
      "step: 6435, loss: 0.0008052827324718237\n",
      "step: 6436, loss: 0.003216780722141266\n",
      "step: 6437, loss: 0.0005510151386260986\n",
      "step: 6438, loss: 0.03520241752266884\n",
      "step: 6439, loss: 0.002971054520457983\n",
      "step: 6440, loss: 0.05153454840183258\n",
      "step: 6441, loss: 0.008713573217391968\n",
      "step: 6442, loss: 0.0012502814643085003\n",
      "step: 6443, loss: 0.010932017117738724\n",
      "step: 6444, loss: 0.003010088112205267\n",
      "step: 6445, loss: 0.0019441461190581322\n",
      "step: 6446, loss: 0.03370450809597969\n",
      "step: 6447, loss: 0.011456651613116264\n",
      "step: 6448, loss: 0.03690147027373314\n",
      "step: 6449, loss: 0.0011338534532114863\n",
      "step: 6450, loss: 0.0025459195021539927\n",
      "step: 6451, loss: 0.10137683898210526\n",
      "step: 6452, loss: 0.012124435976147652\n",
      "step: 6453, loss: 0.0010971985757350922\n",
      "step: 6454, loss: 0.002066511893644929\n",
      "step: 6455, loss: 0.001789215486496687\n",
      "step: 6456, loss: 0.009099269285798073\n",
      "step: 6457, loss: 0.010605101473629475\n",
      "step: 6458, loss: 0.0462183952331543\n",
      "step: 6459, loss: 0.08612345904111862\n",
      "step: 6460, loss: 0.004054066725075245\n",
      "step: 6461, loss: 0.03638945519924164\n",
      "step: 6462, loss: 0.0004862795758526772\n",
      "step: 6463, loss: 0.0013087119441479445\n",
      "step: 6464, loss: 0.0023714532144367695\n",
      "step: 6465, loss: 0.008595643565058708\n",
      "step: 6466, loss: 0.06754252314567566\n",
      "step: 6467, loss: 0.0003692879108712077\n",
      "step: 6468, loss: 0.04543938487768173\n",
      "step: 6469, loss: 0.33426177501678467\n",
      "step: 6470, loss: 0.04667556285858154\n",
      "step: 6471, loss: 0.00022651077597402036\n",
      "step: 6472, loss: 0.011707233265042305\n",
      "step: 6473, loss: 0.0029906861018389463\n",
      "step: 6474, loss: 0.0009305189014412463\n",
      "step: 6475, loss: 0.01380306575447321\n",
      "step: 6476, loss: 0.001907721278257668\n",
      "step: 6477, loss: 0.008562898263335228\n",
      "step: 6478, loss: 0.011232593096792698\n",
      "step: 6479, loss: 0.027962494641542435\n",
      "step: 6480, loss: 0.004211716819554567\n",
      "step: 6481, loss: 0.0016543033998459578\n",
      "step: 6482, loss: 0.0012669196585193276\n",
      "step: 6483, loss: 0.12460698187351227\n",
      "step: 6484, loss: 0.015168769285082817\n",
      "step: 6485, loss: 0.043616827577352524\n",
      "step: 6486, loss: 0.12025624513626099\n",
      "step: 6487, loss: 0.0032964011188596487\n",
      "step: 6488, loss: 0.0054433816112577915\n",
      "step: 6489, loss: 0.00700844032689929\n",
      "step: 6490, loss: 0.0030216446612030268\n",
      "step: 6491, loss: 0.07283901423215866\n",
      "step: 6492, loss: 0.002439209260046482\n",
      "step: 6493, loss: 0.09228448569774628\n",
      "step: 6494, loss: 0.0007648690952919424\n",
      "step: 6495, loss: 0.03628452867269516\n",
      "step: 6496, loss: 0.014120211824774742\n",
      "step: 6497, loss: 0.059496503323316574\n",
      "step: 6498, loss: 0.002202979288995266\n",
      "step: 6499, loss: 0.0007559203077107668\n",
      "step: 6500, loss: 0.009321562014520168\n",
      "step: 6501, loss: 0.004427364096045494\n",
      "step: 6502, loss: 0.001587647246196866\n",
      "step: 6503, loss: 0.0015572701813653111\n",
      "step: 6504, loss: 0.5416934490203857\n",
      "step: 6505, loss: 0.011149253696203232\n",
      "step: 6506, loss: 0.002529191318899393\n",
      "step: 6507, loss: 0.0002977999683935195\n",
      "step: 6508, loss: 0.0030725516844540834\n",
      "step: 6509, loss: 0.015134956687688828\n",
      "step: 6510, loss: 0.0005640782183036208\n",
      "step: 6511, loss: 0.026950426399707794\n",
      "step: 6512, loss: 0.11617308855056763\n",
      "step: 6513, loss: 0.02412995882332325\n",
      "step: 6514, loss: 0.0033355671912431717\n",
      "step: 6515, loss: 0.0062240599654614925\n",
      "step: 6516, loss: 0.00037686433643102646\n",
      "step: 6517, loss: 0.005257445387542248\n",
      "step: 6518, loss: 0.001213177922181785\n",
      "step: 6519, loss: 0.0008405239786952734\n",
      "step: 6520, loss: 0.002854120684787631\n",
      "step: 6521, loss: 0.0076058972626924515\n",
      "step: 6522, loss: 0.03786979988217354\n",
      "step: 6523, loss: 0.0014924833085387945\n",
      "step: 6524, loss: 0.16253894567489624\n",
      "step: 6525, loss: 4.488446575123817e-05\n",
      "step: 6526, loss: 0.012521302327513695\n",
      "step: 6527, loss: 0.045715004205703735\n",
      "step: 6528, loss: 0.001737305661663413\n",
      "step: 6529, loss: 0.041954778134822845\n",
      "step: 6530, loss: 0.0030309269204735756\n",
      "step: 6531, loss: 0.0472087636590004\n",
      "step: 6532, loss: 0.00044932274613529444\n",
      "step: 6533, loss: 0.049048274755477905\n",
      "step: 6534, loss: 0.002653063042089343\n",
      "step: 6535, loss: 0.013549375347793102\n",
      "step: 6536, loss: 0.0012154420837759972\n",
      "step: 6537, loss: 0.0005838806391693652\n",
      "step: 6538, loss: 0.004134590271860361\n",
      "step: 6539, loss: 0.0025553822051733732\n",
      "step: 6540, loss: 0.0009480626904405653\n",
      "step: 6541, loss: 0.0012898541754111648\n",
      "step: 6542, loss: 0.0044182962737977505\n",
      "step: 6543, loss: 0.000764944648835808\n",
      "step: 6544, loss: 0.010655289515852928\n",
      "step: 6545, loss: 0.316837340593338\n",
      "step: 6546, loss: 0.002379860496148467\n",
      "step: 6547, loss: 0.011578134261071682\n",
      "step: 6548, loss: 0.009608455002307892\n",
      "step: 6549, loss: 0.0015414752997457981\n",
      "step: 6550, loss: 0.001522487960755825\n",
      "step: 6551, loss: 0.013889285735785961\n",
      "step: 6552, loss: 0.0029157281387597322\n",
      "step: 6553, loss: 0.03504751995205879\n",
      "step: 6554, loss: 0.0033385667484253645\n",
      "step: 6555, loss: 0.002644466469064355\n",
      "step: 6556, loss: 0.0011441708775237203\n",
      "step: 6557, loss: 0.0020462488755583763\n",
      "step: 6558, loss: 0.017819790169596672\n",
      "step: 6559, loss: 0.009437285363674164\n",
      "step: 6560, loss: 0.0006371967610903084\n",
      "step: 6561, loss: 0.10480362921953201\n",
      "step: 6562, loss: 0.002697594463825226\n",
      "step: 6563, loss: 0.00013996948837302625\n",
      "step: 6564, loss: 0.0003353734209667891\n",
      "step: 6565, loss: 0.0020112451165914536\n",
      "step: 6566, loss: 0.06689991056919098\n",
      "step: 6567, loss: 0.003822608618065715\n",
      "step: 6568, loss: 0.052251581102609634\n",
      "step: 6569, loss: 0.0011521396227180958\n",
      "step: 6570, loss: 0.03486279770731926\n",
      "step: 6571, loss: 0.003003950696438551\n",
      "step: 6572, loss: 0.00017741377814672887\n",
      "step: 6573, loss: 0.2944539189338684\n",
      "step: 6574, loss: 0.0009688151767477393\n",
      "step: 6575, loss: 0.011349395848810673\n",
      "step: 6576, loss: 0.010442894883453846\n",
      "step: 6577, loss: 0.004736226983368397\n",
      "step: 6578, loss: 0.010504249483346939\n",
      "step: 6579, loss: 0.04767966642975807\n",
      "step: 6580, loss: 0.003746012458577752\n",
      "step: 6581, loss: 0.0014348886907100677\n",
      "step: 6582, loss: 0.005016797222197056\n",
      "step: 6583, loss: 0.1737336665391922\n",
      "step: 6584, loss: 0.01685498096048832\n",
      "step: 6585, loss: 0.038679443299770355\n",
      "step: 6586, loss: 0.0015270967269316316\n",
      "step: 6587, loss: 0.002335382392629981\n",
      "step: 6588, loss: 0.0007694459054619074\n",
      "step: 6589, loss: 0.003392757847905159\n",
      "step: 6590, loss: 0.027414273470640182\n",
      "step: 6591, loss: 0.05554709956049919\n",
      "step: 6592, loss: 0.007023045793175697\n",
      "step: 6593, loss: 0.0011838657082989812\n",
      "step: 6594, loss: 0.0007617746014147997\n",
      "step: 6595, loss: 0.0024918364360928535\n",
      "step: 6596, loss: 0.002627233974635601\n",
      "step: 6597, loss: 0.00038717538700439036\n",
      "step: 6598, loss: 0.025417398661375046\n",
      "step: 6599, loss: 0.001534919487312436\n",
      "step: 6600, loss: 0.0026056200731545687\n",
      "step: 6601, loss: 0.008625310845673084\n",
      "step: 6602, loss: 0.0015582782216370106\n",
      "step: 6603, loss: 0.009405933320522308\n",
      "step: 6604, loss: 0.00065040827030316\n",
      "step: 6605, loss: 0.06382672488689423\n",
      "step: 6606, loss: 0.013419363647699356\n",
      "step: 6607, loss: 0.002686050022020936\n",
      "step: 6608, loss: 0.0011802389053627849\n",
      "step: 6609, loss: 0.04177933186292648\n",
      "step: 6610, loss: 0.0025563740637153387\n",
      "step: 6611, loss: 0.0014317441964522004\n",
      "step: 6612, loss: 0.019623160362243652\n",
      "step: 6613, loss: 0.0012060543522238731\n",
      "step: 6614, loss: 0.000725282880011946\n",
      "step: 6615, loss: 0.006519191898405552\n",
      "step: 6616, loss: 0.010080404579639435\n",
      "step: 6617, loss: 0.0011250367388129234\n",
      "step: 6618, loss: 0.03542249649763107\n",
      "step: 6619, loss: 0.04074917733669281\n",
      "step: 6620, loss: 0.0011990276398137212\n",
      "step: 6621, loss: 0.09484413266181946\n",
      "step: 6622, loss: 0.00025322329020127654\n",
      "step: 6623, loss: 0.0022355697583407164\n",
      "step: 6624, loss: 0.002825602190569043\n",
      "step: 6625, loss: 0.0019205962307751179\n",
      "step: 6626, loss: 0.0004619586979970336\n",
      "step: 6627, loss: 0.004880176391452551\n",
      "step: 6628, loss: 0.0011620621662586927\n",
      "step: 6629, loss: 0.19732753932476044\n",
      "step: 6630, loss: 0.001759652397595346\n",
      "step: 6631, loss: 0.0010132883908227086\n",
      "step: 6632, loss: 0.0027273648884147406\n",
      "step: 6633, loss: 0.001697307568974793\n",
      "step: 6634, loss: 0.009900868870317936\n",
      "step: 6635, loss: 0.0010955524630844593\n",
      "step: 6636, loss: 0.002559412969276309\n",
      "step: 6637, loss: 0.009014773182570934\n",
      "step: 6638, loss: 0.008030936121940613\n",
      "step: 6639, loss: 0.012373464182019234\n",
      "step: 6640, loss: 0.08020136505365372\n",
      "step: 6641, loss: 0.09194537997245789\n",
      "step: 6642, loss: 0.033018626272678375\n",
      "step: 6643, loss: 0.033438172191381454\n",
      "step: 6644, loss: 0.0008986613247543573\n",
      "step: 6645, loss: 0.003689112374559045\n",
      "step: 6646, loss: 0.001999583328142762\n",
      "step: 6647, loss: 0.050418101251125336\n",
      "step: 6648, loss: 0.005294415168464184\n",
      "step: 6649, loss: 0.04695119336247444\n",
      "step: 6650, loss: 0.0010621583787724376\n",
      "step: 6651, loss: 0.004242392256855965\n",
      "step: 6652, loss: 0.05237850174307823\n",
      "step: 6653, loss: 0.0017823796952143312\n",
      "step: 6654, loss: 0.11487826704978943\n",
      "step: 6655, loss: 0.0006333761848509312\n",
      "step: 6656, loss: 0.0066650863736867905\n",
      "step: 6657, loss: 0.001422047964297235\n",
      "step: 6658, loss: 0.0322808139026165\n",
      "step: 6659, loss: 0.039058148860931396\n",
      "step: 6660, loss: 0.005530792288482189\n",
      "step: 6661, loss: 0.0020887088030576706\n",
      "step: 6662, loss: 0.06393072009086609\n",
      "step: 6663, loss: 0.0005194263067096472\n",
      "step: 6664, loss: 0.0006040406879037619\n",
      "step: 6665, loss: 0.03218997269868851\n",
      "step: 6666, loss: 0.010648949071764946\n",
      "step: 6667, loss: 0.002293798141181469\n",
      "step: 6668, loss: 0.00237860344350338\n",
      "step: 6669, loss: 0.0005720017943531275\n",
      "step: 6670, loss: 0.02151336893439293\n",
      "step: 6671, loss: 0.004593374207615852\n",
      "step: 6672, loss: 0.013753587380051613\n",
      "step: 6673, loss: 0.0006773700006306171\n",
      "step: 6674, loss: 0.052713219076395035\n",
      "step: 6675, loss: 0.0006777732050977647\n",
      "step: 6676, loss: 0.0008755192393437028\n",
      "step: 6677, loss: 0.004177536815404892\n",
      "step: 6678, loss: 0.004941332153975964\n",
      "step: 6679, loss: 0.004123171791434288\n",
      "step: 6680, loss: 0.0024642841890454292\n",
      "step: 6681, loss: 0.038178883492946625\n",
      "step: 6682, loss: 0.008992058224976063\n",
      "step: 6683, loss: 0.04500449076294899\n",
      "step: 6684, loss: 0.0011662696488201618\n",
      "step: 6685, loss: 0.10245542228221893\n",
      "step: 6686, loss: 0.041954245418310165\n",
      "step: 6687, loss: 0.014396542683243752\n",
      "step: 6688, loss: 0.002145016100257635\n",
      "step: 6689, loss: 0.011168957687914371\n",
      "step: 6690, loss: 0.002598185557872057\n",
      "step: 6691, loss: 0.055655673146247864\n",
      "step: 6692, loss: 0.008586997166275978\n",
      "step: 6693, loss: 0.003895872039720416\n",
      "step: 6694, loss: 0.0006318538798950613\n",
      "step: 6695, loss: 0.046163901686668396\n",
      "step: 6696, loss: 0.0013896076707169414\n",
      "step: 6697, loss: 0.055360790342092514\n",
      "step: 6698, loss: 0.001480794046074152\n",
      "step: 6699, loss: 0.0010398434242233634\n",
      "step: 6700, loss: 0.05292712152004242\n",
      "step: 6701, loss: 0.00548538425937295\n",
      "step: 6702, loss: 0.09866707026958466\n",
      "step: 6703, loss: 0.0016519436612725258\n",
      "step: 6704, loss: 0.0014024946140125394\n",
      "step: 6705, loss: 0.00022127859119791538\n",
      "step: 6706, loss: 0.04146840423345566\n",
      "step: 6707, loss: 0.001362477196380496\n",
      "step: 6708, loss: 0.001667367178015411\n",
      "step: 6709, loss: 0.052412476390600204\n",
      "step: 6710, loss: 0.0490221343934536\n",
      "step: 6711, loss: 0.0010567870922386646\n",
      "step: 6712, loss: 0.0009718319633975625\n",
      "step: 6713, loss: 0.05401083454489708\n",
      "step: 6714, loss: 0.0019922631327062845\n",
      "step: 6715, loss: 0.0006167005631141365\n",
      "step: 6716, loss: 0.012905709445476532\n",
      "step: 6717, loss: 0.005363169126212597\n",
      "step: 6718, loss: 0.042139749974012375\n",
      "step: 6719, loss: 0.011778476648032665\n",
      "step: 6720, loss: 0.003986494615674019\n",
      "step: 6721, loss: 0.021341657266020775\n",
      "step: 6722, loss: 0.002133558737114072\n",
      "step: 6723, loss: 0.0008752736612223089\n",
      "step: 6724, loss: 0.1495690494775772\n",
      "step: 6725, loss: 0.0015102890320122242\n",
      "step: 6726, loss: 0.006037531886249781\n",
      "step: 6727, loss: 0.037622276693582535\n",
      "step: 6728, loss: 0.00192954670637846\n",
      "step: 6729, loss: 0.024287505075335503\n",
      "step: 6730, loss: 0.09241542220115662\n",
      "step: 6731, loss: 0.048994749784469604\n",
      "step: 6732, loss: 0.03896205127239227\n",
      "step: 6733, loss: 0.05339185893535614\n",
      "step: 6734, loss: 0.5145000219345093\n",
      "step: 6735, loss: 0.019389968365430832\n",
      "step: 6736, loss: 0.009276089258491993\n",
      "step: 6737, loss: 0.0020052636973559856\n",
      "step: 6738, loss: 0.0008497685194015503\n",
      "step: 6739, loss: 0.01261928677558899\n",
      "step: 6740, loss: 0.0034942023921757936\n",
      "step: 6741, loss: 0.0015569489914923906\n",
      "step: 6742, loss: 0.0015065600164234638\n",
      "step: 6743, loss: 0.0006559172179549932\n",
      "step: 6744, loss: 0.04826388880610466\n",
      "step: 6745, loss: 0.001045589568093419\n",
      "step: 6746, loss: 0.07478585839271545\n",
      "step: 6747, loss: 0.004192003048956394\n",
      "step: 6748, loss: 0.0012065998744219542\n",
      "step: 6749, loss: 0.018135584890842438\n",
      "step: 6750, loss: 0.0004020997148472816\n",
      "step: 6751, loss: 0.0343308225274086\n",
      "step: 6752, loss: 0.004893592558801174\n",
      "step: 6753, loss: 0.0018108004005625844\n",
      "step: 6754, loss: 0.04918398708105087\n",
      "step: 6755, loss: 0.001976317958906293\n",
      "step: 6756, loss: 0.0008710345136933029\n",
      "step: 6757, loss: 0.02404792048037052\n",
      "step: 6758, loss: 0.002345203422009945\n",
      "step: 6759, loss: 0.012922718189656734\n",
      "step: 6760, loss: 0.001239103265106678\n",
      "step: 6761, loss: 0.04574666917324066\n",
      "step: 6762, loss: 0.08309231698513031\n",
      "step: 6763, loss: 0.013739127665758133\n",
      "step: 6764, loss: 0.00042765645775943995\n",
      "step: 6765, loss: 0.0006337400991469622\n",
      "step: 6766, loss: 0.015779349952936172\n",
      "step: 6767, loss: 0.0013232907513156533\n",
      "step: 6768, loss: 0.017953304573893547\n",
      "step: 6769, loss: 0.005578733514994383\n",
      "step: 6770, loss: 0.00038893599412404\n",
      "step: 6771, loss: 0.011995231732726097\n",
      "step: 6772, loss: 0.06177714839577675\n",
      "step: 6773, loss: 0.006856352556496859\n",
      "step: 6774, loss: 0.02033369056880474\n",
      "step: 6775, loss: 0.049980271607637405\n",
      "step: 6776, loss: 0.019673945382237434\n",
      "step: 6777, loss: 0.08239898830652237\n",
      "step: 6778, loss: 0.0006891986122354865\n",
      "step: 6779, loss: 0.03336949273943901\n",
      "step: 6780, loss: 0.002105873776599765\n",
      "step: 6781, loss: 0.001028729253448546\n",
      "step: 6782, loss: 0.06416449695825577\n",
      "step: 6783, loss: 0.0008033609483391047\n",
      "step: 6784, loss: 0.056618280708789825\n",
      "step: 6785, loss: 0.0002749107079580426\n",
      "step: 6786, loss: 0.04305146634578705\n",
      "step: 6787, loss: 0.0003948659577872604\n",
      "step: 6788, loss: 0.009186132811009884\n",
      "step: 6789, loss: 0.002672165632247925\n",
      "step: 6790, loss: 0.0006474370020441711\n",
      "step: 6791, loss: 0.004788194317370653\n",
      "step: 6792, loss: 0.002921618754044175\n",
      "step: 6793, loss: 0.011561094783246517\n",
      "step: 6794, loss: 0.0059258113615214825\n",
      "step: 6795, loss: 0.020271115005016327\n",
      "step: 6796, loss: 0.04769067466259003\n",
      "step: 6797, loss: 0.0017862229142338037\n",
      "step: 6798, loss: 0.0035545534919947386\n",
      "step: 6799, loss: 0.011596654541790485\n",
      "step: 6800, loss: 0.0006014254176989198\n",
      "step: 6801, loss: 0.047298312187194824\n",
      "step: 6802, loss: 0.0017559239640831947\n",
      "step: 6803, loss: 0.014771881513297558\n",
      "step: 6804, loss: 0.0008881192188709974\n",
      "step: 6805, loss: 0.002966419793665409\n",
      "step: 6806, loss: 0.0008854777552187443\n",
      "step: 6807, loss: 0.0007282980368472636\n",
      "step: 6808, loss: 0.0018666534451767802\n",
      "step: 6809, loss: 0.015004614368081093\n",
      "step: 6810, loss: 0.011288456618785858\n",
      "step: 6811, loss: 0.014531170949339867\n",
      "step: 6812, loss: 0.00694917282089591\n",
      "step: 6813, loss: 0.010853519663214684\n",
      "step: 6814, loss: 0.006282338406890631\n",
      "step: 6815, loss: 0.019652539864182472\n",
      "step: 6816, loss: 0.001491962349973619\n",
      "step: 6817, loss: 0.0019460412440821528\n",
      "step: 6818, loss: 0.034361112862825394\n",
      "step: 6819, loss: 0.00121434370521456\n",
      "step: 6820, loss: 0.000715582282282412\n",
      "step: 6821, loss: 0.0003502796171233058\n",
      "step: 6822, loss: 0.030772218480706215\n",
      "step: 6823, loss: 0.002558286301791668\n",
      "step: 6824, loss: 0.04002195969223976\n",
      "step: 6825, loss: 0.06449894607067108\n",
      "step: 6826, loss: 0.0057379319332540035\n",
      "step: 6827, loss: 0.00995591189712286\n",
      "step: 6828, loss: 0.003208506153896451\n",
      "step: 6829, loss: 0.0005770798306912184\n",
      "step: 6830, loss: 0.0009662213851697743\n",
      "step: 6831, loss: 0.0029034987092018127\n",
      "step: 6832, loss: 0.04801376163959503\n",
      "step: 6833, loss: 0.000941369216889143\n",
      "step: 6834, loss: 0.0005492305499501526\n",
      "step: 6835, loss: 0.0035769257228821516\n",
      "step: 6836, loss: 0.004665229469537735\n",
      "step: 6837, loss: 0.004045544657856226\n",
      "step: 6838, loss: 0.01687481813132763\n",
      "step: 6839, loss: 0.003285846905782819\n",
      "step: 6840, loss: 0.0007158195949159563\n",
      "step: 6841, loss: 0.11964817345142365\n",
      "step: 6842, loss: 0.028482310473918915\n",
      "step: 6843, loss: 0.011631547473371029\n",
      "step: 6844, loss: 0.0023367619141936302\n",
      "step: 6845, loss: 0.054627977311611176\n",
      "step: 6846, loss: 0.01567576453089714\n",
      "step: 6847, loss: 0.04777999967336655\n",
      "step: 6848, loss: 0.03965142369270325\n",
      "step: 6849, loss: 0.007201061118394136\n",
      "step: 6850, loss: 0.02708522230386734\n",
      "step: 6851, loss: 0.06482529640197754\n",
      "step: 6852, loss: 0.05628461018204689\n",
      "step: 6853, loss: 0.10531380027532578\n",
      "step: 6854, loss: 0.15662413835525513\n",
      "step: 6855, loss: 0.00095446000341326\n",
      "step: 6856, loss: 0.05075372755527496\n",
      "step: 6857, loss: 0.004737133160233498\n",
      "step: 6858, loss: 0.02858574688434601\n",
      "step: 6859, loss: 0.0004617296508513391\n",
      "step: 6860, loss: 0.09552880376577377\n",
      "step: 6861, loss: 0.0357019416987896\n",
      "step: 6862, loss: 0.07000105082988739\n",
      "step: 6863, loss: 0.008131943643093109\n",
      "step: 6864, loss: 0.0006843105074949563\n",
      "step: 6865, loss: 0.0016900051850825548\n",
      "step: 6866, loss: 0.0015572404954582453\n",
      "step: 6867, loss: 0.0167443435639143\n",
      "step: 6868, loss: 0.0010038373293355107\n",
      "step: 6869, loss: 0.0022482704371213913\n",
      "step: 6870, loss: 0.001730118878185749\n",
      "step: 6871, loss: 0.002122738864272833\n",
      "step: 6872, loss: 0.03761724382638931\n",
      "step: 6873, loss: 0.06279873847961426\n",
      "step: 6874, loss: 0.002880110638216138\n",
      "step: 6875, loss: 0.009708352386951447\n",
      "step: 6876, loss: 0.000678144337143749\n",
      "step: 6877, loss: 0.021683162078261375\n",
      "step: 6878, loss: 0.35562074184417725\n",
      "step: 6879, loss: 0.0030088424682617188\n",
      "step: 6880, loss: 0.014286642894148827\n",
      "step: 6881, loss: 0.0019816120620816946\n",
      "step: 6882, loss: 0.003765991423279047\n",
      "step: 6883, loss: 0.010976456105709076\n",
      "step: 6884, loss: 0.04077020660042763\n",
      "step: 6885, loss: 0.0015981757314875722\n",
      "step: 6886, loss: 0.003156633349135518\n",
      "step: 6887, loss: 0.03466297313570976\n",
      "step: 6888, loss: 0.05347689241170883\n",
      "step: 6889, loss: 0.0039094723761081696\n",
      "step: 6890, loss: 0.31217142939567566\n",
      "step: 6891, loss: 0.0012220124481245875\n",
      "step: 6892, loss: 0.011234985664486885\n",
      "step: 6893, loss: 0.04720696434378624\n",
      "step: 6894, loss: 0.010147125460207462\n",
      "step: 6895, loss: 0.0016012823907658458\n",
      "step: 6896, loss: 0.0037503899075090885\n",
      "step: 6897, loss: 0.0022746711038053036\n",
      "step: 6898, loss: 0.002128719352185726\n",
      "step: 6899, loss: 0.039158329367637634\n",
      "step: 6900, loss: 0.010996151715517044\n",
      "step: 6901, loss: 0.001698666950687766\n",
      "step: 6902, loss: 0.003878294723108411\n",
      "step: 6903, loss: 0.058798376470804214\n",
      "step: 6904, loss: 0.0006234419997781515\n",
      "step: 6905, loss: 0.00790720246732235\n",
      "step: 6906, loss: 0.0017770415870472789\n",
      "step: 6907, loss: 0.0015836483798921108\n",
      "step: 6908, loss: 0.0033159127924591303\n",
      "step: 6909, loss: 0.05651446431875229\n",
      "step: 6910, loss: 0.05257505550980568\n",
      "step: 6911, loss: 0.0018770056776702404\n",
      "step: 6912, loss: 0.001296559115871787\n",
      "step: 6913, loss: 0.003467319067567587\n",
      "step: 6914, loss: 0.029157163575291634\n",
      "step: 6915, loss: 0.0022565911058336496\n",
      "step: 6916, loss: 0.06222134083509445\n",
      "step: 6917, loss: 0.002997723175212741\n",
      "step: 6918, loss: 0.04443861171603203\n",
      "step: 6919, loss: 0.0037909287493675947\n",
      "step: 6920, loss: 0.015524905174970627\n",
      "step: 6921, loss: 0.05390214920043945\n",
      "step: 6922, loss: 0.0012043069582432508\n",
      "step: 6923, loss: 0.015572327189147472\n",
      "step: 6924, loss: 0.000625198008492589\n",
      "step: 6925, loss: 0.04789947345852852\n",
      "step: 6926, loss: 0.02478497475385666\n",
      "step: 6927, loss: 0.0019967462867498398\n",
      "step: 6928, loss: 0.0025985529646277428\n",
      "step: 6929, loss: 0.001502111554145813\n",
      "step: 6930, loss: 0.00395380100235343\n",
      "step: 6931, loss: 0.4292773902416229\n",
      "step: 6932, loss: 0.032524749636650085\n",
      "step: 6933, loss: 0.021668456494808197\n",
      "step: 6934, loss: 0.0002115888928528875\n",
      "step: 6935, loss: 0.011013839393854141\n",
      "step: 6936, loss: 0.03804432228207588\n",
      "step: 6937, loss: 0.0015796020161360502\n",
      "step: 6938, loss: 0.0020305297803133726\n",
      "step: 6939, loss: 0.0006735032657161355\n",
      "step: 6940, loss: 0.017643002793192863\n",
      "step: 6941, loss: 0.0010711795184761286\n",
      "step: 6942, loss: 0.018381284549832344\n",
      "step: 6943, loss: 0.004363770131021738\n",
      "step: 6944, loss: 0.01415327563881874\n",
      "step: 6945, loss: 0.0009086142526939511\n",
      "step: 6946, loss: 0.0011920538963750005\n",
      "step: 6947, loss: 0.000843200134113431\n",
      "step: 6948, loss: 0.05292047932744026\n",
      "step: 6949, loss: 0.00010807991202455014\n",
      "step: 6950, loss: 0.052779246121644974\n",
      "step: 6951, loss: 0.0007936130277812481\n",
      "step: 6952, loss: 0.02495507150888443\n",
      "step: 6953, loss: 0.0033363387919962406\n",
      "step: 6954, loss: 0.003751862095668912\n",
      "step: 6955, loss: 0.003927783574908972\n",
      "step: 6956, loss: 0.05061599239706993\n",
      "step: 6957, loss: 0.06845272332429886\n",
      "step: 6958, loss: 0.0005529134068638086\n",
      "step: 6959, loss: 0.021558057516813278\n",
      "step: 6960, loss: 0.0008637192659080029\n",
      "step: 6961, loss: 0.0008878193330019712\n",
      "step: 6962, loss: 0.0450839102268219\n",
      "step: 6963, loss: 0.00018424175505060703\n",
      "step: 6964, loss: 0.0032426277175545692\n",
      "step: 6965, loss: 0.001927449251525104\n",
      "step: 6966, loss: 0.0010568343568593264\n",
      "step: 6967, loss: 0.010716402903199196\n",
      "step: 6968, loss: 0.0035055046901106834\n",
      "step: 6969, loss: 0.015693046152591705\n",
      "step: 6970, loss: 0.0007467870600521564\n",
      "step: 6971, loss: 0.010862750001251698\n",
      "step: 6972, loss: 0.03539780154824257\n",
      "step: 6973, loss: 0.08865027129650116\n",
      "step: 6974, loss: 0.0036229758989065886\n",
      "step: 6975, loss: 0.041788287460803986\n",
      "step: 6976, loss: 0.007926415652036667\n",
      "step: 6977, loss: 0.08745795488357544\n",
      "step: 6978, loss: 0.011979008093476295\n",
      "step: 6979, loss: 0.060445498675107956\n",
      "step: 6980, loss: 0.5871476531028748\n",
      "step: 6981, loss: 0.006160172168165445\n",
      "step: 6982, loss: 0.0008671805844642222\n",
      "step: 6983, loss: 0.010250345803797245\n",
      "step: 6984, loss: 0.002543801674619317\n",
      "step: 6985, loss: 0.010323924943804741\n",
      "step: 6986, loss: 0.04896827042102814\n",
      "step: 6987, loss: 0.018169643357396126\n",
      "step: 6988, loss: 0.006530798505991697\n",
      "step: 6989, loss: 0.0014792263973504305\n",
      "step: 6990, loss: 0.003053512657061219\n",
      "step: 6991, loss: 0.04779502749443054\n",
      "step: 6992, loss: 0.0014447665307670832\n",
      "step: 6993, loss: 0.0014814931200817227\n",
      "step: 6994, loss: 0.0009038809221237898\n",
      "step: 6995, loss: 0.00206952472217381\n",
      "step: 6996, loss: 0.001208125613629818\n",
      "step: 6997, loss: 0.002239126479253173\n",
      "step: 6998, loss: 0.0038979249075055122\n",
      "step: 6999, loss: 0.1493827849626541\n",
      "step: 7000, loss: 0.006170653738081455\n",
      "step: 7001, loss: 0.050880130380392075\n",
      "step: 7002, loss: 0.001068209414370358\n",
      "step: 7003, loss: 0.0010979081271216273\n",
      "step: 7004, loss: 0.006834579631686211\n",
      "step: 7005, loss: 0.0019818576984107494\n",
      "step: 7006, loss: 0.05454936623573303\n",
      "step: 7007, loss: 0.03229484707117081\n",
      "step: 7008, loss: 0.0066274795681238174\n",
      "step: 7009, loss: 0.005009022541344166\n",
      "step: 7010, loss: 0.004598164930939674\n",
      "step: 7011, loss: 0.008186239749193192\n",
      "step: 7012, loss: 0.001508372719399631\n",
      "step: 7013, loss: 0.026136651635169983\n",
      "step: 7014, loss: 0.0036291691940277815\n",
      "step: 7015, loss: 0.0016052392311394215\n",
      "step: 7016, loss: 0.0030679202172905207\n",
      "step: 7017, loss: 0.003111724741756916\n",
      "step: 7018, loss: 0.010332039557397366\n",
      "step: 7019, loss: 0.0010469830594956875\n",
      "step: 7020, loss: 0.014245927333831787\n",
      "step: 7021, loss: 0.01751931756734848\n",
      "step: 7022, loss: 0.0017471942119300365\n",
      "step: 7023, loss: 0.001139218918979168\n",
      "step: 7024, loss: 0.03470906615257263\n",
      "step: 7025, loss: 0.002702469937503338\n",
      "step: 7026, loss: 0.0008012493490241468\n",
      "step: 7027, loss: 0.01380899827927351\n",
      "step: 7028, loss: 0.0005632714019156992\n",
      "step: 7029, loss: 0.0012972061522305012\n",
      "step: 7030, loss: 0.0030503273010253906\n",
      "step: 7031, loss: 0.010286620818078518\n",
      "step: 7032, loss: 0.000697774114087224\n",
      "step: 7033, loss: 0.02248932048678398\n",
      "step: 7034, loss: 0.044871166348457336\n",
      "step: 7035, loss: 0.3472893536090851\n",
      "step: 7036, loss: 0.0031233993358910084\n",
      "step: 7037, loss: 0.0638965293765068\n",
      "step: 7038, loss: 0.04034847766160965\n",
      "step: 7039, loss: 0.03906076028943062\n",
      "step: 7040, loss: 0.06471769511699677\n",
      "step: 7041, loss: 0.11715492606163025\n",
      "step: 7042, loss: 0.00281767756678164\n",
      "step: 7043, loss: 0.4996422231197357\n",
      "step: 7044, loss: 0.061134111136198044\n",
      "step: 7045, loss: 0.14481812715530396\n",
      "step: 7046, loss: 0.09399998933076859\n",
      "step: 7047, loss: 0.09229612350463867\n",
      "step: 7048, loss: 0.0016079521737992764\n",
      "step: 7049, loss: 0.00268094171769917\n",
      "step: 7050, loss: 0.0008535140659660101\n",
      "step: 7051, loss: 0.06035204976797104\n",
      "step: 7052, loss: 0.0018762366380542517\n",
      "step: 7053, loss: 0.0009379444527439773\n",
      "step: 7054, loss: 0.014463169500231743\n",
      "step: 7055, loss: 0.0032329666428267956\n",
      "step: 7056, loss: 0.003964886534959078\n",
      "step: 7057, loss: 0.0567840114235878\n",
      "step: 7058, loss: 0.0483115054666996\n",
      "step: 7059, loss: 0.002933645620942116\n",
      "step: 7060, loss: 0.04691247642040253\n",
      "step: 7061, loss: 0.005123711656779051\n",
      "step: 7062, loss: 0.03603377565741539\n",
      "step: 7063, loss: 0.0014256755821406841\n",
      "step: 7064, loss: 0.005048519931733608\n",
      "step: 7065, loss: 0.00273260660469532\n",
      "step: 7066, loss: 0.00301182409748435\n",
      "step: 7067, loss: 0.002114662202075124\n",
      "step: 7068, loss: 0.0020300578325986862\n",
      "step: 7069, loss: 0.0020394991151988506\n",
      "step: 7070, loss: 0.0016100184293463826\n",
      "step: 7071, loss: 0.0006214152672328055\n",
      "step: 7072, loss: 0.001908533158712089\n",
      "step: 7073, loss: 0.0018446039175614715\n",
      "step: 7074, loss: 0.0007328025531023741\n",
      "step: 7075, loss: 0.008110681548714638\n",
      "step: 7076, loss: 0.013827141374349594\n",
      "step: 7077, loss: 0.0016166290733963251\n",
      "step: 7078, loss: 0.00995867419987917\n",
      "step: 7079, loss: 0.001451193238608539\n",
      "step: 7080, loss: 0.000485686759930104\n",
      "step: 7081, loss: 0.0012366932351142168\n",
      "step: 7082, loss: 0.0008671994437463582\n",
      "step: 7083, loss: 0.0024504840839654207\n",
      "step: 7084, loss: 0.0019036276498809457\n",
      "step: 7085, loss: 0.013825002126395702\n",
      "step: 7086, loss: 0.0008817732450552285\n",
      "step: 7087, loss: 0.03887655586004257\n",
      "step: 7088, loss: 0.022114483639597893\n",
      "step: 7089, loss: 0.0009111472172662616\n",
      "step: 7090, loss: 0.014071793295443058\n",
      "step: 7091, loss: 0.0035580166149884462\n",
      "step: 7092, loss: 0.013811853714287281\n",
      "step: 7093, loss: 0.018723800778388977\n",
      "step: 7094, loss: 0.007761965971440077\n",
      "step: 7095, loss: 0.16398634016513824\n",
      "step: 7096, loss: 0.0004529282741714269\n",
      "step: 7097, loss: 0.0010558358626440167\n",
      "step: 7098, loss: 0.004773109685629606\n",
      "step: 7099, loss: 0.0015888608759269118\n",
      "step: 7100, loss: 0.011269975453615189\n",
      "step: 7101, loss: 0.002177346497774124\n",
      "step: 7102, loss: 0.02426823414862156\n",
      "step: 7103, loss: 0.002130868611857295\n",
      "step: 7104, loss: 0.0009052458335645497\n",
      "step: 7105, loss: 0.002184199169278145\n",
      "step: 7106, loss: 0.002086515538394451\n",
      "step: 7107, loss: 0.0009668876300565898\n",
      "step: 7108, loss: 0.0005531877977773547\n",
      "step: 7109, loss: 0.0021671021822839975\n",
      "step: 7110, loss: 0.0010017422027885914\n",
      "step: 7111, loss: 0.004622515290975571\n",
      "step: 7112, loss: 0.002564474940299988\n",
      "step: 7113, loss: 0.04091845080256462\n",
      "step: 7114, loss: 0.001845162478275597\n",
      "step: 7115, loss: 0.0011232615215703845\n",
      "step: 7116, loss: 0.0002737581671681255\n",
      "step: 7117, loss: 0.04592869430780411\n",
      "step: 7118, loss: 0.0016760138096287847\n",
      "step: 7119, loss: 0.08623573929071426\n",
      "step: 7120, loss: 0.046492625027894974\n",
      "step: 7121, loss: 0.0009129808167926967\n",
      "step: 7122, loss: 0.00834516528993845\n",
      "step: 7123, loss: 0.001801072503440082\n",
      "step: 7124, loss: 0.002408352680504322\n",
      "step: 7125, loss: 0.01426943764090538\n",
      "step: 7126, loss: 0.0027326815761625767\n",
      "step: 7127, loss: 0.04226415231823921\n",
      "step: 7128, loss: 0.0005009490414522588\n",
      "step: 7129, loss: 0.012104835361242294\n",
      "step: 7130, loss: 0.00031530801788903773\n",
      "step: 7131, loss: 0.0007041580975055695\n",
      "step: 7132, loss: 0.0058825393207371235\n",
      "step: 7133, loss: 0.036641016602516174\n",
      "step: 7134, loss: 0.0011317890603095293\n",
      "step: 7135, loss: 0.0023706278298050165\n",
      "step: 7136, loss: 0.04418274387717247\n",
      "step: 7137, loss: 0.02161961980164051\n",
      "step: 7138, loss: 0.002544476417824626\n",
      "step: 7139, loss: 0.004638723097741604\n",
      "step: 7140, loss: 0.004192723426967859\n",
      "step: 7141, loss: 0.016129346564412117\n",
      "step: 7142, loss: 0.05274008587002754\n",
      "step: 7143, loss: 0.0007012750138528645\n",
      "step: 7144, loss: 0.012536904774606228\n",
      "step: 7145, loss: 0.0027174712158739567\n",
      "step: 7146, loss: 0.0020558368414640427\n",
      "step: 7147, loss: 0.00954136811196804\n",
      "step: 7148, loss: 0.0018403828144073486\n",
      "step: 7149, loss: 0.0014260682510212064\n",
      "step: 7150, loss: 0.008283824659883976\n",
      "step: 7151, loss: 0.001937367138452828\n",
      "step: 7152, loss: 0.0045706420205533504\n",
      "step: 7153, loss: 0.011565578170120716\n",
      "step: 7154, loss: 0.020315777510404587\n",
      "step: 7155, loss: 0.011402267962694168\n",
      "step: 7156, loss: 0.31031733751296997\n",
      "step: 7157, loss: 0.043583761900663376\n",
      "step: 7158, loss: 0.03789522498846054\n",
      "step: 7159, loss: 0.016721922904253006\n",
      "step: 7160, loss: 0.049923721700906754\n",
      "step: 7161, loss: 0.0021165965590626\n",
      "step: 7162, loss: 0.0002600039297249168\n",
      "step: 7163, loss: 0.046555548906326294\n",
      "step: 7164, loss: 0.008996918797492981\n",
      "step: 7165, loss: 0.0017293707933276892\n",
      "step: 7166, loss: 0.0031521408818662167\n",
      "step: 7167, loss: 0.0011085034348070621\n",
      "step: 7168, loss: 0.016699783504009247\n",
      "step: 7169, loss: 0.0016462365165352821\n",
      "step: 7170, loss: 0.00048125803004950285\n",
      "step: 7171, loss: 0.0009397102985531092\n",
      "step: 7172, loss: 0.0016309896018356085\n",
      "step: 7173, loss: 0.0002531587961129844\n",
      "step: 7174, loss: 0.04539205878973007\n",
      "step: 7175, loss: 0.0006450841319747269\n",
      "step: 7176, loss: 0.0010719820857048035\n",
      "step: 7177, loss: 0.060837339609861374\n",
      "step: 7178, loss: 0.011570582166314125\n",
      "step: 7179, loss: 0.10732714831829071\n",
      "step: 7180, loss: 0.0005606748745776713\n",
      "step: 7181, loss: 0.010576168075203896\n",
      "step: 7182, loss: 0.009492036886513233\n",
      "step: 7183, loss: 0.0028718612156808376\n",
      "step: 7184, loss: 0.0017969137988984585\n",
      "step: 7185, loss: 0.0178545992821455\n",
      "step: 7186, loss: 0.000767798104789108\n",
      "step: 7187, loss: 0.0027638175524771214\n",
      "step: 7188, loss: 0.0015514425467699766\n",
      "step: 7189, loss: 0.0008438570657745004\n",
      "step: 7190, loss: 0.0011830260045826435\n",
      "step: 7191, loss: 0.05033983662724495\n",
      "step: 7192, loss: 0.010438341647386551\n",
      "step: 7193, loss: 0.0004964361432939768\n",
      "step: 7194, loss: 0.04803978279232979\n",
      "step: 7195, loss: 0.007344776298850775\n",
      "step: 7196, loss: 0.041881855577230453\n",
      "step: 7197, loss: 0.04215346649289131\n",
      "step: 7198, loss: 0.0012780566466972232\n",
      "step: 7199, loss: 0.03251684829592705\n",
      "step: 7200, loss: 0.002735007321462035\n",
      "step: 7201, loss: 0.0012320198584347963\n",
      "step: 7202, loss: 0.002558486070483923\n",
      "step: 7203, loss: 0.025402452796697617\n",
      "step: 7204, loss: 0.001296596834436059\n",
      "step: 7205, loss: 0.00029094552155584097\n",
      "step: 7206, loss: 0.0011540822451934218\n",
      "step: 7207, loss: 0.0006981258047744632\n",
      "step: 7208, loss: 0.026736535131931305\n",
      "step: 7209, loss: 0.0015433287480846047\n",
      "step: 7210, loss: 0.0016552135348320007\n",
      "step: 7211, loss: 0.011118961498141289\n",
      "step: 7212, loss: 0.002169195795431733\n",
      "step: 7213, loss: 0.11065034568309784\n",
      "step: 7214, loss: 0.0015393915819004178\n",
      "step: 7215, loss: 0.0008367638802155852\n",
      "step: 7216, loss: 0.050477780401706696\n",
      "step: 7217, loss: 0.009878154844045639\n",
      "step: 7218, loss: 0.012248452752828598\n",
      "step: 7219, loss: 0.0006972603732720017\n",
      "step: 7220, loss: 0.0024305114056915045\n",
      "step: 7221, loss: 0.013064452446997166\n",
      "step: 7222, loss: 0.0021712123416364193\n",
      "step: 7223, loss: 0.0035546557046473026\n",
      "step: 7224, loss: 0.0661114752292633\n",
      "step: 7225, loss: 0.0022824774496257305\n",
      "step: 7226, loss: 0.001275696442462504\n",
      "step: 7227, loss: 0.0013331547379493713\n",
      "step: 7228, loss: 0.049302686005830765\n",
      "step: 7229, loss: 0.14404723048210144\n",
      "step: 7230, loss: 0.0022424866911023855\n",
      "step: 7231, loss: 0.0452737919986248\n",
      "step: 7232, loss: 0.018802311271429062\n",
      "step: 7233, loss: 0.010844527743756771\n",
      "step: 7234, loss: 0.0028897610027343035\n",
      "step: 7235, loss: 0.0021928935311734676\n",
      "step: 7236, loss: 0.0011283386265859008\n",
      "step: 7237, loss: 0.0034064503852277994\n",
      "step: 7238, loss: 0.0010074361925944686\n",
      "step: 7239, loss: 0.07523323595523834\n",
      "step: 7240, loss: 0.0034464620985090733\n",
      "step: 7241, loss: 0.0008053517085500062\n",
      "step: 7242, loss: 0.0031897732987999916\n",
      "step: 7243, loss: 0.010994945652782917\n",
      "step: 7244, loss: 0.0016739555867388844\n",
      "step: 7245, loss: 0.0905635803937912\n",
      "step: 7246, loss: 0.0005501042469404638\n",
      "step: 7247, loss: 0.0014412486925721169\n",
      "step: 7248, loss: 0.005618047900497913\n",
      "step: 7249, loss: 0.0008686360088177025\n",
      "step: 7250, loss: 0.001113019185140729\n",
      "step: 7251, loss: 0.0006535935681313276\n",
      "step: 7252, loss: 0.14494897425174713\n",
      "step: 7253, loss: 0.0006407799082808197\n",
      "step: 7254, loss: 0.07130841165781021\n",
      "step: 7255, loss: 0.0009774318896234035\n",
      "step: 7256, loss: 0.014880015514791012\n",
      "step: 7257, loss: 0.01379473228007555\n",
      "step: 7258, loss: 0.001962723210453987\n",
      "step: 7259, loss: 0.0003871151711791754\n",
      "step: 7260, loss: 0.01015737745910883\n",
      "step: 7261, loss: 0.037242043763399124\n",
      "step: 7262, loss: 0.0007227736059576273\n",
      "step: 7263, loss: 0.0008166272309608757\n",
      "step: 7264, loss: 0.001737720798701048\n",
      "step: 7265, loss: 0.027703218162059784\n",
      "step: 7266, loss: 0.003520033322274685\n",
      "step: 7267, loss: 0.0010122921084985137\n",
      "step: 7268, loss: 0.010632587596774101\n",
      "step: 7269, loss: 0.005431364756077528\n",
      "step: 7270, loss: 0.051063746213912964\n",
      "step: 7271, loss: 0.001295299269258976\n",
      "step: 7272, loss: 0.0018268923740833998\n",
      "step: 7273, loss: 0.0009594617295078933\n",
      "step: 7274, loss: 0.002914816839620471\n",
      "step: 7275, loss: 0.0525260865688324\n",
      "step: 7276, loss: 0.4297581613063812\n",
      "step: 7277, loss: 0.0009590345434844494\n",
      "step: 7278, loss: 0.022456448525190353\n",
      "step: 7279, loss: 0.0009028706699609756\n",
      "step: 7280, loss: 0.0009181427885778248\n",
      "step: 7281, loss: 0.0011742656351998448\n",
      "step: 7282, loss: 0.003725044196471572\n",
      "step: 7283, loss: 0.0019054986769333482\n",
      "step: 7284, loss: 0.0015309624141082168\n",
      "step: 7285, loss: 0.0017197525594383478\n",
      "step: 7286, loss: 0.04713411629199982\n",
      "step: 7287, loss: 0.0030732040759176016\n",
      "step: 7288, loss: 0.016413869336247444\n",
      "step: 7289, loss: 0.0009485784103162587\n",
      "step: 7290, loss: 0.003144091460853815\n",
      "step: 7291, loss: 0.011461835354566574\n",
      "step: 7292, loss: 0.012627159245312214\n",
      "step: 7293, loss: 0.015208409167826176\n",
      "step: 7294, loss: 0.002786968369036913\n",
      "step: 7295, loss: 0.0001453112781746313\n",
      "step: 7296, loss: 0.0010820120805874467\n",
      "step: 7297, loss: 0.0008601945592090487\n",
      "step: 7298, loss: 0.03961825743317604\n",
      "step: 7299, loss: 0.03549351915717125\n",
      "step: 7300, loss: 0.001068250508978963\n",
      "step: 7301, loss: 0.003427862422540784\n",
      "step: 7302, loss: 0.05113138630986214\n",
      "step: 7303, loss: 0.0283303614705801\n",
      "step: 7304, loss: 0.004983861465007067\n",
      "step: 7305, loss: 0.0019529488636180758\n",
      "step: 7306, loss: 0.004620645195245743\n",
      "step: 7307, loss: 0.00300649949349463\n",
      "step: 7308, loss: 0.034257326275110245\n",
      "step: 7309, loss: 0.0035396874882280827\n",
      "step: 7310, loss: 0.00035847173421643674\n",
      "step: 7311, loss: 0.0048229144886136055\n",
      "step: 7312, loss: 0.00016828879597596824\n",
      "step: 7313, loss: 0.027175236493349075\n",
      "step: 7314, loss: 0.0020316792652010918\n",
      "step: 7315, loss: 0.07395827770233154\n",
      "step: 7316, loss: 0.001555123133584857\n",
      "step: 7317, loss: 0.008939412422478199\n",
      "step: 7318, loss: 0.03183145448565483\n",
      "step: 7319, loss: 0.0092295678332448\n",
      "step: 7320, loss: 0.0014708146918565035\n",
      "step: 7321, loss: 0.0023968571331351995\n",
      "step: 7322, loss: 0.0047796727158129215\n",
      "step: 7323, loss: 0.0019662531558424234\n",
      "step: 7324, loss: 0.29879704117774963\n",
      "step: 7325, loss: 0.0007379592279903591\n",
      "step: 7326, loss: 0.0016523372614756227\n",
      "step: 7327, loss: 0.000798292167019099\n",
      "step: 7328, loss: 0.0009086985373869538\n",
      "step: 7329, loss: 0.055988021194934845\n",
      "step: 7330, loss: 0.0010905228555202484\n",
      "step: 7331, loss: 0.0011971995700150728\n",
      "step: 7332, loss: 0.002095212694257498\n",
      "step: 7333, loss: 0.04072188958525658\n",
      "step: 7334, loss: 0.09866010397672653\n",
      "step: 7335, loss: 0.010917340405285358\n",
      "step: 7336, loss: 0.026177413761615753\n",
      "step: 7337, loss: 0.03366035968065262\n",
      "step: 7338, loss: 0.07947147637605667\n",
      "step: 7339, loss: 0.00012936542043462396\n",
      "step: 7340, loss: 0.001413719612173736\n",
      "step: 7341, loss: 0.0011850666487589478\n",
      "step: 7342, loss: 0.03125489503145218\n",
      "step: 7343, loss: 0.025204425677657127\n",
      "step: 7344, loss: 0.03297099471092224\n",
      "step: 7345, loss: 0.0009629159467294812\n",
      "step: 7346, loss: 0.04231084883213043\n",
      "step: 7347, loss: 0.001428289688192308\n",
      "step: 7348, loss: 0.0017170266946777701\n",
      "step: 7349, loss: 0.00200208299793303\n",
      "step: 7350, loss: 0.011680300347507\n",
      "step: 7351, loss: 0.035467881709337234\n",
      "step: 7352, loss: 0.02572006918489933\n",
      "step: 7353, loss: 0.0007954098982736468\n",
      "step: 7354, loss: 0.004307329189032316\n",
      "step: 7355, loss: 0.04651380702853203\n",
      "step: 7356, loss: 0.0016214564675465226\n",
      "step: 7357, loss: 0.0019178518559783697\n",
      "step: 7358, loss: 0.0015080654993653297\n",
      "step: 7359, loss: 0.025190874934196472\n",
      "step: 7360, loss: 0.003827662207186222\n",
      "step: 7361, loss: 0.0011366053950041533\n",
      "step: 7362, loss: 0.009296412579715252\n",
      "step: 7363, loss: 0.05045519769191742\n",
      "step: 7364, loss: 0.0009953383123502135\n",
      "step: 7365, loss: 0.04478729888796806\n",
      "step: 7366, loss: 0.012328344397246838\n",
      "step: 7367, loss: 0.012096780352294445\n",
      "step: 7368, loss: 0.002216195221990347\n",
      "step: 7369, loss: 0.000253745325608179\n",
      "step: 7370, loss: 0.004475020803511143\n",
      "step: 7371, loss: 0.004314184654504061\n",
      "step: 7372, loss: 0.049429308623075485\n",
      "step: 7373, loss: 0.09146387875080109\n",
      "step: 7374, loss: 0.0007377206347882748\n",
      "step: 7375, loss: 0.0033277675975114107\n",
      "step: 7376, loss: 0.0991043671965599\n",
      "step: 7377, loss: 0.010410594753921032\n",
      "step: 7378, loss: 0.022576119750738144\n",
      "step: 7379, loss: 0.0006637406186200678\n",
      "step: 7380, loss: 0.010537172667682171\n",
      "step: 7381, loss: 0.00153107731603086\n",
      "step: 7382, loss: 0.037418484687805176\n",
      "step: 7383, loss: 0.03454536944627762\n",
      "step: 7384, loss: 0.21789692342281342\n",
      "step: 7385, loss: 0.000509371398948133\n",
      "step: 7386, loss: 0.0015075242845341563\n",
      "step: 7387, loss: 0.6605061292648315\n",
      "step: 7388, loss: 0.009162540547549725\n",
      "step: 7389, loss: 0.01875232346355915\n",
      "step: 7390, loss: 0.00033613943378441036\n",
      "step: 7391, loss: 0.0007920102216303349\n",
      "step: 7392, loss: 0.00041971993050538003\n",
      "step: 7393, loss: 0.04663974791765213\n",
      "step: 7394, loss: 0.00041421366040594876\n",
      "step: 7395, loss: 0.0004940229118801653\n",
      "step: 7396, loss: 0.03830159083008766\n",
      "step: 7397, loss: 0.003246271051466465\n",
      "step: 7398, loss: 0.0015004329616203904\n",
      "step: 7399, loss: 0.062091514468193054\n",
      "step: 7400, loss: 0.0015204616356641054\n",
      "step: 7401, loss: 0.00048340114881284535\n",
      "step: 7402, loss: 0.0011897897347807884\n",
      "step: 7403, loss: 0.0005861945101059973\n",
      "step: 7404, loss: 0.01542816311120987\n",
      "step: 7405, loss: 0.046620212495326996\n",
      "step: 7406, loss: 0.05514226481318474\n",
      "step: 7407, loss: 0.0011204619659110904\n",
      "step: 7408, loss: 0.0016187381697818637\n",
      "step: 7409, loss: 0.00011465808347566053\n",
      "step: 7410, loss: 0.25938931107521057\n",
      "step: 7411, loss: 0.0007708125631324947\n",
      "step: 7412, loss: 0.0001783019833965227\n",
      "step: 7413, loss: 0.0004354143457021564\n",
      "step: 7414, loss: 0.000891549454536289\n",
      "step: 7415, loss: 0.002426941879093647\n",
      "step: 7416, loss: 0.025534776970744133\n",
      "step: 7417, loss: 0.021900374442338943\n",
      "step: 7418, loss: 0.01304064504802227\n",
      "step: 7419, loss: 0.001180008752271533\n",
      "step: 7420, loss: 0.008607425726950169\n",
      "step: 7421, loss: 0.023194091394543648\n",
      "step: 7422, loss: 0.10130691528320312\n",
      "step: 7423, loss: 0.024644171819090843\n",
      "step: 7424, loss: 0.0014498905511572957\n",
      "step: 7425, loss: 0.0003948063822463155\n",
      "step: 7426, loss: 0.0003147641255054623\n",
      "step: 7427, loss: 0.0426945723593235\n",
      "step: 7428, loss: 0.007929983548820019\n",
      "step: 7429, loss: 0.024034669622778893\n",
      "step: 7430, loss: 0.0013843276537954807\n",
      "step: 7431, loss: 0.02056545577943325\n",
      "step: 7432, loss: 0.04203370213508606\n",
      "step: 7433, loss: 0.0011524673318490386\n",
      "step: 7434, loss: 0.0004969184519723058\n",
      "step: 7435, loss: 0.0016242628917098045\n",
      "step: 7436, loss: 0.0036135870032012463\n",
      "step: 7437, loss: 0.0021718882489949465\n",
      "step: 7438, loss: 0.0019192860927432775\n",
      "step: 7439, loss: 0.0012449786299839616\n",
      "step: 7440, loss: 0.0013132410822436213\n",
      "step: 7441, loss: 0.008627575822174549\n",
      "step: 7442, loss: 0.0010729521745815873\n",
      "step: 7443, loss: 0.003462611697614193\n",
      "step: 7444, loss: 0.10349076986312866\n",
      "step: 7445, loss: 0.00494825653731823\n",
      "step: 7446, loss: 0.00038905604742467403\n",
      "step: 7447, loss: 0.046622782945632935\n",
      "step: 7448, loss: 0.03105865977704525\n",
      "step: 7449, loss: 0.2004786878824234\n",
      "step: 7450, loss: 0.0006935760611668229\n",
      "step: 7451, loss: 0.00103606132324785\n",
      "step: 7452, loss: 0.0011423829710111022\n",
      "step: 7453, loss: 0.006192448548972607\n",
      "step: 7454, loss: 0.014059286564588547\n",
      "step: 7455, loss: 0.0020036331843584776\n",
      "step: 7456, loss: 0.005983374547213316\n",
      "step: 7457, loss: 0.0013524683890864253\n",
      "step: 7458, loss: 0.0005777368787676096\n",
      "step: 7459, loss: 0.012525740079581738\n",
      "step: 7460, loss: 0.0010160187957808375\n",
      "step: 7461, loss: 0.06055264174938202\n",
      "step: 7462, loss: 0.003165694186463952\n",
      "step: 7463, loss: 0.0010519878705963492\n",
      "step: 7464, loss: 0.0008115436066873372\n",
      "step: 7465, loss: 0.0009747670846991241\n",
      "step: 7466, loss: 0.05024851858615875\n",
      "step: 7467, loss: 0.000635067350231111\n",
      "step: 7468, loss: 0.004143433179706335\n",
      "step: 7469, loss: 0.020340735092759132\n",
      "step: 7470, loss: 0.048950206488370895\n",
      "step: 7471, loss: 0.0007437285967171192\n",
      "step: 7472, loss: 0.0010822602780535817\n",
      "step: 7473, loss: 0.024106154218316078\n",
      "step: 7474, loss: 0.00580303929746151\n",
      "step: 7475, loss: 0.001298154005780816\n",
      "step: 7476, loss: 0.012495612725615501\n",
      "step: 7477, loss: 0.0004126213607378304\n",
      "step: 7478, loss: 0.01879407837986946\n",
      "step: 7479, loss: 0.0019512721337378025\n",
      "step: 7480, loss: 0.0009680925286374986\n",
      "step: 7481, loss: 0.049920275807380676\n",
      "step: 7482, loss: 0.0008182944147847593\n",
      "step: 7483, loss: 0.0007402787450700998\n",
      "step: 7484, loss: 0.003376530483365059\n",
      "step: 7485, loss: 0.0276848953217268\n",
      "step: 7486, loss: 0.0017188850324600935\n",
      "step: 7487, loss: 0.00032737755100242794\n",
      "step: 7488, loss: 0.04686066135764122\n",
      "step: 7489, loss: 0.004107242915779352\n",
      "step: 7490, loss: 0.03749531880021095\n",
      "step: 7491, loss: 0.009770698845386505\n",
      "step: 7492, loss: 0.00141836644615978\n",
      "step: 7493, loss: 0.034246522933244705\n",
      "step: 7494, loss: 0.0003142140049021691\n",
      "step: 7495, loss: 0.0044580986723303795\n",
      "step: 7496, loss: 0.42424556612968445\n",
      "step: 7497, loss: 0.0029251896776258945\n",
      "step: 7498, loss: 0.01772170513868332\n",
      "step: 7499, loss: 0.009428378194570541\n",
      "step: 7500, loss: 0.00042832575854845345\n",
      "step: 7501, loss: 0.021761059761047363\n",
      "step: 7502, loss: 0.002811012789607048\n",
      "step: 7503, loss: 0.0014903877163305879\n",
      "step: 7504, loss: 0.0009298758814111352\n",
      "step: 7505, loss: 0.07972382009029388\n",
      "step: 7506, loss: 0.0363737978041172\n",
      "step: 7507, loss: 0.0007586001884192228\n",
      "step: 7508, loss: 0.035156119614839554\n",
      "step: 7509, loss: 0.001558156916871667\n",
      "step: 7510, loss: 0.0023837557528167963\n",
      "step: 7511, loss: 0.04284510761499405\n",
      "step: 7512, loss: 0.004388941917568445\n",
      "step: 7513, loss: 0.01163838617503643\n",
      "step: 7514, loss: 0.0009653661982156336\n",
      "step: 7515, loss: 0.002470041625201702\n",
      "step: 7516, loss: 0.010228215716779232\n",
      "step: 7517, loss: 0.01956891268491745\n",
      "step: 7518, loss: 0.003562849946320057\n",
      "step: 7519, loss: 0.0012593951541930437\n",
      "step: 7520, loss: 0.021973762661218643\n",
      "step: 7521, loss: 0.0027048084884881973\n",
      "step: 7522, loss: 0.04757876694202423\n",
      "step: 7523, loss: 0.0008380758226849139\n",
      "step: 7524, loss: 0.0039488463662564754\n",
      "step: 7525, loss: 0.0007914536399766803\n",
      "step: 7526, loss: 0.049440182745456696\n",
      "step: 7527, loss: 0.06020680442452431\n",
      "step: 7528, loss: 0.009960727766156197\n",
      "step: 7529, loss: 0.0036170887760818005\n",
      "step: 7530, loss: 0.0073539502918720245\n",
      "step: 7531, loss: 0.03347156569361687\n",
      "step: 7532, loss: 0.0023311588447541\n",
      "step: 7533, loss: 0.0017576984828338027\n",
      "step: 7534, loss: 0.0012267790734767914\n",
      "step: 7535, loss: 0.027143340557813644\n",
      "step: 7536, loss: 0.0019284588051959872\n",
      "step: 7537, loss: 0.026230396702885628\n",
      "step: 7538, loss: 0.0006607662071473897\n",
      "step: 7539, loss: 0.0014482269762083888\n",
      "step: 7540, loss: 0.01042967475950718\n",
      "step: 7541, loss: 0.011901496909558773\n",
      "step: 7542, loss: 0.08715048432350159\n",
      "step: 7543, loss: 0.00896691158413887\n",
      "step: 7544, loss: 0.0008434530464001\n",
      "step: 7545, loss: 0.0025414545089006424\n",
      "step: 7546, loss: 0.0006683261599391699\n",
      "step: 7547, loss: 0.08670114725828171\n",
      "step: 7548, loss: 0.0016887851525098085\n",
      "step: 7549, loss: 0.002286195056512952\n",
      "step: 7550, loss: 0.02029748447239399\n",
      "step: 7551, loss: 0.00040081870974972844\n",
      "step: 7552, loss: 0.0004752677050419152\n",
      "step: 7553, loss: 0.000992444809526205\n",
      "step: 7554, loss: 0.001142158405855298\n",
      "step: 7555, loss: 0.03856905549764633\n",
      "step: 7556, loss: 0.0002371689915889874\n",
      "step: 7557, loss: 0.0020189941860735416\n",
      "step: 7558, loss: 0.017267093062400818\n",
      "step: 7559, loss: 0.000338271347573027\n",
      "step: 7560, loss: 0.0018724430119618773\n",
      "step: 7561, loss: 0.005133990664035082\n",
      "step: 7562, loss: 0.0031145415268838406\n",
      "step: 7563, loss: 0.000650960486382246\n",
      "step: 7564, loss: 0.001176134217530489\n",
      "step: 7565, loss: 0.01088759582489729\n",
      "step: 7566, loss: 0.0008554120431654155\n",
      "step: 7567, loss: 0.0021429015323519707\n",
      "step: 7568, loss: 0.04586704075336456\n",
      "step: 7569, loss: 0.004341358318924904\n",
      "step: 7570, loss: 0.0010149963200092316\n",
      "step: 7571, loss: 0.0010969951981678605\n",
      "step: 7572, loss: 0.05473676323890686\n",
      "step: 7573, loss: 0.001693114754743874\n",
      "step: 7574, loss: 0.010214178822934628\n",
      "step: 7575, loss: 0.00042506362660788\n",
      "step: 7576, loss: 0.03347000107169151\n",
      "step: 7577, loss: 0.0007384486962109804\n",
      "step: 7578, loss: 0.0008632942917756736\n",
      "step: 7579, loss: 0.004069233313202858\n",
      "step: 7580, loss: 0.004595658741891384\n",
      "step: 7581, loss: 0.0024203427601605654\n",
      "step: 7582, loss: 0.007627253420650959\n",
      "step: 7583, loss: 0.001396426698192954\n",
      "step: 7584, loss: 0.00042018279782496393\n",
      "step: 7585, loss: 0.032069090753793716\n",
      "step: 7586, loss: 0.0007268213666975498\n",
      "step: 7587, loss: 0.0015895288670435548\n",
      "step: 7588, loss: 0.061744626611471176\n",
      "step: 7589, loss: 0.00082139961887151\n",
      "step: 7590, loss: 0.0011954006040468812\n",
      "step: 7591, loss: 0.0010522283846512437\n",
      "step: 7592, loss: 0.000756806752178818\n",
      "step: 7593, loss: 0.0006713371258229017\n",
      "step: 7594, loss: 0.036553993821144104\n",
      "step: 7595, loss: 0.05512338504195213\n",
      "step: 7596, loss: 0.0004881982458755374\n",
      "step: 7597, loss: 0.0004011359123978764\n",
      "step: 7598, loss: 0.0007590614259243011\n",
      "step: 7599, loss: 0.0017116161761805415\n",
      "step: 7600, loss: 0.0007830800022929907\n",
      "step: 7601, loss: 0.017859064042568207\n",
      "step: 7602, loss: 0.00011389272549422458\n",
      "step: 7603, loss: 0.00015586322115268558\n",
      "step: 7604, loss: 0.012847702950239182\n",
      "step: 7605, loss: 0.009927958250045776\n",
      "step: 7606, loss: 0.0012771502370014787\n",
      "step: 7607, loss: 0.0012186046224087477\n",
      "step: 7608, loss: 0.0010291187791153789\n",
      "step: 7609, loss: 0.001928795245476067\n",
      "step: 7610, loss: 0.001142846536822617\n",
      "step: 7611, loss: 0.06150474399328232\n",
      "step: 7612, loss: 0.00134319718927145\n",
      "step: 7613, loss: 0.002400818048045039\n",
      "step: 7614, loss: 0.0034433272667229176\n",
      "step: 7615, loss: 0.013621140271425247\n",
      "step: 7616, loss: 0.0032738912850618362\n",
      "step: 7617, loss: 0.02391350269317627\n",
      "step: 7618, loss: 0.04894373565912247\n",
      "step: 7619, loss: 0.24380303919315338\n",
      "step: 7620, loss: 0.12824931740760803\n",
      "step: 7621, loss: 0.0007785975467413664\n",
      "step: 7622, loss: 0.0010286950273439288\n",
      "step: 7623, loss: 0.003196385456249118\n",
      "step: 7624, loss: 0.014159495010972023\n",
      "step: 7625, loss: 0.00035169391776435077\n",
      "step: 7626, loss: 0.01209527999162674\n",
      "step: 7627, loss: 0.006842444650828838\n",
      "step: 7628, loss: 0.010771777480840683\n",
      "step: 7629, loss: 0.001821860671043396\n",
      "step: 7630, loss: 0.04210454598069191\n",
      "step: 7631, loss: 0.04585200920701027\n",
      "step: 7632, loss: 0.002239824738353491\n",
      "step: 7633, loss: 0.01430872455239296\n",
      "step: 7634, loss: 0.011754665523767471\n",
      "step: 7635, loss: 0.04572353512048721\n",
      "step: 7636, loss: 0.0001641477138036862\n",
      "step: 7637, loss: 0.0035703897010535\n",
      "step: 7638, loss: 0.023633701726794243\n",
      "step: 7639, loss: 0.0009455344988964498\n",
      "step: 7640, loss: 0.08527029305696487\n",
      "step: 7641, loss: 0.0007250707130879164\n",
      "step: 7642, loss: 0.000494255858939141\n",
      "step: 7643, loss: 0.0010842803167179227\n",
      "step: 7644, loss: 0.08055054396390915\n",
      "step: 7645, loss: 0.002055881079286337\n",
      "step: 7646, loss: 0.0010016135638579726\n",
      "step: 7647, loss: 0.10351160168647766\n",
      "step: 7648, loss: 0.05281723663210869\n",
      "step: 7649, loss: 0.0006638632039539516\n",
      "step: 7650, loss: 0.03560004383325577\n",
      "step: 7651, loss: 0.002171016065403819\n",
      "step: 7652, loss: 0.000932168448343873\n",
      "step: 7653, loss: 0.00039556113188154995\n",
      "step: 7654, loss: 0.0641014352440834\n",
      "step: 7655, loss: 0.13424210250377655\n",
      "step: 7656, loss: 0.00047669350169599056\n",
      "step: 7657, loss: 0.0007596870418637991\n",
      "step: 7658, loss: 0.0025171544402837753\n",
      "step: 7659, loss: 0.052409473806619644\n",
      "step: 7660, loss: 0.0011030525201931596\n",
      "step: 7661, loss: 0.0028086956590414047\n",
      "step: 7662, loss: 0.011838549748063087\n",
      "step: 7663, loss: 0.011258237063884735\n",
      "step: 7664, loss: 0.05876510590314865\n",
      "step: 7665, loss: 0.0044693732634186745\n",
      "step: 7666, loss: 0.0373079739511013\n",
      "step: 7667, loss: 0.011575732380151749\n",
      "step: 7668, loss: 0.010012713260948658\n",
      "step: 7669, loss: 0.07706732302904129\n",
      "step: 7670, loss: 0.04492141306400299\n",
      "step: 7671, loss: 0.008987399749457836\n",
      "step: 7672, loss: 0.010119104757905006\n",
      "step: 7673, loss: 0.013142629526555538\n",
      "step: 7674, loss: 0.00048024210263974965\n",
      "step: 7675, loss: 0.03627725690603256\n",
      "step: 7676, loss: 0.0006734653725288808\n",
      "step: 7677, loss: 0.0010416831355541945\n",
      "step: 7678, loss: 0.011838901787996292\n",
      "step: 7679, loss: 0.0013117844937369227\n",
      "step: 7680, loss: 0.009122435003519058\n",
      "step: 7681, loss: 0.02170281484723091\n",
      "step: 7682, loss: 0.00043189467396587133\n",
      "step: 7683, loss: 0.011344739235937595\n",
      "step: 7684, loss: 0.006868300493806601\n",
      "step: 7685, loss: 0.14900627732276917\n",
      "step: 7686, loss: 0.0011982509167864919\n",
      "step: 7687, loss: 0.0026607969775795937\n",
      "step: 7688, loss: 0.013097354210913181\n",
      "step: 7689, loss: 0.0007129217847250402\n",
      "step: 7690, loss: 0.002527605276554823\n",
      "step: 7691, loss: 0.0020133666694164276\n",
      "step: 7692, loss: 0.001320456969551742\n",
      "step: 7693, loss: 0.01061957236379385\n",
      "step: 7694, loss: 0.023665299639105797\n",
      "step: 7695, loss: 0.06152667850255966\n",
      "step: 7696, loss: 0.0018408349715173244\n",
      "step: 7697, loss: 0.07298740744590759\n",
      "step: 7698, loss: 0.04933338984847069\n",
      "step: 7699, loss: 0.0010991794988512993\n",
      "step: 7700, loss: 0.06230135262012482\n",
      "step: 7701, loss: 0.001088523305952549\n",
      "step: 7702, loss: 0.008077920414507389\n",
      "step: 7703, loss: 0.001083538867533207\n",
      "step: 7704, loss: 0.01144895888864994\n",
      "step: 7705, loss: 0.03731969743967056\n",
      "step: 7706, loss: 0.0007932691951282322\n",
      "step: 7707, loss: 0.011142700910568237\n",
      "step: 7708, loss: 0.0010232210624963045\n",
      "step: 7709, loss: 0.00787520781159401\n",
      "step: 7710, loss: 0.008673968724906445\n",
      "step: 7711, loss: 0.15288180112838745\n",
      "step: 7712, loss: 0.00990602932870388\n",
      "step: 7713, loss: 0.0036683217622339725\n",
      "step: 7714, loss: 0.011640134267508984\n",
      "step: 7715, loss: 0.01618744432926178\n",
      "step: 7716, loss: 0.011526567861437798\n",
      "step: 7717, loss: 0.0009334978531114757\n",
      "step: 7718, loss: 0.0006061407038941979\n",
      "step: 7719, loss: 0.0054906453005969524\n",
      "step: 7720, loss: 0.002401605946943164\n",
      "step: 7721, loss: 0.0018413268262520432\n",
      "step: 7722, loss: 0.004002266097813845\n",
      "step: 7723, loss: 0.019375186413526535\n",
      "step: 7724, loss: 0.00705733522772789\n",
      "step: 7725, loss: 0.0038274589460343122\n",
      "step: 7726, loss: 0.08382073044776917\n",
      "step: 7727, loss: 0.002997917588800192\n",
      "step: 7728, loss: 0.0007030735723674297\n",
      "step: 7729, loss: 0.002597934566438198\n",
      "step: 7730, loss: 0.023143310099840164\n",
      "step: 7731, loss: 0.03667647764086723\n",
      "step: 7732, loss: 0.0005184835754334927\n",
      "step: 7733, loss: 0.004117502365261316\n",
      "step: 7734, loss: 0.003051162464544177\n",
      "step: 7735, loss: 0.0010002866620197892\n",
      "step: 7736, loss: 0.0007473371806554496\n",
      "step: 7737, loss: 0.023136401548981667\n",
      "step: 7738, loss: 0.0007241623243317008\n",
      "step: 7739, loss: 0.00036992074456065893\n",
      "step: 7740, loss: 0.0010122779058292508\n",
      "step: 7741, loss: 0.04602373018860817\n",
      "step: 7742, loss: 0.0020593570079654455\n",
      "step: 7743, loss: 0.0031080753542482853\n",
      "step: 7744, loss: 0.05448344722390175\n",
      "step: 7745, loss: 0.04021279513835907\n",
      "step: 7746, loss: 0.004145230632275343\n",
      "step: 7747, loss: 0.02490529976785183\n",
      "step: 7748, loss: 0.0016396497376263142\n",
      "step: 7749, loss: 0.04782954603433609\n",
      "step: 7750, loss: 0.0007821331964805722\n",
      "step: 7751, loss: 0.0026844232343137264\n",
      "step: 7752, loss: 0.003244395600631833\n",
      "step: 7753, loss: 0.008632384240627289\n",
      "step: 7754, loss: 0.0008800577488727868\n",
      "step: 7755, loss: 0.0004015775048173964\n",
      "step: 7756, loss: 0.03733466938138008\n",
      "step: 7757, loss: 0.03229444846510887\n",
      "step: 7758, loss: 0.0016602554824203253\n",
      "step: 7759, loss: 0.00033150907256640494\n",
      "step: 7760, loss: 0.012074252590537071\n",
      "step: 7761, loss: 0.0004581190296448767\n",
      "step: 7762, loss: 0.09180590510368347\n",
      "step: 7763, loss: 0.004373781383037567\n",
      "step: 7764, loss: 0.011294448748230934\n",
      "step: 7765, loss: 0.0011753940489143133\n",
      "step: 7766, loss: 0.0023602857254445553\n",
      "step: 7767, loss: 0.009372463449835777\n",
      "step: 7768, loss: 0.001095168641768396\n",
      "step: 7769, loss: 0.0013784217881038785\n",
      "step: 7770, loss: 0.005246832501143217\n",
      "step: 7771, loss: 0.0009271150920540094\n",
      "step: 7772, loss: 0.10584423691034317\n",
      "step: 7773, loss: 0.018442433327436447\n",
      "step: 7774, loss: 0.042961325496435165\n",
      "step: 7775, loss: 0.0017005245899781585\n",
      "step: 7776, loss: 0.0027590706013143063\n",
      "step: 7777, loss: 0.020627522841095924\n",
      "step: 7778, loss: 0.0489424504339695\n",
      "step: 7779, loss: 0.045288968831300735\n",
      "step: 7780, loss: 0.001547932275570929\n",
      "step: 7781, loss: 0.022540517151355743\n",
      "step: 7782, loss: 0.012409545481204987\n",
      "step: 7783, loss: 0.0018300050869584084\n",
      "step: 7784, loss: 0.0028321275021880865\n",
      "step: 7785, loss: 0.042076725512742996\n",
      "step: 7786, loss: 0.0002338587655685842\n",
      "step: 7787, loss: 0.01302676647901535\n",
      "step: 7788, loss: 0.006422913167625666\n",
      "step: 7789, loss: 0.00018868091865442693\n",
      "step: 7790, loss: 0.012345949187874794\n",
      "step: 7791, loss: 0.007585621904581785\n",
      "step: 7792, loss: 0.001371488906443119\n",
      "step: 7793, loss: 0.45799732208251953\n",
      "step: 7794, loss: 0.01574913039803505\n",
      "step: 7795, loss: 0.07439200580120087\n",
      "step: 7796, loss: 0.07051654160022736\n",
      "step: 7797, loss: 0.0026083618868142366\n",
      "step: 7798, loss: 0.045022014528512955\n",
      "step: 7799, loss: 0.0011167210759595037\n",
      "step: 7800, loss: 0.013968115672469139\n",
      "step: 7801, loss: 0.0018825771985575557\n",
      "step: 7802, loss: 0.013009334914386272\n",
      "step: 7803, loss: 0.050589218735694885\n",
      "step: 7804, loss: 0.04580148309469223\n",
      "step: 7805, loss: 0.000876725185662508\n",
      "step: 7806, loss: 0.010151616297662258\n",
      "step: 7807, loss: 0.001697799889370799\n",
      "step: 7808, loss: 0.001155284233391285\n",
      "step: 7809, loss: 0.01225205883383751\n",
      "step: 7810, loss: 0.008905294351279736\n",
      "step: 7811, loss: 0.047062527388334274\n",
      "step: 7812, loss: 0.0021187809761613607\n",
      "step: 7813, loss: 0.08094848692417145\n",
      "step: 7814, loss: 0.003097009612247348\n",
      "step: 7815, loss: 0.004712167661637068\n",
      "step: 7816, loss: 0.00037692682235501707\n",
      "step: 7817, loss: 0.08276446163654327\n",
      "step: 7818, loss: 0.0005450085736811161\n",
      "step: 7819, loss: 0.03134618699550629\n",
      "step: 7820, loss: 0.048763684928417206\n",
      "step: 7821, loss: 0.0042212000116705894\n",
      "step: 7822, loss: 0.0001049621423589997\n",
      "step: 7823, loss: 0.005265695508569479\n",
      "step: 7824, loss: 0.002390175126492977\n",
      "step: 7825, loss: 0.0014823583187535405\n",
      "step: 7826, loss: 0.08951129019260406\n",
      "step: 7827, loss: 0.009943176060914993\n",
      "step: 7828, loss: 0.0007898256881162524\n",
      "step: 7829, loss: 0.003779687685891986\n",
      "step: 7830, loss: 0.012082151137292385\n",
      "step: 7831, loss: 0.0003078079898841679\n",
      "step: 7832, loss: 0.0021158624440431595\n",
      "step: 7833, loss: 0.003640257753431797\n",
      "step: 7834, loss: 0.0017137486720457673\n",
      "step: 7835, loss: 0.013717064633965492\n",
      "step: 7836, loss: 0.010311087593436241\n",
      "step: 7837, loss: 0.008277539163827896\n",
      "step: 7838, loss: 0.007133181672543287\n",
      "step: 7839, loss: 0.00215801689773798\n",
      "step: 7840, loss: 0.0009374485816806555\n",
      "step: 7841, loss: 0.0005074117216281593\n",
      "step: 7842, loss: 0.002038814825937152\n",
      "step: 7843, loss: 0.0009085698984563351\n",
      "step: 7844, loss: 0.004832617472857237\n",
      "step: 7845, loss: 0.011641493998467922\n",
      "step: 7846, loss: 0.03305449336767197\n",
      "step: 7847, loss: 0.0003383909643162042\n",
      "step: 7848, loss: 0.00035448369453661144\n",
      "step: 7849, loss: 0.04757210239768028\n",
      "step: 7850, loss: 0.00034070623223669827\n",
      "step: 7851, loss: 0.0012760217068716884\n",
      "step: 7852, loss: 0.0013809370575472713\n",
      "step: 7853, loss: 0.001739323721267283\n",
      "step: 7854, loss: 0.0007888723630458117\n",
      "step: 7855, loss: 0.012468243949115276\n",
      "step: 7856, loss: 0.0020666560158133507\n",
      "step: 7857, loss: 0.0009019139688462019\n",
      "step: 7858, loss: 0.03728422522544861\n",
      "step: 7859, loss: 0.0005131815560162067\n",
      "step: 7860, loss: 0.0010039592161774635\n",
      "step: 7861, loss: 0.0032508252188563347\n",
      "step: 7862, loss: 0.0009893819224089384\n",
      "step: 7863, loss: 0.013743770308792591\n",
      "step: 7864, loss: 0.0010030060075223446\n",
      "step: 7865, loss: 0.045432187616825104\n",
      "step: 7866, loss: 0.03745255991816521\n",
      "step: 7867, loss: 0.00030504362075589597\n",
      "step: 7868, loss: 0.0009102935437113047\n",
      "step: 7869, loss: 0.00010930161806754768\n",
      "step: 7870, loss: 0.00304124946705997\n",
      "step: 7871, loss: 0.00117501278873533\n",
      "step: 7872, loss: 0.0027514358516782522\n",
      "step: 7873, loss: 0.010436372831463814\n",
      "step: 7874, loss: 0.0014382428489625454\n",
      "step: 7875, loss: 0.0024109857622534037\n",
      "step: 7876, loss: 0.014846313744783401\n",
      "step: 7877, loss: 0.002481987001374364\n",
      "step: 7878, loss: 0.011425206437706947\n",
      "step: 7879, loss: 0.0019303010776638985\n",
      "step: 7880, loss: 0.05112309381365776\n",
      "step: 7881, loss: 0.012855160050094128\n",
      "step: 7882, loss: 0.0014338722685351968\n",
      "step: 7883, loss: 0.0009305112180300057\n",
      "step: 7884, loss: 0.0023299267049878836\n",
      "step: 7885, loss: 0.0012770501198247075\n",
      "step: 7886, loss: 0.000540606677532196\n",
      "step: 7887, loss: 0.0010215346701443195\n",
      "step: 7888, loss: 0.001058539841324091\n",
      "step: 7889, loss: 0.00012913101818412542\n",
      "step: 7890, loss: 0.0012555202702060342\n",
      "step: 7891, loss: 4.334047844167799e-05\n",
      "step: 7892, loss: 0.13593652844429016\n",
      "step: 7893, loss: 0.06817451119422913\n",
      "step: 7894, loss: 0.048516057431697845\n",
      "step: 7895, loss: 0.035303812474012375\n",
      "step: 7896, loss: 0.0012157544260844588\n",
      "step: 7897, loss: 0.08268176764249802\n",
      "step: 7898, loss: 0.0004947062698192894\n",
      "step: 7899, loss: 0.0009413593215867877\n",
      "step: 7900, loss: 0.0038565981667488813\n",
      "step: 7901, loss: 0.25461918115615845\n",
      "step: 7902, loss: 0.036298442631959915\n",
      "step: 7903, loss: 0.0024748011492192745\n",
      "step: 7904, loss: 0.46101778745651245\n",
      "step: 7905, loss: 0.0007199179963208735\n",
      "step: 7906, loss: 0.01672268472611904\n",
      "step: 7907, loss: 0.0007377765141427517\n",
      "step: 7908, loss: 0.011834940873086452\n",
      "step: 7909, loss: 0.0012050170917063951\n",
      "step: 7910, loss: 0.0017620951402932405\n",
      "step: 7911, loss: 0.0006659626960754395\n",
      "step: 7912, loss: 0.0008093789801932871\n",
      "step: 7913, loss: 0.0007642310229130089\n",
      "step: 7914, loss: 0.055238813161849976\n",
      "step: 7915, loss: 0.010662640444934368\n",
      "step: 7916, loss: 0.0005400461959652603\n",
      "step: 7917, loss: 0.0012485821498557925\n",
      "step: 7918, loss: 0.048438891768455505\n",
      "step: 7919, loss: 0.0001343432377325371\n",
      "step: 7920, loss: 0.03637692332267761\n",
      "step: 7921, loss: 0.0009940157178789377\n",
      "step: 7922, loss: 0.060953009873628616\n",
      "step: 7923, loss: 0.08434729278087616\n",
      "step: 7924, loss: 0.02130984328687191\n",
      "step: 7925, loss: 0.00038801535265520215\n",
      "step: 7926, loss: 0.010869831778109074\n",
      "step: 7927, loss: 0.008500728756189346\n",
      "step: 7928, loss: 0.0016527288826182485\n",
      "step: 7929, loss: 0.011893052607774734\n",
      "step: 7930, loss: 0.012972895987331867\n",
      "step: 7931, loss: 0.05019758641719818\n",
      "step: 7932, loss: 0.008473016321659088\n",
      "step: 7933, loss: 0.0005717388703487813\n",
      "step: 7934, loss: 0.0033334053587168455\n",
      "step: 7935, loss: 0.00043570235720835626\n",
      "step: 7936, loss: 0.04790415242314339\n",
      "step: 7937, loss: 0.0008358100312761962\n",
      "step: 7938, loss: 0.0024480412248522043\n",
      "step: 7939, loss: 0.0007490952266380191\n",
      "step: 7940, loss: 0.007088750600814819\n",
      "step: 7941, loss: 0.003073974046856165\n",
      "step: 7942, loss: 0.0004775606212206185\n",
      "step: 7943, loss: 0.016600338742136955\n",
      "step: 7944, loss: 0.00015507952775806189\n",
      "step: 7945, loss: 0.056430745869874954\n",
      "step: 7946, loss: 0.03616868704557419\n",
      "step: 7947, loss: 0.002275958424434066\n",
      "step: 7948, loss: 0.00039624879718758166\n",
      "step: 7949, loss: 0.0096074678003788\n",
      "step: 7950, loss: 0.00032552823540754616\n",
      "step: 7951, loss: 0.0015998679446056485\n",
      "step: 7952, loss: 0.0011117549147456884\n",
      "step: 7953, loss: 0.0017051341710612178\n",
      "step: 7954, loss: 0.031214864924550056\n",
      "step: 7955, loss: 0.0005690789548680186\n",
      "step: 7956, loss: 0.0017484680283814669\n",
      "step: 7957, loss: 0.03506457433104515\n",
      "step: 7958, loss: 0.0004211004124954343\n",
      "step: 7959, loss: 0.021268773823976517\n",
      "step: 7960, loss: 0.0016604693373665214\n",
      "step: 7961, loss: 0.0009277303470298648\n",
      "step: 7962, loss: 0.0023326766677200794\n",
      "step: 7963, loss: 0.0007845942745916545\n",
      "step: 7964, loss: 0.0019228393211960793\n",
      "step: 7965, loss: 0.0006796218222007155\n",
      "step: 7966, loss: 0.03547556698322296\n",
      "step: 7967, loss: 0.03308393433690071\n",
      "step: 7968, loss: 0.0011339973425492644\n",
      "step: 7969, loss: 0.0013839032035320997\n",
      "step: 7970, loss: 0.0015990878455340862\n",
      "step: 7971, loss: 0.0025108489207923412\n",
      "step: 7972, loss: 0.003775474149733782\n",
      "step: 7973, loss: 0.03901136666536331\n",
      "step: 7974, loss: 0.0009669212740845978\n",
      "step: 7975, loss: 0.0007046315004117787\n",
      "step: 7976, loss: 0.0017751946579664946\n",
      "step: 7977, loss: 0.006313297897577286\n",
      "step: 7978, loss: 0.0017310011899098754\n",
      "step: 7979, loss: 0.004500414710491896\n",
      "step: 7980, loss: 0.0021563724149018526\n",
      "step: 7981, loss: 0.033255625516176224\n",
      "step: 7982, loss: 0.0031702888663858175\n",
      "step: 7983, loss: 0.0009812902426347136\n",
      "step: 7984, loss: 0.032202113419771194\n",
      "step: 7985, loss: 0.0007624783902429044\n",
      "step: 7986, loss: 0.09819885343313217\n",
      "step: 7987, loss: 0.014996536076068878\n",
      "step: 7988, loss: 0.00043969362741336226\n",
      "step: 7989, loss: 0.003496685530990362\n",
      "step: 7990, loss: 0.0020237481221556664\n",
      "step: 7991, loss: 0.0017329372931271791\n",
      "step: 7992, loss: 0.0005308724939823151\n",
      "step: 7993, loss: 0.0061428602784872055\n",
      "step: 7994, loss: 0.0018409484764561057\n",
      "step: 7995, loss: 0.005801665596663952\n",
      "step: 7996, loss: 0.0028823052998632193\n",
      "step: 7997, loss: 0.002590051619336009\n",
      "step: 7998, loss: 0.0011482476256787777\n",
      "step: 7999, loss: 0.001354108564555645\n",
      "step: 8000, loss: 0.1156664490699768\n",
      "step: 8001, loss: 0.0007394953281618655\n",
      "step: 8002, loss: 0.0016163657419383526\n",
      "step: 8003, loss: 0.0008296809392049909\n",
      "step: 8004, loss: 0.0011106820311397314\n",
      "step: 8005, loss: 0.0008356538601219654\n",
      "step: 8006, loss: 0.0005564977182075381\n",
      "step: 8007, loss: 0.0012656014878302813\n",
      "step: 8008, loss: 0.001276448485441506\n",
      "step: 8009, loss: 0.0014840998919680715\n",
      "step: 8010, loss: 0.0028514117002487183\n",
      "step: 8011, loss: 0.07589855790138245\n",
      "step: 8012, loss: 0.08186786621809006\n",
      "step: 8013, loss: 0.0007977578788995743\n",
      "step: 8014, loss: 0.020440585911273956\n",
      "step: 8015, loss: 0.011891952715814114\n",
      "step: 8016, loss: 0.013459302484989166\n",
      "step: 8017, loss: 0.018511299043893814\n",
      "step: 8018, loss: 0.15126819908618927\n",
      "step: 8019, loss: 0.046767015010118484\n",
      "step: 8020, loss: 0.0032600397244095802\n",
      "step: 8021, loss: 0.0043630474247038364\n",
      "step: 8022, loss: 0.0008172166999429464\n",
      "step: 8023, loss: 0.0003305504214949906\n",
      "step: 8024, loss: 0.06499713659286499\n",
      "step: 8025, loss: 0.012654300779104233\n",
      "step: 8026, loss: 0.010974351316690445\n",
      "step: 8027, loss: 0.000651639187708497\n",
      "step: 8028, loss: 0.00157631014008075\n",
      "step: 8029, loss: 0.0007800568710081279\n",
      "step: 8030, loss: 0.04073667526245117\n",
      "step: 8031, loss: 0.0007540743099525571\n",
      "step: 8032, loss: 0.001309370738454163\n",
      "step: 8033, loss: 0.0017786120297387242\n",
      "step: 8034, loss: 0.022463226690888405\n",
      "step: 8035, loss: 0.03260824829339981\n",
      "step: 8036, loss: 0.0020631407387554646\n",
      "step: 8037, loss: 0.052471864968538284\n",
      "step: 8038, loss: 0.002552306978031993\n",
      "step: 8039, loss: 0.01206995639950037\n",
      "step: 8040, loss: 0.009101983159780502\n",
      "step: 8041, loss: 0.0007972651510499418\n",
      "step: 8042, loss: 0.0022725267335772514\n",
      "step: 8043, loss: 0.04749120771884918\n",
      "step: 8044, loss: 0.11717895418405533\n",
      "step: 8045, loss: 0.0013933866284787655\n",
      "step: 8046, loss: 0.0008796424954198301\n",
      "step: 8047, loss: 0.006533119827508926\n",
      "step: 8048, loss: 0.012560221366584301\n",
      "step: 8049, loss: 0.013324555940926075\n",
      "step: 8050, loss: 0.010342559777200222\n",
      "step: 8051, loss: 0.0024960676673799753\n",
      "step: 8052, loss: 0.04445242881774902\n",
      "step: 8053, loss: 0.0013593269977718592\n",
      "step: 8054, loss: 0.0829707533121109\n",
      "step: 8055, loss: 0.03882318362593651\n",
      "step: 8056, loss: 0.00028356333496049047\n",
      "step: 8057, loss: 0.0011691212421283126\n",
      "step: 8058, loss: 0.001213319250382483\n",
      "step: 8059, loss: 0.03942505642771721\n",
      "step: 8060, loss: 0.00029224681202322245\n",
      "step: 8061, loss: 0.0015217178734019399\n",
      "step: 8062, loss: 0.005939345806837082\n",
      "step: 8063, loss: 0.046945419162511826\n",
      "step: 8064, loss: 0.005559409968554974\n",
      "step: 8065, loss: 0.0009507821523584425\n",
      "step: 8066, loss: 0.08542612195014954\n",
      "step: 8067, loss: 0.00034733646316453815\n",
      "step: 8068, loss: 0.07057169824838638\n",
      "step: 8069, loss: 0.0005171206430532038\n",
      "step: 8070, loss: 0.03066847287118435\n",
      "step: 8071, loss: 0.04748883843421936\n",
      "step: 8072, loss: 0.003836918855085969\n",
      "step: 8073, loss: 0.07806802541017532\n",
      "step: 8074, loss: 0.015656298026442528\n",
      "step: 8075, loss: 0.0014975873054936528\n",
      "step: 8076, loss: 0.0031007742509245872\n",
      "step: 8077, loss: 0.0008895937353372574\n",
      "step: 8078, loss: 0.0018507225904613733\n",
      "step: 8079, loss: 0.001957249827682972\n",
      "step: 8080, loss: 0.12744945287704468\n",
      "step: 8081, loss: 0.0023799350019544363\n",
      "step: 8082, loss: 0.00012101719039492309\n",
      "step: 8083, loss: 0.014269569888710976\n",
      "step: 8084, loss: 0.011468017473816872\n",
      "step: 8085, loss: 0.0037434124387800694\n",
      "step: 8086, loss: 0.0010332884266972542\n",
      "step: 8087, loss: 0.002347350586205721\n",
      "step: 8088, loss: 0.00141643057577312\n",
      "step: 8089, loss: 0.06189702823758125\n",
      "step: 8090, loss: 9.572664566803724e-05\n",
      "step: 8091, loss: 0.01599871553480625\n",
      "step: 8092, loss: 0.0013251324417069554\n",
      "step: 8093, loss: 0.0010651379125192761\n",
      "step: 8094, loss: 0.04881715774536133\n",
      "step: 8095, loss: 0.0009889212669804692\n",
      "step: 8096, loss: 0.0033465793821960688\n",
      "step: 8097, loss: 0.012968138791620731\n",
      "step: 8098, loss: 0.0025973382871598005\n",
      "step: 8099, loss: 0.003501113271340728\n",
      "step: 8100, loss: 0.0013433352578431368\n",
      "step: 8101, loss: 0.03540237247943878\n",
      "step: 8102, loss: 0.0016563491662964225\n",
      "step: 8103, loss: 0.011634587310254574\n",
      "step: 8104, loss: 0.052637699991464615\n",
      "step: 8105, loss: 0.001570702064782381\n",
      "step: 8106, loss: 0.022701598703861237\n",
      "step: 8107, loss: 0.04600721225142479\n",
      "step: 8108, loss: 0.003687170334160328\n",
      "step: 8109, loss: 0.009013200178742409\n",
      "step: 8110, loss: 0.0003420767898205668\n",
      "step: 8111, loss: 0.004604999907314777\n",
      "step: 8112, loss: 0.0004034534504171461\n",
      "step: 8113, loss: 0.003914802800863981\n",
      "step: 8114, loss: 0.001744904788210988\n",
      "step: 8115, loss: 0.0012653449084609747\n",
      "step: 8116, loss: 0.001099310931749642\n",
      "step: 8117, loss: 0.0003871425869874656\n",
      "step: 8118, loss: 0.00011844463733723387\n",
      "step: 8119, loss: 0.0033383904956281185\n",
      "step: 8120, loss: 0.0013333570677787066\n",
      "step: 8121, loss: 0.01259785145521164\n",
      "step: 8122, loss: 0.0007607368170283735\n",
      "step: 8123, loss: 0.01006131898611784\n",
      "step: 8124, loss: 0.05659394711256027\n",
      "step: 8125, loss: 0.009649506770074368\n",
      "step: 8126, loss: 0.0007090634317137301\n",
      "step: 8127, loss: 0.002093774499371648\n",
      "step: 8128, loss: 0.0014291106490418315\n",
      "step: 8129, loss: 0.010235637426376343\n",
      "step: 8130, loss: 0.014847581274807453\n",
      "step: 8131, loss: 0.001115909544751048\n",
      "step: 8132, loss: 0.0010877749882638454\n",
      "step: 8133, loss: 0.004534068517386913\n",
      "step: 8134, loss: 0.0003792478528339416\n",
      "step: 8135, loss: 0.001801772741600871\n",
      "step: 8136, loss: 0.011234628036618233\n",
      "step: 8137, loss: 0.0015166732482612133\n",
      "step: 8138, loss: 0.000412167573813349\n",
      "step: 8139, loss: 0.0035792822018265724\n",
      "step: 8140, loss: 0.0016983795212581754\n",
      "step: 8141, loss: 0.034760572016239166\n",
      "step: 8142, loss: 0.001489042304456234\n",
      "step: 8143, loss: 0.00998317077755928\n",
      "step: 8144, loss: 0.0003568941028788686\n",
      "step: 8145, loss: 0.05918051674962044\n",
      "step: 8146, loss: 0.047458529472351074\n",
      "step: 8147, loss: 0.044939398765563965\n",
      "step: 8148, loss: 0.00013014080468565226\n",
      "step: 8149, loss: 0.015317960642278194\n",
      "step: 8150, loss: 0.04437096416950226\n",
      "step: 8151, loss: 0.003533783834427595\n",
      "step: 8152, loss: 0.007982942275702953\n",
      "step: 8153, loss: 0.0022773616947233677\n",
      "step: 8154, loss: 0.01285411324352026\n",
      "step: 8155, loss: 0.01749313436448574\n",
      "step: 8156, loss: 0.0027330347802489996\n",
      "step: 8157, loss: 0.004890359006822109\n",
      "step: 8158, loss: 0.0006678951322101057\n",
      "step: 8159, loss: 0.0011599624995142221\n",
      "step: 8160, loss: 0.0034716958180069923\n",
      "step: 8161, loss: 0.04854157567024231\n",
      "step: 8162, loss: 0.017515625804662704\n",
      "step: 8163, loss: 0.0023246086202561855\n",
      "step: 8164, loss: 0.0006474737310782075\n",
      "step: 8165, loss: 0.04509563744068146\n",
      "step: 8166, loss: 0.02846226654946804\n",
      "step: 8167, loss: 0.026484636589884758\n",
      "step: 8168, loss: 0.027871515601873398\n",
      "step: 8169, loss: 0.0005741565837524831\n",
      "step: 8170, loss: 0.00031062366906553507\n",
      "step: 8171, loss: 0.007122967857867479\n",
      "step: 8172, loss: 0.001986462390050292\n",
      "step: 8173, loss: 0.05887316167354584\n",
      "step: 8174, loss: 0.0004247240722179413\n",
      "step: 8175, loss: 0.001634262502193451\n",
      "step: 8176, loss: 0.010923509486019611\n",
      "step: 8177, loss: 0.0006093825795687735\n",
      "step: 8178, loss: 0.002346822526305914\n",
      "step: 8179, loss: 0.001208915957249701\n",
      "step: 8180, loss: 0.11372017860412598\n",
      "step: 8181, loss: 0.01108176726847887\n",
      "step: 8182, loss: 0.0024769201409071684\n",
      "step: 8183, loss: 0.050856199115514755\n",
      "step: 8184, loss: 0.0027309167198836803\n",
      "step: 8185, loss: 0.0005846270360052586\n",
      "step: 8186, loss: 0.0005048003513365984\n",
      "step: 8187, loss: 0.01173847634345293\n",
      "step: 8188, loss: 0.0002082713763229549\n",
      "step: 8189, loss: 0.001481531886383891\n",
      "step: 8190, loss: 0.0017705742502585053\n",
      "step: 8191, loss: 0.0005806850967928767\n",
      "step: 8192, loss: 0.0016855985159054399\n",
      "step: 8193, loss: 0.037763528525829315\n",
      "step: 8194, loss: 0.017422545701265335\n",
      "step: 8195, loss: 0.000332672061631456\n",
      "step: 8196, loss: 0.0015869885683059692\n",
      "step: 8197, loss: 0.003128915326669812\n",
      "step: 8198, loss: 0.0022162466775625944\n",
      "step: 8199, loss: 0.001256722491234541\n",
      "step: 8200, loss: 0.013584058731794357\n",
      "step: 8201, loss: 0.0005976267857477069\n",
      "step: 8202, loss: 0.002861828776076436\n",
      "step: 8203, loss: 0.012575944885611534\n",
      "step: 8204, loss: 0.0009123116615228355\n",
      "step: 8205, loss: 0.0010825740173459053\n",
      "step: 8206, loss: 0.0016242462443187833\n",
      "step: 8207, loss: 0.08233995735645294\n",
      "step: 8208, loss: 0.0019369625952094793\n",
      "step: 8209, loss: 0.1503450721502304\n",
      "step: 8210, loss: 0.0010304447496309876\n",
      "step: 8211, loss: 0.0609658844769001\n",
      "step: 8212, loss: 0.0007925783284008503\n",
      "step: 8213, loss: 0.010197348892688751\n",
      "step: 8214, loss: 0.0021601205226033926\n",
      "step: 8215, loss: 0.06972351670265198\n",
      "step: 8216, loss: 0.004459332674741745\n",
      "step: 8217, loss: 0.013014236465096474\n",
      "step: 8218, loss: 0.0010903582442551851\n",
      "step: 8219, loss: 0.012942010536789894\n",
      "step: 8220, loss: 0.10976175963878632\n",
      "step: 8221, loss: 0.07239774614572525\n",
      "step: 8222, loss: 0.00119198695756495\n",
      "step: 8223, loss: 0.002307942369952798\n",
      "step: 8224, loss: 0.001199243706651032\n",
      "step: 8225, loss: 0.002892550313845277\n",
      "step: 8226, loss: 0.009736728854477406\n",
      "step: 8227, loss: 0.0015361924888566136\n",
      "step: 8228, loss: 0.004185749217867851\n",
      "step: 8229, loss: 0.001127248746342957\n",
      "step: 8230, loss: 0.037929486483335495\n",
      "step: 8231, loss: 0.00010999506048392504\n",
      "step: 8232, loss: 0.00047702412120997906\n",
      "step: 8233, loss: 0.00039175787242129445\n",
      "step: 8234, loss: 0.0034753349609673023\n",
      "step: 8235, loss: 0.0007331109954975545\n",
      "step: 8236, loss: 0.04151524603366852\n",
      "step: 8237, loss: 0.0016603654948994517\n",
      "step: 8238, loss: 0.10703304409980774\n",
      "step: 8239, loss: 0.0011239481391385198\n",
      "step: 8240, loss: 0.0016763187013566494\n",
      "step: 8241, loss: 0.009767558425664902\n",
      "step: 8242, loss: 0.0510985404253006\n",
      "step: 8243, loss: 0.0008707770612090826\n",
      "step: 8244, loss: 0.0027136944700032473\n",
      "step: 8245, loss: 0.0008217633585445583\n",
      "step: 8246, loss: 0.010703708045184612\n",
      "step: 8247, loss: 0.003000629832968116\n",
      "step: 8248, loss: 0.0049023753963410854\n",
      "step: 8249, loss: 0.0013761721784248948\n",
      "step: 8250, loss: 0.00456292275339365\n",
      "step: 8251, loss: 0.0009865306783467531\n",
      "step: 8252, loss: 0.040301550179719925\n",
      "step: 8253, loss: 0.0018246662802994251\n",
      "step: 8254, loss: 0.0003233674797229469\n",
      "step: 8255, loss: 0.04930682107806206\n",
      "step: 8256, loss: 0.07757056504487991\n",
      "step: 8257, loss: 0.01428793091326952\n",
      "step: 8258, loss: 0.001313794986344874\n",
      "step: 8259, loss: 0.002037306549027562\n",
      "step: 8260, loss: 0.009697554633021355\n",
      "step: 8261, loss: 0.00040089432150125504\n",
      "step: 8262, loss: 0.033645644783973694\n",
      "step: 8263, loss: 0.0006859077257104218\n",
      "step: 8264, loss: 0.004928060341626406\n",
      "step: 8265, loss: 0.013092168606817722\n",
      "step: 8266, loss: 0.0003429044736549258\n",
      "step: 8267, loss: 0.0022270435001701117\n",
      "step: 8268, loss: 0.035124555230140686\n",
      "step: 8269, loss: 0.040015049278736115\n",
      "step: 8270, loss: 0.0004707043990492821\n",
      "step: 8271, loss: 0.020246289670467377\n",
      "step: 8272, loss: 0.0010896519524976611\n",
      "step: 8273, loss: 0.04882694408297539\n",
      "step: 8274, loss: 0.08266715705394745\n",
      "step: 8275, loss: 0.0012347543379291892\n",
      "step: 8276, loss: 0.0018381642876192927\n",
      "step: 8277, loss: 0.0015967990038916469\n",
      "step: 8278, loss: 0.0024083727039396763\n",
      "step: 8279, loss: 0.00018513042596168816\n",
      "step: 8280, loss: 0.007035876624286175\n",
      "step: 8281, loss: 0.0035706765484064817\n",
      "step: 8282, loss: 0.012585652992129326\n",
      "step: 8283, loss: 0.000727791921235621\n",
      "step: 8284, loss: 0.01805945672094822\n",
      "step: 8285, loss: 0.036050621420145035\n",
      "step: 8286, loss: 0.0003859316639136523\n",
      "step: 8287, loss: 0.0006133273127488792\n",
      "step: 8288, loss: 0.051839347928762436\n",
      "step: 8289, loss: 0.013789770193397999\n",
      "step: 8290, loss: 0.0007838773890398443\n",
      "step: 8291, loss: 0.002747482620179653\n",
      "step: 8292, loss: 0.011675288900732994\n",
      "step: 8293, loss: 0.0028754225932061672\n",
      "step: 8294, loss: 0.00042218249291181564\n",
      "step: 8295, loss: 0.0005264541250653565\n",
      "step: 8296, loss: 0.00039631695835851133\n",
      "step: 8297, loss: 0.002692623995244503\n",
      "step: 8298, loss: 0.0011344118975102901\n",
      "step: 8299, loss: 0.0012336946092545986\n",
      "step: 8300, loss: 0.01232079602777958\n",
      "step: 8301, loss: 0.0006729034939780831\n",
      "step: 8302, loss: 0.002281646244227886\n",
      "step: 8303, loss: 0.006224989891052246\n",
      "step: 8304, loss: 0.0049326615408062935\n",
      "step: 8305, loss: 0.0095197893679142\n",
      "step: 8306, loss: 0.001716815517283976\n",
      "step: 8307, loss: 0.00043111550621688366\n",
      "step: 8308, loss: 0.0030704664532095194\n",
      "step: 8309, loss: 0.0009112033294513822\n",
      "step: 8310, loss: 0.0012685349211096764\n",
      "step: 8311, loss: 0.0005514423246495426\n",
      "step: 8312, loss: 0.0010681372368708253\n",
      "step: 8313, loss: 0.0029076344799250364\n",
      "step: 8314, loss: 0.036980126053094864\n",
      "step: 8315, loss: 0.003076936351135373\n",
      "step: 8316, loss: 0.0005259354948066175\n",
      "step: 8317, loss: 0.0011398553615435958\n",
      "step: 8318, loss: 0.004054514225572348\n",
      "step: 8319, loss: 0.0034130895510315895\n",
      "step: 8320, loss: 0.04726920649409294\n",
      "step: 8321, loss: 0.0015394962392747402\n",
      "step: 8322, loss: 0.000752513122279197\n",
      "step: 8323, loss: 0.01226760819554329\n",
      "step: 8324, loss: 0.04521603509783745\n",
      "step: 8325, loss: 0.0006334941135719419\n",
      "step: 8326, loss: 0.00046353493235073984\n",
      "step: 8327, loss: 0.00984929408878088\n",
      "step: 8328, loss: 0.00868640560656786\n",
      "step: 8329, loss: 0.06323013454675674\n",
      "step: 8330, loss: 0.04836033657193184\n",
      "step: 8331, loss: 0.0007961987867020071\n",
      "step: 8332, loss: 7.29736711946316e-05\n",
      "step: 8333, loss: 0.009270863607525826\n",
      "step: 8334, loss: 0.0001167560403700918\n",
      "step: 8335, loss: 0.0483628511428833\n",
      "step: 8336, loss: 0.0378202348947525\n",
      "step: 8337, loss: 0.002168011851608753\n",
      "step: 8338, loss: 0.0007971869199536741\n",
      "step: 8339, loss: 0.048161961138248444\n",
      "step: 8340, loss: 0.00013336649863049388\n",
      "step: 8341, loss: 0.0021927827037870884\n",
      "step: 8342, loss: 0.0037164813838899136\n",
      "step: 8343, loss: 0.023388009518384933\n",
      "step: 8344, loss: 0.04756984859704971\n",
      "step: 8345, loss: 0.002113955095410347\n",
      "step: 8346, loss: 0.04528989642858505\n",
      "step: 8347, loss: 0.03228442743420601\n",
      "step: 8348, loss: 0.0005327946855686605\n",
      "step: 8349, loss: 0.0008449626038782299\n",
      "step: 8350, loss: 0.0004575220518745482\n",
      "step: 8351, loss: 0.0006170318811200559\n",
      "step: 8352, loss: 0.04728332906961441\n",
      "step: 8353, loss: 0.0009649637504480779\n",
      "step: 8354, loss: 0.0004106906708329916\n",
      "step: 8355, loss: 0.018992843106389046\n",
      "step: 8356, loss: 0.047347117215394974\n",
      "step: 8357, loss: 0.024535486474633217\n",
      "step: 8358, loss: 0.0014344598166644573\n",
      "step: 8359, loss: 0.0019566365517675877\n",
      "step: 8360, loss: 0.0019336161203682423\n",
      "step: 8361, loss: 0.00045023628626950085\n",
      "step: 8362, loss: 0.01757746934890747\n",
      "step: 8363, loss: 0.004235307220369577\n",
      "step: 8364, loss: 0.004281571600586176\n",
      "step: 8365, loss: 0.07146251946687698\n",
      "step: 8366, loss: 0.003142847679555416\n",
      "step: 8367, loss: 0.00016430577670689672\n",
      "step: 8368, loss: 0.0008279391913674772\n",
      "step: 8369, loss: 0.00048245766083709896\n",
      "step: 8370, loss: 0.0008011518511921167\n",
      "step: 8371, loss: 0.000705790298525244\n",
      "step: 8372, loss: 0.013469883240759373\n",
      "step: 8373, loss: 0.0007665817392989993\n",
      "step: 8374, loss: 0.0008923974819481373\n",
      "step: 8375, loss: 0.021203534677624702\n",
      "step: 8376, loss: 0.011003028601408005\n",
      "step: 8377, loss: 0.01313578337430954\n",
      "step: 8378, loss: 0.0009023552993312478\n",
      "step: 8379, loss: 0.0003756242513190955\n",
      "step: 8380, loss: 0.0007450750563293695\n",
      "step: 8381, loss: 0.000995074980892241\n",
      "step: 8382, loss: 0.05009317398071289\n",
      "step: 8383, loss: 0.05154723674058914\n",
      "step: 8384, loss: 0.01100957952439785\n",
      "step: 8385, loss: 0.00042671296978369355\n",
      "step: 8386, loss: 0.00034417270217090845\n",
      "step: 8387, loss: 0.009252626448869705\n",
      "step: 8388, loss: 0.0007580244564451277\n",
      "step: 8389, loss: 0.0025567326229065657\n",
      "step: 8390, loss: 0.04047983139753342\n",
      "step: 8391, loss: 0.014882927760481834\n",
      "step: 8392, loss: 0.0014010671293362975\n",
      "step: 8393, loss: 0.0006626347312703729\n",
      "step: 8394, loss: 0.0037239089142531157\n",
      "step: 8395, loss: 0.00028239531093277037\n",
      "step: 8396, loss: 0.03339245915412903\n",
      "step: 8397, loss: 0.0015544609632343054\n",
      "step: 8398, loss: 0.00030440837144851685\n",
      "step: 8399, loss: 0.005134023260325193\n",
      "step: 8400, loss: 0.0007435567677021027\n",
      "step: 8401, loss: 0.0029202732257544994\n",
      "step: 8402, loss: 0.14304599165916443\n",
      "step: 8403, loss: 0.00035428747651167214\n",
      "step: 8404, loss: 0.009563070721924305\n",
      "step: 8405, loss: 0.001699176267720759\n",
      "step: 8406, loss: 0.00012105786299798638\n",
      "step: 8407, loss: 0.002837007399648428\n",
      "step: 8408, loss: 0.00038636050885543227\n",
      "step: 8409, loss: 0.0025658051017671824\n",
      "step: 8410, loss: 0.03989790380001068\n",
      "step: 8411, loss: 0.11745700240135193\n",
      "step: 8412, loss: 0.0014335208106786013\n",
      "step: 8413, loss: 0.0028077035676687956\n",
      "step: 8414, loss: 0.0009100568131543696\n",
      "step: 8415, loss: 0.0014701752224937081\n",
      "step: 8416, loss: 0.0007057162583805621\n",
      "step: 8417, loss: 2.8041435143677518e-05\n",
      "step: 8418, loss: 0.0007577190408483148\n",
      "step: 8419, loss: 0.0018648606492206454\n",
      "step: 8420, loss: 0.0006519824382849038\n",
      "step: 8421, loss: 0.0005826147389598191\n",
      "step: 8422, loss: 0.0494532585144043\n",
      "step: 8423, loss: 0.0011481039691716433\n",
      "step: 8424, loss: 0.011780950240790844\n",
      "step: 8425, loss: 0.00010641844710335135\n",
      "step: 8426, loss: 0.0017945285653695464\n",
      "step: 8427, loss: 0.025785809382796288\n",
      "step: 8428, loss: 0.0007447911775670946\n",
      "step: 8429, loss: 0.03780711442232132\n",
      "step: 8430, loss: 0.00016884518845472485\n",
      "step: 8431, loss: 0.0004534534818958491\n",
      "step: 8432, loss: 0.0010709968628361821\n",
      "step: 8433, loss: 0.03426294028759003\n",
      "step: 8434, loss: 0.05514829233288765\n",
      "step: 8435, loss: 0.037712790071964264\n",
      "step: 8436, loss: 0.03384511545300484\n",
      "step: 8437, loss: 0.012331180274486542\n",
      "step: 8438, loss: 0.010097984224557877\n",
      "step: 8439, loss: 0.009380143135786057\n",
      "step: 8440, loss: 0.0015644064405933022\n",
      "step: 8441, loss: 0.012236291542649269\n",
      "step: 8442, loss: 0.00015141896437853575\n",
      "step: 8443, loss: 0.004161621443927288\n",
      "step: 8444, loss: 0.03685016557574272\n",
      "step: 8445, loss: 0.01594480685889721\n",
      "step: 8446, loss: 0.04831099137663841\n",
      "step: 8447, loss: 0.010206840932369232\n",
      "step: 8448, loss: 0.03362297639250755\n",
      "step: 8449, loss: 0.0031660133972764015\n",
      "step: 8450, loss: 0.0009156665182672441\n",
      "step: 8451, loss: 0.0022208020091056824\n",
      "step: 8452, loss: 0.00041767992661334574\n",
      "step: 8453, loss: 0.035763490945100784\n",
      "step: 8454, loss: 0.03553000092506409\n",
      "step: 8455, loss: 0.004208218306303024\n",
      "step: 8456, loss: 0.0028809683863073587\n",
      "step: 8457, loss: 0.0007478892803192139\n",
      "step: 8458, loss: 0.010559610091149807\n",
      "step: 8459, loss: 0.000775209569837898\n",
      "step: 8460, loss: 0.0019257357344031334\n",
      "step: 8461, loss: 0.019465524703264236\n",
      "step: 8462, loss: 0.0021875742822885513\n",
      "step: 8463, loss: 0.03594321385025978\n",
      "step: 8464, loss: 0.00013401109026744962\n",
      "step: 8465, loss: 0.020206240937113762\n",
      "step: 8466, loss: 0.00046940677566453815\n",
      "step: 8467, loss: 0.04714246839284897\n",
      "step: 8468, loss: 0.0011422240640968084\n",
      "step: 8469, loss: 0.0010276933899149299\n",
      "step: 8470, loss: 0.04992445930838585\n",
      "step: 8471, loss: 0.0025986998807638884\n",
      "step: 8472, loss: 0.05539914593100548\n",
      "step: 8473, loss: 0.0006974210846237838\n",
      "step: 8474, loss: 0.0008991967770271003\n",
      "step: 8475, loss: 0.008975239470601082\n",
      "step: 8476, loss: 0.0007923265802673995\n",
      "step: 8477, loss: 0.001010832842439413\n",
      "step: 8478, loss: 0.0047137197107076645\n",
      "step: 8479, loss: 0.0015910507645457983\n",
      "step: 8480, loss: 0.0019736322574317455\n",
      "step: 8481, loss: 0.001575133646838367\n",
      "step: 8482, loss: 0.004976531025022268\n",
      "step: 8483, loss: 0.008397464640438557\n",
      "step: 8484, loss: 0.0012193768052384257\n",
      "step: 8485, loss: 0.0007452690624631941\n",
      "step: 8486, loss: 0.002165947575122118\n",
      "step: 8487, loss: 0.011033684015274048\n",
      "step: 8488, loss: 0.05093608424067497\n",
      "step: 8489, loss: 0.00031871069222688675\n",
      "step: 8490, loss: 0.0562065988779068\n",
      "step: 8491, loss: 0.008007418364286423\n",
      "step: 8492, loss: 0.00031447672517970204\n",
      "step: 8493, loss: 0.0016583900433033705\n",
      "step: 8494, loss: 0.003850107081234455\n",
      "step: 8495, loss: 0.0014288723468780518\n",
      "step: 8496, loss: 0.024483120068907738\n",
      "step: 8497, loss: 0.0006130104302428663\n",
      "step: 8498, loss: 0.0028760519344359636\n",
      "step: 8499, loss: 0.0007563072722405195\n",
      "step: 8500, loss: 0.00010801868484122679\n",
      "step: 8501, loss: 0.015402705408632755\n",
      "step: 8502, loss: 0.0007292353548109531\n",
      "step: 8503, loss: 0.01297074556350708\n",
      "step: 8504, loss: 0.0010670659830793738\n",
      "step: 8505, loss: 0.0006943822372704744\n",
      "step: 8506, loss: 0.004050271585583687\n",
      "step: 8507, loss: 0.04922818765044212\n",
      "step: 8508, loss: 0.0023320394102483988\n",
      "step: 8509, loss: 0.0010773856192827225\n",
      "step: 8510, loss: 0.04826251044869423\n",
      "step: 8511, loss: 0.0030612170230597258\n",
      "step: 8512, loss: 0.0010053804144263268\n",
      "step: 8513, loss: 0.0016517357435077429\n",
      "step: 8514, loss: 0.0011710127582773566\n",
      "step: 8515, loss: 0.0016346032498404384\n",
      "step: 8516, loss: 0.09785611927509308\n",
      "step: 8517, loss: 0.0018760531675070524\n",
      "step: 8518, loss: 0.016208454966545105\n",
      "step: 8519, loss: 0.011811540462076664\n",
      "step: 8520, loss: 0.0006814116495661438\n",
      "step: 8521, loss: 0.04509767144918442\n",
      "step: 8522, loss: 0.00011296862066956237\n",
      "step: 8523, loss: 0.00799760315567255\n",
      "step: 8524, loss: 0.0013808619696646929\n",
      "step: 8525, loss: 0.00232247868552804\n",
      "step: 8526, loss: 0.0010790795786306262\n",
      "step: 8527, loss: 0.0020442805252969265\n",
      "step: 8528, loss: 0.0007776282727718353\n",
      "step: 8529, loss: 0.00046461925376206636\n",
      "step: 8530, loss: 0.012448063120245934\n",
      "step: 8531, loss: 0.012506140395998955\n",
      "step: 8532, loss: 0.0007793369004502892\n",
      "step: 8533, loss: 0.0014016496716067195\n",
      "step: 8534, loss: 0.045962635427713394\n",
      "step: 8535, loss: 0.04356035590171814\n",
      "step: 8536, loss: 0.0017804955132305622\n",
      "step: 8537, loss: 0.00112648936919868\n",
      "step: 8538, loss: 0.010373945347964764\n",
      "step: 8539, loss: 0.00044888112461194396\n",
      "step: 8540, loss: 0.0018614837899804115\n",
      "step: 8541, loss: 0.03396245092153549\n",
      "step: 8542, loss: 0.049267906695604324\n",
      "step: 8543, loss: 0.1320732831954956\n",
      "step: 8544, loss: 0.0010038488544523716\n",
      "step: 8545, loss: 0.014193011447787285\n",
      "step: 8546, loss: 0.0008491477346979082\n",
      "step: 8547, loss: 0.0008923813002184033\n",
      "step: 8548, loss: 0.048312969505786896\n",
      "step: 8549, loss: 0.03500930964946747\n",
      "step: 8550, loss: 0.00032928443397395313\n",
      "step: 8551, loss: 0.00043443674803711474\n",
      "step: 8552, loss: 0.0022940654307603836\n",
      "step: 8553, loss: 0.002151248510926962\n",
      "step: 8554, loss: 0.06807529181241989\n",
      "step: 8555, loss: 0.03604166954755783\n",
      "step: 8556, loss: 0.000221593480091542\n",
      "step: 8557, loss: 0.010539128445088863\n",
      "step: 8558, loss: 0.03141767159104347\n",
      "step: 8559, loss: 0.009429641999304295\n",
      "step: 8560, loss: 0.035421960055828094\n",
      "step: 8561, loss: 0.022498179227113724\n",
      "step: 8562, loss: 0.002014375291764736\n",
      "step: 8563, loss: 0.000926754844840616\n",
      "step: 8564, loss: 0.0014093364588916302\n",
      "step: 8565, loss: 0.0019276776583865285\n",
      "step: 8566, loss: 0.0008132855873554945\n",
      "step: 8567, loss: 0.0007810883107595146\n",
      "step: 8568, loss: 0.0001750374212861061\n",
      "step: 8569, loss: 0.029585640877485275\n",
      "step: 8570, loss: 0.00029520929092541337\n",
      "step: 8571, loss: 0.01835830882191658\n",
      "step: 8572, loss: 0.017776988446712494\n",
      "step: 8573, loss: 0.0017218251014128327\n",
      "step: 8574, loss: 0.0026399819180369377\n",
      "step: 8575, loss: 0.10875820368528366\n",
      "step: 8576, loss: 0.002600267296656966\n",
      "step: 8577, loss: 0.03694523498415947\n",
      "step: 8578, loss: 0.0028724169824272394\n",
      "step: 8579, loss: 0.000812084530480206\n",
      "step: 8580, loss: 0.03607485815882683\n",
      "step: 8581, loss: 0.00034160169889219105\n",
      "step: 8582, loss: 0.0004959364887326956\n",
      "step: 8583, loss: 0.00203885231167078\n",
      "step: 8584, loss: 0.0005475576617754996\n",
      "step: 8585, loss: 0.0004387787776067853\n",
      "step: 8586, loss: 0.055920857936143875\n",
      "step: 8587, loss: 0.01505428459495306\n",
      "step: 8588, loss: 0.0016902917996048927\n",
      "step: 8589, loss: 0.0018586120568215847\n",
      "step: 8590, loss: 0.0015281325904652476\n",
      "step: 8591, loss: 0.012335946783423424\n",
      "step: 8592, loss: 0.0014321174239739776\n",
      "step: 8593, loss: 0.03855342045426369\n",
      "step: 8594, loss: 0.0004030341515317559\n",
      "step: 8595, loss: 0.0016615501372143626\n",
      "step: 8596, loss: 0.0006937083671800792\n",
      "step: 8597, loss: 0.006400647573173046\n",
      "step: 8598, loss: 0.0026182555593550205\n",
      "step: 8599, loss: 0.01197044923901558\n",
      "step: 8600, loss: 0.0002850688761100173\n",
      "step: 8601, loss: 0.0018229392590001225\n",
      "step: 8602, loss: 0.008345622569322586\n",
      "step: 8603, loss: 0.0004377016157377511\n",
      "step: 8604, loss: 0.0027621728368103504\n",
      "step: 8605, loss: 0.005229850299656391\n",
      "step: 8606, loss: 0.05405783653259277\n",
      "step: 8607, loss: 0.002765075536444783\n",
      "step: 8608, loss: 0.06131129339337349\n",
      "step: 8609, loss: 0.012614714913070202\n",
      "step: 8610, loss: 0.052236832678318024\n",
      "step: 8611, loss: 0.009016629308462143\n",
      "step: 8612, loss: 0.03164292871952057\n",
      "step: 8613, loss: 0.0008155767573043704\n",
      "step: 8614, loss: 0.0009858692064881325\n",
      "step: 8615, loss: 0.02064112387597561\n",
      "step: 8616, loss: 0.015215489082038403\n",
      "step: 8617, loss: 0.013198712840676308\n",
      "step: 8618, loss: 0.0005436809151433408\n",
      "step: 8619, loss: 0.057622361928224564\n",
      "step: 8620, loss: 0.0005829957663081586\n",
      "step: 8621, loss: 0.02859821543097496\n",
      "step: 8622, loss: 0.00014734957949258387\n",
      "step: 8623, loss: 0.0015581439947709441\n",
      "step: 8624, loss: 0.0029973008204251528\n",
      "step: 8625, loss: 0.0004984313854947686\n",
      "step: 8626, loss: 0.0006338888197205961\n",
      "step: 8627, loss: 0.0014821053482592106\n",
      "step: 8628, loss: 0.0018603929784148932\n",
      "step: 8629, loss: 0.05814318731427193\n",
      "step: 8630, loss: 0.007087063975632191\n",
      "step: 8631, loss: 0.0008142103324644268\n",
      "step: 8632, loss: 0.004496017005294561\n",
      "step: 8633, loss: 0.0008505329606123269\n",
      "step: 8634, loss: 0.0001037993497448042\n",
      "step: 8635, loss: 0.014513435773551464\n",
      "step: 8636, loss: 0.0004503471136558801\n",
      "step: 8637, loss: 0.0006421777070499957\n",
      "step: 8638, loss: 0.0007848021923564374\n",
      "step: 8639, loss: 4.761568197864108e-05\n",
      "step: 8640, loss: 0.0008362856460735202\n",
      "step: 8641, loss: 0.0011361378710716963\n",
      "step: 8642, loss: 0.002905808389186859\n",
      "step: 8643, loss: 0.03773006051778793\n",
      "step: 8644, loss: 0.0013388741062954068\n",
      "step: 8645, loss: 0.0008306269883178174\n",
      "step: 8646, loss: 0.055343180894851685\n",
      "step: 8647, loss: 0.004739896859973669\n",
      "step: 8648, loss: 0.0029921226669102907\n",
      "step: 8649, loss: 0.051101990044116974\n",
      "step: 8650, loss: 0.0018285150872543454\n",
      "step: 8651, loss: 0.015037484467029572\n",
      "step: 8652, loss: 0.002176448702812195\n",
      "step: 8653, loss: 0.0017316484590992332\n",
      "step: 8654, loss: 0.003219437086954713\n",
      "step: 8655, loss: 0.0022231177426874638\n",
      "step: 8656, loss: 0.001191036426462233\n",
      "step: 8657, loss: 0.012742068618535995\n",
      "step: 8658, loss: 0.0015488198259845376\n",
      "step: 8659, loss: 0.001331627951003611\n",
      "step: 8660, loss: 0.0009397771209478378\n",
      "step: 8661, loss: 0.023882247507572174\n",
      "step: 8662, loss: 0.0008470645407214761\n",
      "step: 8663, loss: 0.002535885898396373\n",
      "step: 8664, loss: 0.000399428594391793\n",
      "step: 8665, loss: 0.012625706382095814\n",
      "step: 8666, loss: 0.004136021714657545\n",
      "step: 8667, loss: 0.0008060731925070286\n",
      "step: 8668, loss: 0.000335158605594188\n",
      "step: 8669, loss: 0.05648264288902283\n",
      "step: 8670, loss: 0.00139044935349375\n",
      "step: 8671, loss: 0.0006182813667692244\n",
      "step: 8672, loss: 0.001083152717910707\n",
      "step: 8673, loss: 0.00037894980050623417\n",
      "step: 8674, loss: 0.0013667064486071467\n",
      "step: 8675, loss: 0.06641295552253723\n",
      "step: 8676, loss: 0.047016631811857224\n",
      "step: 8677, loss: 0.0015624718507751822\n",
      "step: 8678, loss: 0.022922687232494354\n",
      "step: 8679, loss: 0.0018185204826295376\n",
      "step: 8680, loss: 0.00018890775390900671\n",
      "step: 8681, loss: 0.0003708892036229372\n",
      "step: 8682, loss: 0.0005491854972206056\n",
      "step: 8683, loss: 0.0012284222757443786\n",
      "step: 8684, loss: 0.0010236358502879739\n",
      "step: 8685, loss: 0.011137298308312893\n",
      "step: 8686, loss: 0.017704084515571594\n",
      "step: 8687, loss: 0.033634837716817856\n",
      "step: 8688, loss: 0.00045147165656089783\n",
      "step: 8689, loss: 0.0036937869153916836\n",
      "step: 8690, loss: 0.022413037717342377\n",
      "step: 8691, loss: 0.0008902160916477442\n",
      "step: 8692, loss: 0.012194241397082806\n",
      "step: 8693, loss: 0.11023589968681335\n",
      "step: 8694, loss: 0.004863375332206488\n",
      "step: 8695, loss: 0.0023937863297760487\n",
      "step: 8696, loss: 0.005680954083800316\n",
      "step: 8697, loss: 0.0017605553148314357\n",
      "step: 8698, loss: 0.0319923497736454\n",
      "step: 8699, loss: 0.05215369164943695\n",
      "step: 8700, loss: 0.004764757584780455\n",
      "step: 8701, loss: 0.0020336988382041454\n",
      "step: 8702, loss: 0.036810778081417084\n",
      "step: 8703, loss: 0.0005843185354024172\n",
      "step: 8704, loss: 0.000652005837764591\n",
      "step: 8705, loss: 0.0013964694226160645\n",
      "step: 8706, loss: 0.03213498368859291\n",
      "step: 8707, loss: 0.011417180299758911\n",
      "step: 8708, loss: 0.011319812387228012\n",
      "step: 8709, loss: 0.14725247025489807\n",
      "step: 8710, loss: 0.037947941571474075\n",
      "step: 8711, loss: 0.0062375860288739204\n",
      "step: 8712, loss: 0.0007493284065276384\n",
      "step: 8713, loss: 0.0009718831861391664\n",
      "step: 8714, loss: 0.03704860061407089\n",
      "step: 8715, loss: 0.001380800735205412\n",
      "step: 8716, loss: 0.011579075828194618\n",
      "step: 8717, loss: 0.045254532247781754\n",
      "step: 8718, loss: 0.0020659426227211952\n",
      "step: 8719, loss: 0.0016443096101284027\n",
      "step: 8720, loss: 0.0017632875824347138\n",
      "step: 8721, loss: 0.029216116294264793\n",
      "step: 8722, loss: 0.08501015603542328\n",
      "step: 8723, loss: 0.0007270521600730717\n",
      "step: 8724, loss: 0.045998211950063705\n",
      "step: 8725, loss: 0.0029541151598095894\n",
      "step: 8726, loss: 0.0023058655206114054\n",
      "step: 8727, loss: 0.0017693722620606422\n",
      "step: 8728, loss: 0.001055441447533667\n",
      "step: 8729, loss: 0.029076332226395607\n",
      "step: 8730, loss: 0.035213910043239594\n",
      "step: 8731, loss: 0.0016028833342716098\n",
      "step: 8732, loss: 0.0008803145028650761\n",
      "step: 8733, loss: 0.0028478053864091635\n",
      "step: 8734, loss: 0.05741918459534645\n",
      "step: 8735, loss: 0.010586405172944069\n",
      "step: 8736, loss: 0.04642777889966965\n",
      "step: 8737, loss: 0.0003157036262564361\n",
      "step: 8738, loss: 0.002452664077281952\n",
      "step: 8739, loss: 0.044577788561582565\n",
      "step: 8740, loss: 0.0007746237679384649\n",
      "step: 8741, loss: 0.013834979385137558\n",
      "step: 8742, loss: 0.0005264610517770052\n",
      "step: 8743, loss: 0.0005037170485593379\n",
      "step: 8744, loss: 0.08870639652013779\n",
      "step: 8745, loss: 0.025197820737957954\n",
      "step: 8746, loss: 0.002256960840895772\n",
      "step: 8747, loss: 0.0001277999544981867\n",
      "step: 8748, loss: 0.04440503939986229\n",
      "step: 8749, loss: 0.0013063178630545735\n",
      "step: 8750, loss: 0.001947119366377592\n",
      "step: 8751, loss: 0.03843624144792557\n",
      "step: 8752, loss: 0.0005431579775176942\n",
      "step: 8753, loss: 0.002061601495370269\n",
      "step: 8754, loss: 0.04830658808350563\n",
      "step: 8755, loss: 0.002605513436719775\n",
      "step: 8756, loss: 0.020467044785618782\n",
      "step: 8757, loss: 0.002081917831674218\n",
      "step: 8758, loss: 0.0024797169025987387\n",
      "step: 8759, loss: 0.003001709934324026\n",
      "step: 8760, loss: 0.0009102479089051485\n",
      "step: 8761, loss: 0.010211745277047157\n",
      "step: 8762, loss: 0.052790239453315735\n",
      "step: 8763, loss: 0.007938345894217491\n",
      "step: 8764, loss: 0.00080774916568771\n",
      "step: 8765, loss: 0.005405120085924864\n",
      "step: 8766, loss: 0.0010283858282491565\n",
      "step: 8767, loss: 0.003468170063570142\n",
      "step: 8768, loss: 0.04887296259403229\n",
      "step: 8769, loss: 0.047483667731285095\n",
      "step: 8770, loss: 0.01861940138041973\n",
      "step: 8771, loss: 0.001390091609209776\n",
      "step: 8772, loss: 0.0004965693806298077\n",
      "step: 8773, loss: 0.0003440120199229568\n",
      "step: 8774, loss: 0.0018779763486236334\n",
      "step: 8775, loss: 0.01836252026259899\n",
      "step: 8776, loss: 0.001220874022692442\n",
      "step: 8777, loss: 0.0009172172867693007\n",
      "step: 8778, loss: 0.013252811506390572\n",
      "step: 8779, loss: 0.000646760337986052\n",
      "step: 8780, loss: 0.01962297037243843\n",
      "step: 8781, loss: 0.0005054848152212799\n",
      "step: 8782, loss: 0.0010484774829819798\n",
      "step: 8783, loss: 0.016341548413038254\n",
      "step: 8784, loss: 0.013078716583549976\n",
      "step: 8785, loss: 0.0018694075988605618\n",
      "step: 8786, loss: 0.002400017576292157\n",
      "step: 8787, loss: 0.0007656675297766924\n",
      "step: 8788, loss: 0.0008573658997192979\n",
      "step: 8789, loss: 0.003316141664981842\n",
      "step: 8790, loss: 0.0004338662838563323\n",
      "step: 8791, loss: 0.0005652414401993155\n",
      "step: 8792, loss: 0.0018367007141932845\n",
      "step: 8793, loss: 0.011335032992064953\n",
      "step: 8794, loss: 0.025514772161841393\n",
      "step: 8795, loss: 0.0006177898612804711\n",
      "step: 8796, loss: 0.04495743289589882\n",
      "step: 8797, loss: 0.0005629105726256967\n",
      "step: 8798, loss: 0.045490726828575134\n",
      "step: 8799, loss: 0.0005665166536346078\n",
      "step: 8800, loss: 0.0009425410535186529\n",
      "step: 8801, loss: 0.0021385878790169954\n",
      "step: 8802, loss: 0.060217197984457016\n",
      "step: 8803, loss: 0.0019952210132032633\n",
      "step: 8804, loss: 0.0017786455573514104\n",
      "step: 8805, loss: 0.0010825375793501735\n",
      "step: 8806, loss: 0.0008232543477788568\n",
      "step: 8807, loss: 0.0025367361959069967\n",
      "step: 8808, loss: 0.03676441311836243\n",
      "step: 8809, loss: 0.0005980907590128481\n",
      "step: 8810, loss: 0.0003089903329964727\n",
      "step: 8811, loss: 0.15164822340011597\n",
      "step: 8812, loss: 0.0036785078700631857\n",
      "step: 8813, loss: 0.001028201775625348\n",
      "step: 8814, loss: 0.004473187029361725\n",
      "step: 8815, loss: 0.05780813843011856\n",
      "step: 8816, loss: 0.014771275222301483\n",
      "step: 8817, loss: 0.00019992043962702155\n",
      "step: 8818, loss: 0.0005550812929868698\n",
      "step: 8819, loss: 0.0006630689022131264\n",
      "step: 8820, loss: 0.02290000393986702\n",
      "step: 8821, loss: 0.0007223766297101974\n",
      "step: 8822, loss: 0.0007573898183181882\n",
      "step: 8823, loss: 0.038245126605033875\n",
      "step: 8824, loss: 0.0021696467883884907\n",
      "step: 8825, loss: 0.0026320654433220625\n",
      "step: 8826, loss: 0.013193666934967041\n",
      "step: 8827, loss: 0.0005284099606797099\n",
      "step: 8828, loss: 0.0029942437540739775\n",
      "step: 8829, loss: 0.000583799323067069\n",
      "step: 8830, loss: 0.020861797034740448\n",
      "step: 8831, loss: 0.0005950834020040929\n",
      "step: 8832, loss: 0.0007203018758445978\n",
      "step: 8833, loss: 0.003303625388070941\n",
      "step: 8834, loss: 0.006304308772087097\n",
      "step: 8835, loss: 0.011838479898869991\n",
      "step: 8836, loss: 0.01069699414074421\n",
      "step: 8837, loss: 0.0006077563157305121\n",
      "step: 8838, loss: 0.013722855597734451\n",
      "step: 8839, loss: 0.0010135156335309148\n",
      "step: 8840, loss: 0.0141386017203331\n",
      "step: 8841, loss: 0.0011910953326150775\n",
      "step: 8842, loss: 0.0035049894358962774\n",
      "step: 8843, loss: 0.10316770523786545\n",
      "step: 8844, loss: 0.06117968261241913\n",
      "step: 8845, loss: 0.0013210695469751954\n",
      "step: 8846, loss: 0.013126593083143234\n",
      "step: 8847, loss: 0.02040555328130722\n",
      "step: 8848, loss: 0.0020479699596762657\n",
      "step: 8849, loss: 0.003121409798040986\n",
      "step: 8850, loss: 0.03824026882648468\n",
      "step: 8851, loss: 0.0009940494783222675\n",
      "step: 8852, loss: 0.03857509046792984\n",
      "step: 8853, loss: 0.002624337561428547\n",
      "step: 8854, loss: 0.0008540956187061965\n",
      "step: 8855, loss: 0.045119449496269226\n",
      "step: 8856, loss: 0.0005807469715364277\n",
      "step: 8857, loss: 0.0006771121406927705\n",
      "step: 8858, loss: 0.025406159460544586\n",
      "step: 8859, loss: 0.0006221362273208797\n",
      "step: 8860, loss: 0.0004226751334499568\n",
      "step: 8861, loss: 0.0008382402593269944\n",
      "step: 8862, loss: 0.03646514564752579\n",
      "step: 8863, loss: 0.0015148638049140573\n",
      "step: 8864, loss: 0.047846708446741104\n",
      "step: 8865, loss: 0.04654162377119064\n",
      "step: 8866, loss: 0.00899158138781786\n",
      "step: 8867, loss: 0.0005916879163123667\n",
      "step: 8868, loss: 0.008041526190936565\n",
      "step: 8869, loss: 0.0021472256630659103\n",
      "step: 8870, loss: 0.0017357930773869157\n",
      "step: 8871, loss: 0.012010488659143448\n",
      "step: 8872, loss: 0.0006244629039429128\n",
      "step: 8873, loss: 0.0008355838945135474\n",
      "step: 8874, loss: 0.0002245034702355042\n",
      "step: 8875, loss: 0.06200361251831055\n",
      "step: 8876, loss: 0.0006949680391699076\n",
      "step: 8877, loss: 0.00147306639701128\n",
      "step: 8878, loss: 0.004173786845058203\n",
      "step: 8879, loss: 0.0011525873560458422\n",
      "step: 8880, loss: 0.0032209677156060934\n",
      "step: 8881, loss: 0.005428294651210308\n",
      "step: 8882, loss: 0.047501783818006516\n",
      "step: 8883, loss: 0.03694073110818863\n",
      "step: 8884, loss: 0.001244526938535273\n",
      "step: 8885, loss: 0.0008058285457082093\n",
      "step: 8886, loss: 0.0006722917314618826\n",
      "step: 8887, loss: 0.0018053409876301885\n",
      "step: 8888, loss: 0.0006646599504165351\n",
      "step: 8889, loss: 0.010846824385225773\n",
      "step: 8890, loss: 0.0010642597917467356\n",
      "step: 8891, loss: 0.011112697422504425\n",
      "step: 8892, loss: 0.0037631881423294544\n",
      "step: 8893, loss: 0.00015834387158975005\n",
      "step: 8894, loss: 0.012606416828930378\n",
      "step: 8895, loss: 0.04968145489692688\n",
      "step: 8896, loss: 0.0023919048253446817\n",
      "step: 8897, loss: 0.0009747696458362043\n",
      "step: 8898, loss: 0.2274945229291916\n",
      "step: 8899, loss: 0.0035565306898206472\n",
      "step: 8900, loss: 0.00042850515455938876\n",
      "step: 8901, loss: 0.0003727276634890586\n",
      "step: 8902, loss: 0.0011985820019617677\n",
      "step: 8903, loss: 0.00010907224350376055\n",
      "step: 8904, loss: 0.005769961979240179\n",
      "step: 8905, loss: 0.0018251113360747695\n",
      "step: 8906, loss: 0.0019242698326706886\n",
      "step: 8907, loss: 0.001927248085848987\n",
      "step: 8908, loss: 0.0066725038923323154\n",
      "step: 8909, loss: 4.5751068682875484e-05\n",
      "step: 8910, loss: 0.05054576322436333\n",
      "step: 8911, loss: 0.006726048421114683\n",
      "step: 8912, loss: 0.03678339719772339\n",
      "step: 8913, loss: 0.0027592701371759176\n",
      "step: 8914, loss: 0.03246092051267624\n",
      "step: 8915, loss: 0.0003935842833016068\n",
      "step: 8916, loss: 0.0077871959656476974\n",
      "step: 8917, loss: 0.012347079813480377\n",
      "step: 8918, loss: 0.02086811512708664\n",
      "step: 8919, loss: 0.0028180191293358803\n",
      "step: 8920, loss: 0.0029401988722383976\n",
      "step: 8921, loss: 0.0016932502621784806\n",
      "step: 8922, loss: 0.013035466894507408\n",
      "step: 8923, loss: 0.0003945493372157216\n",
      "step: 8924, loss: 0.0014920539688318968\n",
      "step: 8925, loss: 0.004962866194546223\n",
      "step: 8926, loss: 0.0018226606771349907\n",
      "step: 8927, loss: 0.00018351190374232829\n",
      "step: 8928, loss: 0.004033517092466354\n",
      "step: 8929, loss: 0.0016570400912314653\n",
      "step: 8930, loss: 0.014403781853616238\n",
      "step: 8931, loss: 0.010348983108997345\n",
      "step: 8932, loss: 3.979055327363312e-05\n",
      "step: 8933, loss: 0.0003281687095295638\n",
      "step: 8934, loss: 0.004672384355217218\n",
      "step: 8935, loss: 0.0009613822330720723\n",
      "step: 8936, loss: 0.00069069565506652\n",
      "step: 8937, loss: 0.0019677444361150265\n",
      "step: 8938, loss: 0.0009802409913390875\n",
      "step: 8939, loss: 0.011521534062922001\n",
      "step: 8940, loss: 0.015349771827459335\n",
      "step: 8941, loss: 0.0007119486690498888\n",
      "step: 8942, loss: 0.0004538569482974708\n",
      "step: 8943, loss: 0.04364655166864395\n",
      "step: 8944, loss: 0.006313658785074949\n",
      "step: 8945, loss: 0.01402662880718708\n",
      "step: 8946, loss: 0.003168997820466757\n",
      "step: 8947, loss: 0.001216307980939746\n",
      "step: 8948, loss: 0.04258747026324272\n",
      "step: 8949, loss: 0.012422090396285057\n",
      "step: 8950, loss: 0.0013182683615013957\n",
      "step: 8951, loss: 0.0371745228767395\n",
      "step: 8952, loss: 0.020441966131329536\n",
      "step: 8953, loss: 0.08298076689243317\n",
      "step: 8954, loss: 0.05115950107574463\n",
      "step: 8955, loss: 0.03674810752272606\n",
      "step: 8956, loss: 0.0020633451640605927\n",
      "step: 8957, loss: 0.002918934216722846\n",
      "step: 8958, loss: 0.0016331772785633802\n",
      "step: 8959, loss: 0.01457943581044674\n",
      "step: 8960, loss: 0.00032934322371147573\n",
      "step: 8961, loss: 0.002351036062464118\n",
      "step: 8962, loss: 0.002283477457240224\n",
      "step: 8963, loss: 0.0037896919529885054\n",
      "step: 8964, loss: 0.04664678871631622\n",
      "step: 8965, loss: 0.13562807440757751\n",
      "step: 8966, loss: 0.03594563528895378\n",
      "step: 8967, loss: 0.0008879259694367647\n",
      "step: 8968, loss: 0.10166853666305542\n",
      "step: 8969, loss: 0.0003551922272890806\n",
      "step: 8970, loss: 0.006188628729432821\n",
      "step: 8971, loss: 0.0029669443611055613\n",
      "step: 8972, loss: 0.031245209276676178\n",
      "step: 8973, loss: 0.0005904254503548145\n",
      "step: 8974, loss: 0.003657701425254345\n",
      "step: 8975, loss: 0.0011507560266181827\n",
      "step: 8976, loss: 0.0018697296036407351\n",
      "step: 8977, loss: 0.0006717441137880087\n",
      "step: 8978, loss: 0.0008596419356763363\n",
      "step: 8979, loss: 0.011302289552986622\n",
      "step: 8980, loss: 0.0001027950638672337\n",
      "step: 8981, loss: 0.0013341024750843644\n",
      "step: 8982, loss: 0.04644416645169258\n",
      "step: 8983, loss: 0.00471117626875639\n",
      "step: 8984, loss: 0.12273380905389786\n",
      "step: 8985, loss: 0.0200347900390625\n",
      "step: 8986, loss: 0.002071476774290204\n",
      "step: 8987, loss: 0.009663105942308903\n",
      "step: 8988, loss: 0.0014428534777835011\n",
      "step: 8989, loss: 0.0010013732826337218\n",
      "step: 8990, loss: 0.00011481151159387082\n",
      "step: 8991, loss: 0.0004126360872760415\n",
      "step: 8992, loss: 0.0387340784072876\n",
      "step: 8993, loss: 0.0008581724832765758\n",
      "step: 8994, loss: 0.0010876110754907131\n",
      "step: 8995, loss: 0.5682639479637146\n",
      "step: 8996, loss: 0.0009973399573937058\n",
      "step: 8997, loss: 0.052413325756788254\n",
      "step: 8998, loss: 0.0016319763381034136\n",
      "step: 8999, loss: 0.0014564106240868568\n",
      "step: 9000, loss: 0.0005039892857894301\n",
      "step: 9001, loss: 0.04310392215847969\n",
      "step: 9002, loss: 0.011748479679226875\n",
      "step: 9003, loss: 0.0007063661469146609\n",
      "step: 9004, loss: 0.0003612022555898875\n",
      "step: 9005, loss: 0.009902012534439564\n",
      "step: 9006, loss: 0.0006294149206951261\n",
      "step: 9007, loss: 0.06147611513733864\n",
      "step: 9008, loss: 0.0016500249039381742\n",
      "step: 9009, loss: 0.05041291192173958\n",
      "step: 9010, loss: 0.00073018460534513\n",
      "step: 9011, loss: 0.00012262209202162921\n",
      "step: 9012, loss: 0.002526836935430765\n",
      "step: 9013, loss: 0.03361423686146736\n",
      "step: 9014, loss: 0.00013525596295949072\n",
      "step: 9015, loss: 0.015224464237689972\n",
      "step: 9016, loss: 0.10575312376022339\n",
      "step: 9017, loss: 0.002445874037221074\n",
      "step: 9018, loss: 0.0010591630125418305\n",
      "step: 9019, loss: 0.05107923597097397\n",
      "step: 9020, loss: 0.0004111188754905015\n",
      "step: 9021, loss: 0.05730326473712921\n",
      "step: 9022, loss: 0.0030217887833714485\n",
      "step: 9023, loss: 0.001224798383191228\n",
      "step: 9024, loss: 0.0013080138014629483\n",
      "step: 9025, loss: 9.865476022241637e-05\n",
      "step: 9026, loss: 0.022482430562376976\n",
      "step: 9027, loss: 0.0010933601297438145\n",
      "step: 9028, loss: 0.0014609548961743712\n",
      "step: 9029, loss: 0.5772602558135986\n",
      "step: 9030, loss: 0.0008794972090981901\n",
      "step: 9031, loss: 0.012265929952263832\n",
      "step: 9032, loss: 0.03294927254319191\n",
      "step: 9033, loss: 0.00029964084387756884\n",
      "step: 9034, loss: 0.049932584166526794\n",
      "step: 9035, loss: 0.013385889120399952\n",
      "step: 9036, loss: 0.03518245369195938\n",
      "step: 9037, loss: 0.04842405766248703\n",
      "step: 9038, loss: 0.0026039378717541695\n",
      "step: 9039, loss: 0.0008064851281233132\n",
      "step: 9040, loss: 0.0420122891664505\n",
      "step: 9041, loss: 0.038459669798612595\n",
      "step: 9042, loss: 0.0007436255109496415\n",
      "step: 9043, loss: 0.02724437415599823\n",
      "step: 9044, loss: 0.014103183522820473\n",
      "step: 9045, loss: 0.0008788058185018599\n",
      "step: 9046, loss: 0.010970733128488064\n",
      "step: 9047, loss: 0.003163517452776432\n",
      "step: 9048, loss: 0.013244204223155975\n",
      "step: 9049, loss: 0.0010310407960787416\n",
      "step: 9050, loss: 0.05382297933101654\n",
      "step: 9051, loss: 0.013880026526749134\n",
      "step: 9052, loss: 0.06166014075279236\n",
      "step: 9053, loss: 0.062007833272218704\n",
      "step: 9054, loss: 0.001917610876262188\n",
      "step: 9055, loss: 9.166911331703886e-05\n",
      "step: 9056, loss: 0.07124973833560944\n",
      "step: 9057, loss: 0.012317445129156113\n",
      "step: 9058, loss: 0.007422963157296181\n",
      "step: 9059, loss: 0.07961617410182953\n",
      "step: 9060, loss: 0.034659698605537415\n",
      "step: 9061, loss: 0.006194965448230505\n",
      "step: 9062, loss: 0.0014015387278050184\n",
      "step: 9063, loss: 0.0010223467834293842\n",
      "step: 9064, loss: 0.0010224217548966408\n",
      "step: 9065, loss: 0.00090405234368518\n",
      "step: 9066, loss: 0.0010841591283679008\n",
      "step: 9067, loss: 0.0003839931741822511\n",
      "step: 9068, loss: 0.0007166263530962169\n",
      "step: 9069, loss: 0.0008395973709411919\n",
      "step: 9070, loss: 0.04642733931541443\n",
      "step: 9071, loss: 0.05145062133669853\n",
      "step: 9072, loss: 0.09083080291748047\n",
      "step: 9073, loss: 0.002520513255149126\n",
      "step: 9074, loss: 0.00037860768497921526\n",
      "step: 9075, loss: 0.03697625547647476\n",
      "step: 9076, loss: 0.01781625859439373\n",
      "step: 9077, loss: 0.00889922771602869\n",
      "step: 9078, loss: 0.0007983925752341747\n",
      "step: 9079, loss: 0.03672425076365471\n",
      "step: 9080, loss: 0.014721637591719627\n",
      "step: 9081, loss: 0.000554867263417691\n",
      "step: 9082, loss: 0.0042496840469539165\n",
      "step: 9083, loss: 0.001920129987411201\n",
      "step: 9084, loss: 0.0023294277489185333\n",
      "step: 9085, loss: 0.00026438201894052327\n",
      "step: 9086, loss: 9.353282075608149e-05\n",
      "step: 9087, loss: 0.0019153062021359801\n",
      "step: 9088, loss: 0.007391355466097593\n",
      "step: 9089, loss: 0.0022803463507443666\n",
      "step: 9090, loss: 0.0012840923154726624\n",
      "step: 9091, loss: 0.009087599813938141\n",
      "step: 9092, loss: 7.830293907318264e-05\n",
      "step: 9093, loss: 0.0038644252344965935\n",
      "step: 9094, loss: 0.002206320408731699\n",
      "step: 9095, loss: 0.0006636939942836761\n",
      "step: 9096, loss: 0.044491685926914215\n",
      "step: 9097, loss: 0.012124082073569298\n",
      "step: 9098, loss: 0.003627176396548748\n",
      "step: 9099, loss: 0.001425290247425437\n",
      "step: 9100, loss: 0.00044500682270154357\n",
      "step: 9101, loss: 0.010422524996101856\n",
      "step: 9102, loss: 0.02869783155620098\n",
      "step: 9103, loss: 0.0024944397155195475\n",
      "step: 9104, loss: 0.05740879103541374\n",
      "step: 9105, loss: 0.0007387048681266606\n",
      "step: 9106, loss: 0.009932423010468483\n",
      "step: 9107, loss: 0.0005672945990227163\n",
      "step: 9108, loss: 0.0022005161736160517\n",
      "step: 9109, loss: 0.010119272395968437\n",
      "step: 9110, loss: 0.011312601156532764\n",
      "step: 9111, loss: 0.001314943190664053\n",
      "step: 9112, loss: 0.00128113420214504\n",
      "step: 9113, loss: 0.0005760224303230643\n",
      "step: 9114, loss: 0.013643993996083736\n",
      "step: 9115, loss: 0.0008103945874609053\n",
      "step: 9116, loss: 0.0023619532585144043\n",
      "step: 9117, loss: 0.010979766957461834\n",
      "step: 9118, loss: 0.015305996872484684\n",
      "step: 9119, loss: 0.01114711444824934\n",
      "step: 9120, loss: 0.033596914261579514\n",
      "step: 9121, loss: 0.001641405513510108\n",
      "step: 9122, loss: 0.0011514445068314672\n",
      "step: 9123, loss: 0.002171160187572241\n",
      "step: 9124, loss: 0.001700323075056076\n",
      "step: 9125, loss: 0.012480262666940689\n",
      "step: 9126, loss: 0.03647663816809654\n",
      "step: 9127, loss: 0.0010566564742475748\n",
      "step: 9128, loss: 0.009291409514844418\n",
      "step: 9129, loss: 0.0006949888193048537\n",
      "step: 9130, loss: 0.0015118068549782038\n",
      "step: 9131, loss: 0.002758009359240532\n",
      "step: 9132, loss: 0.0006347881280817091\n",
      "step: 9133, loss: 0.000935896357987076\n",
      "step: 9134, loss: 0.0022848874796181917\n",
      "step: 9135, loss: 0.060640908777713776\n",
      "step: 9136, loss: 0.0013147764839231968\n",
      "step: 9137, loss: 0.001591041567735374\n",
      "step: 9138, loss: 0.07040935754776001\n",
      "step: 9139, loss: 0.03314124420285225\n",
      "step: 9140, loss: 0.06493969261646271\n",
      "step: 9141, loss: 0.0019987737759947777\n",
      "step: 9142, loss: 0.0005125280003994703\n",
      "step: 9143, loss: 0.0010280146962031722\n",
      "step: 9144, loss: 0.00030245594098232687\n",
      "step: 9145, loss: 0.15519005060195923\n",
      "step: 9146, loss: 0.000934696989133954\n",
      "step: 9147, loss: 0.0008574529201723635\n",
      "step: 9148, loss: 0.003451216034591198\n",
      "step: 9149, loss: 0.008009647950530052\n",
      "step: 9150, loss: 0.00383859034627676\n",
      "step: 9151, loss: 0.01255787257105112\n",
      "step: 9152, loss: 0.0027791548054665327\n",
      "step: 9153, loss: 0.00011960003757849336\n",
      "step: 9154, loss: 0.00760970963165164\n",
      "step: 9155, loss: 0.0018950467929244041\n",
      "step: 9156, loss: 0.0006355762016028166\n",
      "step: 9157, loss: 0.0005718679167330265\n",
      "step: 9158, loss: 0.0010211149929091334\n",
      "step: 9159, loss: 0.0024976793210953474\n",
      "step: 9160, loss: 0.014888989739120007\n",
      "step: 9161, loss: 0.0017535663209855556\n",
      "step: 9162, loss: 0.0005605272599495947\n",
      "step: 9163, loss: 0.00918334349989891\n",
      "step: 9164, loss: 0.009696689434349537\n",
      "step: 9165, loss: 0.002195260487496853\n",
      "step: 9166, loss: 0.03812200948596001\n",
      "step: 9167, loss: 0.000852367258630693\n",
      "step: 9168, loss: 0.003509768284857273\n",
      "step: 9169, loss: 0.08036806434392929\n",
      "step: 9170, loss: 0.0006188537226989865\n",
      "step: 9171, loss: 0.0008758569601923227\n",
      "step: 9172, loss: 0.0014523521531373262\n",
      "step: 9173, loss: 0.0525006428360939\n",
      "step: 9174, loss: 0.0009490916854701936\n",
      "step: 9175, loss: 0.001382306800223887\n",
      "step: 9176, loss: 0.00215992727316916\n",
      "step: 9177, loss: 0.001128630479797721\n",
      "step: 9178, loss: 0.0008855706546455622\n",
      "step: 9179, loss: 0.0004795564746018499\n",
      "step: 9180, loss: 0.00115663290489465\n",
      "step: 9181, loss: 0.0044719018042087555\n",
      "step: 9182, loss: 0.005497067701071501\n",
      "step: 9183, loss: 0.03775501623749733\n",
      "step: 9184, loss: 0.004616361111402512\n",
      "step: 9185, loss: 0.0013819298474118114\n",
      "step: 9186, loss: 0.0005786357214674354\n",
      "step: 9187, loss: 0.000369208340998739\n",
      "step: 9188, loss: 0.0026463707908988\n",
      "step: 9189, loss: 0.00043348304461687803\n",
      "step: 9190, loss: 0.00013251224299892783\n",
      "step: 9191, loss: 0.0008034839993342757\n",
      "step: 9192, loss: 0.0008020324748940766\n",
      "step: 9193, loss: 0.0007029885309748352\n",
      "step: 9194, loss: 0.0005006907740607858\n",
      "step: 9195, loss: 0.0029907666612416506\n",
      "step: 9196, loss: 0.00040822281152941287\n",
      "step: 9197, loss: 0.0015372440684586763\n",
      "step: 9198, loss: 0.011461833491921425\n",
      "step: 9199, loss: 0.0030172427650541067\n",
      "step: 9200, loss: 0.0014981572749093175\n",
      "step: 9201, loss: 0.00021005939925089478\n",
      "step: 9202, loss: 0.01246580109000206\n",
      "step: 9203, loss: 0.04618382453918457\n",
      "step: 9204, loss: 0.0010462542995810509\n",
      "step: 9205, loss: 0.017469869926571846\n",
      "step: 9206, loss: 0.002358402591198683\n",
      "step: 9207, loss: 0.047460027039051056\n",
      "step: 9208, loss: 0.0010850931284949183\n",
      "step: 9209, loss: 0.00044977941433899105\n",
      "step: 9210, loss: 0.0008483360870741308\n",
      "step: 9211, loss: 0.06708338856697083\n",
      "step: 9212, loss: 0.0008588269120082259\n",
      "step: 9213, loss: 0.0007142858812585473\n",
      "step: 9214, loss: 0.0014299876056611538\n",
      "step: 9215, loss: 0.0012453689705580473\n",
      "step: 9216, loss: 0.0024144547060132027\n",
      "step: 9217, loss: 0.0006674545002169907\n",
      "step: 9218, loss: 0.06816626340150833\n",
      "step: 9219, loss: 0.0005978498375043273\n",
      "step: 9220, loss: 0.04757106676697731\n",
      "step: 9221, loss: 0.0010040190536528826\n",
      "step: 9222, loss: 0.04800897091627121\n",
      "step: 9223, loss: 0.03594981133937836\n",
      "step: 9224, loss: 0.03942684829235077\n",
      "step: 9225, loss: 0.03613601624965668\n",
      "step: 9226, loss: 0.015242819674313068\n",
      "step: 9227, loss: 0.0008973457152023911\n",
      "step: 9228, loss: 0.056594591587781906\n",
      "step: 9229, loss: 0.0009882759768515825\n",
      "step: 9230, loss: 0.0006389202899299562\n",
      "step: 9231, loss: 0.002015206962823868\n",
      "step: 9232, loss: 0.0006395077798515558\n",
      "step: 9233, loss: 0.032364677637815475\n",
      "step: 9234, loss: 0.04064164683222771\n",
      "step: 9235, loss: 0.01941574364900589\n",
      "step: 9236, loss: 0.013897218741476536\n",
      "step: 9237, loss: 0.008538594469428062\n",
      "step: 9238, loss: 0.0002486242156010121\n",
      "step: 9239, loss: 0.000966192688792944\n",
      "step: 9240, loss: 0.00123864714987576\n",
      "step: 9241, loss: 0.0007121692760847509\n",
      "step: 9242, loss: 0.0007897453033365309\n",
      "step: 9243, loss: 0.0014628152130171657\n",
      "step: 9244, loss: 0.0004745289625134319\n",
      "step: 9245, loss: 0.0017786544049158692\n",
      "step: 9246, loss: 0.01880660280585289\n",
      "step: 9247, loss: 0.0009355248766951263\n",
      "step: 9248, loss: 0.012404012493789196\n",
      "step: 9249, loss: 0.03233309090137482\n",
      "step: 9250, loss: 0.001037011155858636\n",
      "step: 9251, loss: 0.0460081584751606\n",
      "step: 9252, loss: 0.04737215116620064\n",
      "step: 9253, loss: 0.033646468073129654\n",
      "step: 9254, loss: 0.0007342012831941247\n",
      "step: 9255, loss: 0.03137568384408951\n",
      "step: 9256, loss: 0.0005959912668913603\n",
      "step: 9257, loss: 0.004668813198804855\n",
      "step: 9258, loss: 0.05128960683941841\n",
      "step: 9259, loss: 0.0015570569084957242\n",
      "step: 9260, loss: 0.011479634791612625\n",
      "step: 9261, loss: 0.05857574939727783\n",
      "step: 9262, loss: 0.0005259296158328652\n",
      "step: 9263, loss: 0.0006945141358301044\n",
      "step: 9264, loss: 0.003968948498368263\n",
      "step: 9265, loss: 0.0019781284499913454\n",
      "step: 9266, loss: 0.03054722771048546\n",
      "step: 9267, loss: 0.0009182955836877227\n",
      "step: 9268, loss: 0.001222890568897128\n",
      "step: 9269, loss: 0.03749297931790352\n",
      "step: 9270, loss: 0.0006566162337549031\n",
      "step: 9271, loss: 0.0031544032972306013\n",
      "step: 9272, loss: 0.0006153600406832993\n",
      "step: 9273, loss: 0.0007597514195367694\n",
      "step: 9274, loss: 0.001346398494206369\n",
      "step: 9275, loss: 0.0026565492153167725\n",
      "step: 9276, loss: 0.0025401345919817686\n",
      "step: 9277, loss: 0.011026263236999512\n",
      "step: 9278, loss: 0.04872071370482445\n",
      "step: 9279, loss: 0.0009171885321848094\n",
      "step: 9280, loss: 0.0005470403120853007\n",
      "step: 9281, loss: 0.045373477041721344\n",
      "step: 9282, loss: 0.004183254670351744\n",
      "step: 9283, loss: 0.04761580377817154\n",
      "step: 9284, loss: 0.0014519229298457503\n",
      "step: 9285, loss: 0.0010178599040955305\n",
      "step: 9286, loss: 0.001725502428598702\n",
      "step: 9287, loss: 0.0020312159322202206\n",
      "step: 9288, loss: 0.0007016989402472973\n",
      "step: 9289, loss: 0.010193828493356705\n",
      "step: 9290, loss: 0.00035960518289357424\n",
      "step: 9291, loss: 0.034102775156497955\n",
      "step: 9292, loss: 0.033847466111183167\n",
      "step: 9293, loss: 0.019111191853880882\n",
      "step: 9294, loss: 0.1454210728406906\n",
      "step: 9295, loss: 0.02864822931587696\n",
      "step: 9296, loss: 0.000278074003290385\n",
      "step: 9297, loss: 0.001312594278715551\n",
      "step: 9298, loss: 0.0116860531270504\n",
      "step: 9299, loss: 0.0005757424514740705\n",
      "step: 9300, loss: 0.0009577494929544628\n",
      "step: 9301, loss: 0.0014557498507201672\n",
      "step: 9302, loss: 0.0010281942086294293\n",
      "step: 9303, loss: 0.04363516718149185\n",
      "step: 9304, loss: 0.000774033076595515\n",
      "step: 9305, loss: 0.0009434729581698775\n",
      "step: 9306, loss: 0.0007266651373356581\n",
      "step: 9307, loss: 0.01906444877386093\n",
      "step: 9308, loss: 0.0008130486239679158\n",
      "step: 9309, loss: 0.0005968168843537569\n",
      "step: 9310, loss: 0.0018668301636353135\n",
      "step: 9311, loss: 0.000729998922906816\n",
      "step: 9312, loss: 0.06526344269514084\n",
      "step: 9313, loss: 0.0014666533097624779\n",
      "step: 9314, loss: 0.0009822987485677004\n",
      "step: 9315, loss: 0.0007648315513506532\n",
      "step: 9316, loss: 0.035348862409591675\n",
      "step: 9317, loss: 0.004583758767694235\n",
      "step: 9318, loss: 0.009052183479070663\n",
      "step: 9319, loss: 0.05561501905322075\n",
      "step: 9320, loss: 0.0016590330051258206\n",
      "step: 9321, loss: 0.0017753900028765202\n",
      "step: 9322, loss: 0.028676221147179604\n",
      "step: 9323, loss: 0.009791422635316849\n",
      "step: 9324, loss: 0.000745380122680217\n",
      "step: 9325, loss: 0.00016290400526486337\n",
      "step: 9326, loss: 0.03524772822856903\n",
      "step: 9327, loss: 0.0104784294962883\n",
      "step: 9328, loss: 0.0024242873769253492\n",
      "step: 9329, loss: 0.012064571492373943\n",
      "step: 9330, loss: 0.00034016542485915124\n",
      "step: 9331, loss: 0.001058533089235425\n",
      "step: 9332, loss: 0.00013013966963626444\n",
      "step: 9333, loss: 0.00046761249541305006\n",
      "step: 9334, loss: 0.003443766152486205\n",
      "step: 9335, loss: 0.0016811704263091087\n",
      "step: 9336, loss: 0.0018938998691737652\n",
      "step: 9337, loss: 0.012976494617760181\n",
      "step: 9338, loss: 0.00034491473343223333\n",
      "step: 9339, loss: 0.036804575473070145\n",
      "step: 9340, loss: 0.000673461880069226\n",
      "step: 9341, loss: 0.0006915928679518402\n",
      "step: 9342, loss: 0.00034940949990414083\n",
      "step: 9343, loss: 0.006831690203398466\n",
      "step: 9344, loss: 0.015792150050401688\n",
      "step: 9345, loss: 0.0013539422070607543\n",
      "step: 9346, loss: 0.001037783920764923\n",
      "step: 9347, loss: 0.036775797605514526\n",
      "step: 9348, loss: 0.0006293585756793618\n",
      "step: 9349, loss: 0.0006223650416359305\n",
      "step: 9350, loss: 0.017034709453582764\n",
      "step: 9351, loss: 0.001156369922682643\n",
      "step: 9352, loss: 0.0003851618675980717\n",
      "step: 9353, loss: 0.02668026275932789\n",
      "step: 9354, loss: 0.0008547042380087078\n",
      "step: 9355, loss: 0.0016422388143837452\n",
      "step: 9356, loss: 0.0008540985872969031\n",
      "step: 9357, loss: 0.004423154518008232\n",
      "step: 9358, loss: 0.0006063930923119187\n",
      "step: 9359, loss: 0.00564254866912961\n",
      "step: 9360, loss: 0.00048271872219629586\n",
      "step: 9361, loss: 0.0009580941405147314\n",
      "step: 9362, loss: 0.00318304100073874\n",
      "step: 9363, loss: 0.0024333179462701082\n",
      "step: 9364, loss: 0.03843582049012184\n",
      "step: 9365, loss: 0.000738539092708379\n",
      "step: 9366, loss: 0.033241089433431625\n",
      "step: 9367, loss: 0.00558133190497756\n",
      "step: 9368, loss: 0.14611279964447021\n",
      "step: 9369, loss: 6.363081047311425e-05\n",
      "step: 9370, loss: 0.0001684488233877346\n",
      "step: 9371, loss: 0.039191633462905884\n",
      "step: 9372, loss: 0.0013315503019839525\n",
      "step: 9373, loss: 0.00424824608489871\n",
      "step: 9374, loss: 0.03248106315732002\n",
      "step: 9375, loss: 0.0005431430181488395\n",
      "step: 9376, loss: 0.009455784223973751\n",
      "step: 9377, loss: 0.022842975333333015\n",
      "step: 9378, loss: 0.012860452756285667\n",
      "step: 9379, loss: 0.001263835234567523\n",
      "step: 9380, loss: 0.0015917341224849224\n",
      "step: 9381, loss: 0.0030152976978570223\n",
      "step: 9382, loss: 0.0006925472407601774\n",
      "step: 9383, loss: 0.0013545568799600005\n",
      "step: 9384, loss: 0.00028364741592667997\n",
      "step: 9385, loss: 0.06183169409632683\n",
      "step: 9386, loss: 0.003356024855747819\n",
      "step: 9387, loss: 0.00863031204789877\n",
      "step: 9388, loss: 0.001238322351127863\n",
      "step: 9389, loss: 0.00018001641728915274\n",
      "step: 9390, loss: 0.011697796173393726\n",
      "step: 9391, loss: 0.0004406467778608203\n",
      "step: 9392, loss: 0.01096117403358221\n",
      "step: 9393, loss: 0.02917858399450779\n",
      "step: 9394, loss: 0.008958986029028893\n",
      "step: 9395, loss: 0.0005262807244434953\n",
      "step: 9396, loss: 0.00040746681042946875\n",
      "step: 9397, loss: 0.012091359123587608\n",
      "step: 9398, loss: 0.00034436918213032186\n",
      "step: 9399, loss: 0.012194079346954823\n",
      "step: 9400, loss: 0.08868958801031113\n",
      "step: 9401, loss: 0.00018766334687825292\n",
      "step: 9402, loss: 0.000787111057434231\n",
      "step: 9403, loss: 0.0008041886030696332\n",
      "step: 9404, loss: 0.0017694809939712286\n",
      "step: 9405, loss: 0.008778461255133152\n",
      "step: 9406, loss: 0.005298533011227846\n",
      "step: 9407, loss: 0.03697190433740616\n",
      "step: 9408, loss: 0.019133780151605606\n",
      "step: 9409, loss: 0.001975675579160452\n",
      "step: 9410, loss: 0.011527961120009422\n",
      "step: 9411, loss: 0.0003491286188364029\n",
      "step: 9412, loss: 0.0660686045885086\n",
      "step: 9413, loss: 0.0008374442695640028\n",
      "step: 9414, loss: 0.0017028728034347296\n",
      "step: 9415, loss: 0.012976300902664661\n",
      "step: 9416, loss: 0.011861861683428288\n",
      "step: 9417, loss: 0.0006958681624382734\n",
      "step: 9418, loss: 0.005267511121928692\n",
      "step: 9419, loss: 0.008789440616965294\n",
      "step: 9420, loss: 0.02813836559653282\n",
      "step: 9421, loss: 0.0004054651071783155\n",
      "step: 9422, loss: 0.0010343958856537938\n",
      "step: 9423, loss: 0.005122484173625708\n",
      "step: 9424, loss: 0.0015922148013487458\n",
      "step: 9425, loss: 0.008099001832306385\n",
      "step: 9426, loss: 0.0017919964157044888\n",
      "step: 9427, loss: 0.00336024584248662\n",
      "step: 9428, loss: 0.0005824754480272532\n",
      "step: 9429, loss: 0.03502502664923668\n",
      "step: 9430, loss: 0.049620114266872406\n",
      "step: 9431, loss: 0.0008544104057364166\n",
      "step: 9432, loss: 0.00011543644359335303\n",
      "step: 9433, loss: 0.04766508564352989\n",
      "step: 9434, loss: 0.03746268153190613\n",
      "step: 9435, loss: 0.00997294019907713\n",
      "step: 9436, loss: 0.0003543161728885025\n",
      "step: 9437, loss: 0.0006995422882027924\n",
      "step: 9438, loss: 0.0007452922873198986\n",
      "step: 9439, loss: 0.05963025614619255\n",
      "step: 9440, loss: 0.000410888169426471\n",
      "step: 9441, loss: 0.014234235510230064\n",
      "step: 9442, loss: 0.00014036337961442769\n",
      "step: 9443, loss: 0.006248995661735535\n",
      "step: 9444, loss: 0.047434259206056595\n",
      "step: 9445, loss: 0.01534795481711626\n",
      "step: 9446, loss: 0.0016426604706794024\n",
      "step: 9447, loss: 0.020747574046254158\n",
      "step: 9448, loss: 0.0019996340852230787\n",
      "step: 9449, loss: 0.059766218066215515\n",
      "step: 9450, loss: 0.03877696394920349\n",
      "step: 9451, loss: 0.0031569453421980143\n",
      "step: 9452, loss: 0.0028768733609467745\n",
      "step: 9453, loss: 0.0011544984299689531\n",
      "step: 9454, loss: 0.000394591101212427\n",
      "step: 9455, loss: 0.0004182105476502329\n",
      "step: 9456, loss: 0.03631874918937683\n",
      "step: 9457, loss: 0.048100780695676804\n",
      "step: 9458, loss: 0.0008808454731479287\n",
      "step: 9459, loss: 0.04107671231031418\n",
      "step: 9460, loss: 0.004391427151858807\n",
      "step: 9461, loss: 0.06644503772258759\n",
      "step: 9462, loss: 0.001640493399463594\n",
      "step: 9463, loss: 0.08392775803804398\n",
      "step: 9464, loss: 0.0010049493284896016\n",
      "step: 9465, loss: 0.0038579830434173346\n",
      "step: 9466, loss: 0.016851741820573807\n",
      "step: 9467, loss: 0.00013999262591823936\n",
      "step: 9468, loss: 0.0021898255217820406\n",
      "step: 9469, loss: 0.011130106635391712\n",
      "step: 9470, loss: 0.06170803681015968\n",
      "step: 9471, loss: 0.00032501539681106806\n",
      "step: 9472, loss: 0.004122224636375904\n",
      "step: 9473, loss: 0.0018636247841641307\n",
      "step: 9474, loss: 0.005029859486967325\n",
      "step: 9475, loss: 0.002747883088886738\n",
      "step: 9476, loss: 0.003928241785615683\n",
      "step: 9477, loss: 0.01300259679555893\n",
      "step: 9478, loss: 0.15220919251441956\n",
      "step: 9479, loss: 0.014749819412827492\n",
      "step: 9480, loss: 0.0007582959369756281\n",
      "step: 9481, loss: 0.012351262383162975\n",
      "step: 9482, loss: 0.017983179539442062\n",
      "step: 9483, loss: 0.06798107177019119\n",
      "step: 9484, loss: 0.0013733134837821126\n",
      "step: 9485, loss: 0.055075809359550476\n",
      "step: 9486, loss: 0.0007013922440819442\n",
      "step: 9487, loss: 0.0007802479085512459\n",
      "step: 9488, loss: 0.009781965985894203\n",
      "step: 9489, loss: 0.007919734343886375\n",
      "step: 9490, loss: 0.03537362441420555\n",
      "step: 9491, loss: 0.0035129713360220194\n",
      "step: 9492, loss: 0.000492843973916024\n",
      "step: 9493, loss: 0.0014444718835875392\n",
      "step: 9494, loss: 0.0003816416719928384\n",
      "step: 9495, loss: 0.0017999877454712987\n",
      "step: 9496, loss: 9.623288497095928e-05\n",
      "step: 9497, loss: 0.048433735966682434\n",
      "step: 9498, loss: 0.0008007894502952695\n",
      "step: 9499, loss: 0.001231780624948442\n",
      "step: 9500, loss: 0.0014659379376098514\n",
      "step: 9501, loss: 0.0015209164703264832\n",
      "step: 9502, loss: 0.04699518531560898\n",
      "step: 9503, loss: 0.0006091598188504577\n",
      "step: 9504, loss: 0.004978541284799576\n",
      "step: 9505, loss: 0.00050501519581303\n",
      "step: 9506, loss: 0.0006992089911364019\n",
      "step: 9507, loss: 0.002466428792104125\n",
      "step: 9508, loss: 0.037820421159267426\n",
      "step: 9509, loss: 0.001178816077299416\n",
      "step: 9510, loss: 0.05789652839303017\n",
      "step: 9511, loss: 0.0029317873995751143\n",
      "step: 9512, loss: 0.0005520184058696032\n",
      "step: 9513, loss: 0.040651872754096985\n",
      "step: 9514, loss: 0.0003022273303940892\n",
      "step: 9515, loss: 0.06729063391685486\n",
      "step: 9516, loss: 0.00036006010486744344\n",
      "step: 9517, loss: 0.0004058547201566398\n",
      "step: 9518, loss: 0.002929029520601034\n",
      "step: 9519, loss: 0.011834421195089817\n",
      "step: 9520, loss: 0.000921345897950232\n",
      "step: 9521, loss: 0.01146268006414175\n",
      "step: 9522, loss: 0.030083445832133293\n",
      "step: 9523, loss: 0.0003805546439252794\n",
      "step: 9524, loss: 0.03595392778515816\n",
      "step: 9525, loss: 0.0016102056251838803\n",
      "step: 9526, loss: 0.00037374196108430624\n",
      "step: 9527, loss: 0.04400051012635231\n",
      "step: 9528, loss: 0.01197863183915615\n",
      "step: 9529, loss: 8.572328806621954e-05\n",
      "step: 9530, loss: 0.0017009400762617588\n",
      "step: 9531, loss: 0.013242526911199093\n",
      "step: 9532, loss: 0.001967956777662039\n",
      "step: 9533, loss: 0.002734640846028924\n",
      "step: 9534, loss: 0.0009986734949052334\n",
      "step: 9535, loss: 0.0027365691494196653\n",
      "step: 9536, loss: 0.032642725855112076\n",
      "step: 9537, loss: 0.0013878783211112022\n",
      "step: 9538, loss: 0.049284644424915314\n",
      "step: 9539, loss: 0.0005820245714858174\n",
      "step: 9540, loss: 0.06192881613969803\n",
      "step: 9541, loss: 0.038508668541908264\n",
      "step: 9542, loss: 0.06396995484828949\n",
      "step: 9543, loss: 0.04343169555068016\n",
      "step: 9544, loss: 0.005349273793399334\n",
      "step: 9545, loss: 0.0007423297502100468\n",
      "step: 9546, loss: 0.04433536157011986\n",
      "step: 9547, loss: 0.0004960591904819012\n",
      "step: 9548, loss: 0.001745713409036398\n",
      "step: 9549, loss: 0.0028951966669410467\n",
      "step: 9550, loss: 0.021064136177301407\n",
      "step: 9551, loss: 0.011099811643362045\n",
      "step: 9552, loss: 0.0001616628433112055\n",
      "step: 9553, loss: 0.000781954440753907\n",
      "step: 9554, loss: 0.0006665343535132706\n",
      "step: 9555, loss: 0.05044163763523102\n",
      "step: 9556, loss: 0.0032132244668900967\n",
      "step: 9557, loss: 0.0025945689994841814\n",
      "step: 9558, loss: 0.00544940959662199\n",
      "step: 9559, loss: 0.0023910156451165676\n",
      "step: 9560, loss: 0.0015402170829474926\n",
      "step: 9561, loss: 0.04332490265369415\n",
      "step: 9562, loss: 0.0022672631312161684\n",
      "step: 9563, loss: 0.0006189458654262125\n",
      "step: 9564, loss: 0.03829645738005638\n",
      "step: 9565, loss: 0.01289907842874527\n",
      "step: 9566, loss: 0.0024557209108024836\n",
      "step: 9567, loss: 0.0009236487094312906\n",
      "step: 9568, loss: 0.024090196937322617\n",
      "step: 9569, loss: 0.010459916666150093\n",
      "step: 9570, loss: 0.0017751563573256135\n",
      "step: 9571, loss: 0.041263408958911896\n",
      "step: 9572, loss: 0.01858256757259369\n",
      "step: 9573, loss: 0.04915697127580643\n",
      "step: 9574, loss: 0.000945637293625623\n",
      "step: 9575, loss: 0.000612583477050066\n",
      "step: 9576, loss: 0.05318530648946762\n",
      "step: 9577, loss: 0.033174555748701096\n",
      "step: 9578, loss: 0.03160204365849495\n",
      "step: 9579, loss: 0.00011933338100789115\n",
      "step: 9580, loss: 0.0009220753563567996\n",
      "step: 9581, loss: 0.011464492417871952\n",
      "step: 9582, loss: 0.0001559615193400532\n",
      "step: 9583, loss: 0.0010449148248881102\n",
      "step: 9584, loss: 0.011183497495949268\n",
      "step: 9585, loss: 0.019675325602293015\n",
      "step: 9586, loss: 0.0018465030007064342\n",
      "step: 9587, loss: 0.03281117230653763\n",
      "step: 9588, loss: 0.0035524731501936913\n",
      "step: 9589, loss: 0.05790315568447113\n",
      "step: 9590, loss: 0.0007582250982522964\n",
      "step: 9591, loss: 0.011475436389446259\n",
      "step: 9592, loss: 0.030119681730866432\n",
      "step: 9593, loss: 0.000663912738673389\n",
      "step: 9594, loss: 0.009505114518105984\n",
      "step: 9595, loss: 0.00019183709810022265\n",
      "step: 9596, loss: 0.0069874865002930164\n",
      "step: 9597, loss: 0.003941109869629145\n",
      "step: 9598, loss: 0.02594165876507759\n",
      "step: 9599, loss: 0.0003166596288792789\n",
      "step: 9600, loss: 0.0005878355586901307\n",
      "step: 9601, loss: 0.0008950871415436268\n",
      "step: 9602, loss: 0.0016720518469810486\n",
      "step: 9603, loss: 0.013534142635762691\n",
      "step: 9604, loss: 0.012806863524019718\n",
      "step: 9605, loss: 0.010624677874147892\n",
      "step: 9606, loss: 0.005734196864068508\n",
      "step: 9607, loss: 0.0016600105445832014\n",
      "step: 9608, loss: 0.0022845377679914236\n",
      "step: 9609, loss: 0.010126125067472458\n",
      "step: 9610, loss: 0.0007701129652559757\n",
      "step: 9611, loss: 0.012039048597216606\n",
      "step: 9612, loss: 0.0003595526795834303\n",
      "step: 9613, loss: 0.00030513005913235247\n",
      "step: 9614, loss: 0.0027520456351339817\n",
      "step: 9615, loss: 7.777528662700206e-05\n",
      "step: 9616, loss: 4.364036794868298e-05\n",
      "step: 9617, loss: 0.0014440525555983186\n",
      "step: 9618, loss: 0.0003924448392353952\n",
      "step: 9619, loss: 0.04741108417510986\n",
      "step: 9620, loss: 0.0006337992381304502\n",
      "step: 9621, loss: 0.0006780856638215482\n",
      "step: 9622, loss: 0.0370662696659565\n",
      "step: 9623, loss: 0.0007507555419579148\n",
      "step: 9624, loss: 0.0005940869450569153\n",
      "step: 9625, loss: 0.010517144575715065\n",
      "step: 9626, loss: 0.0007673082291148603\n",
      "step: 9627, loss: 0.0369628369808197\n",
      "step: 9628, loss: 0.06088314205408096\n",
      "step: 9629, loss: 0.0028578448109328747\n",
      "step: 9630, loss: 0.0016876172740012407\n",
      "step: 9631, loss: 0.01015494205057621\n",
      "step: 9632, loss: 0.0014399195788428187\n",
      "step: 9633, loss: 0.0009824334410950541\n",
      "step: 9634, loss: 0.010136417113244534\n",
      "step: 9635, loss: 0.018870612606406212\n",
      "step: 9636, loss: 0.0012606621021404862\n",
      "step: 9637, loss: 0.002112829126417637\n",
      "step: 9638, loss: 0.0010779217118397355\n",
      "step: 9639, loss: 8.014782360987738e-05\n",
      "step: 9640, loss: 0.0010354534024372697\n",
      "step: 9641, loss: 0.0007473204750567675\n",
      "step: 9642, loss: 0.06338229030370712\n",
      "step: 9643, loss: 0.0006747015286237001\n",
      "step: 9644, loss: 0.02478257566690445\n",
      "step: 9645, loss: 0.0009117767331190407\n",
      "step: 9646, loss: 0.0007869159453548491\n",
      "step: 9647, loss: 0.0004499936767388135\n",
      "step: 9648, loss: 0.006221136078238487\n",
      "step: 9649, loss: 0.015227862633764744\n",
      "step: 9650, loss: 0.0019093375885859132\n",
      "step: 9651, loss: 0.0018389925826340914\n",
      "step: 9652, loss: 0.0004613989149220288\n",
      "step: 9653, loss: 9.470851364312693e-05\n",
      "step: 9654, loss: 0.0012632905272766948\n",
      "step: 9655, loss: 0.050050243735313416\n",
      "step: 9656, loss: 0.000588232243899256\n",
      "step: 9657, loss: 0.0006999195320531726\n",
      "step: 9658, loss: 0.000888126261997968\n",
      "step: 9659, loss: 0.0023581087589263916\n",
      "step: 9660, loss: 0.03510265424847603\n",
      "step: 9661, loss: 0.0009355531656183302\n",
      "step: 9662, loss: 0.0008381333900615573\n",
      "step: 9663, loss: 0.0003255356859881431\n",
      "step: 9664, loss: 0.08198312669992447\n",
      "step: 9665, loss: 0.003268786007538438\n",
      "step: 9666, loss: 0.002260117791593075\n",
      "step: 9667, loss: 0.001201512641273439\n",
      "step: 9668, loss: 0.0024821229744702578\n",
      "step: 9669, loss: 0.003891823347657919\n",
      "step: 9670, loss: 0.0014348977711051702\n",
      "step: 9671, loss: 0.03935302048921585\n",
      "step: 9672, loss: 0.15935613214969635\n",
      "step: 9673, loss: 0.00039588010986335576\n",
      "step: 9674, loss: 0.0019069755217060447\n",
      "step: 9675, loss: 0.0022665541619062424\n",
      "step: 9676, loss: 0.021587662398815155\n",
      "step: 9677, loss: 0.0009692258900031447\n",
      "step: 9678, loss: 0.0012944038026034832\n",
      "step: 9679, loss: 0.0062437504529953\n",
      "step: 9680, loss: 0.004716999363154173\n",
      "step: 9681, loss: 0.016432883217930794\n",
      "step: 9682, loss: 0.010498455725610256\n",
      "step: 9683, loss: 0.00010456096060806885\n",
      "step: 9684, loss: 0.0018778657540678978\n",
      "step: 9685, loss: 0.013059189543128014\n",
      "step: 9686, loss: 0.000654373608995229\n",
      "step: 9687, loss: 0.0006660438375547528\n",
      "step: 9688, loss: 0.011520967818796635\n",
      "step: 9689, loss: 0.010225030593574047\n",
      "step: 9690, loss: 0.01131780631840229\n",
      "step: 9691, loss: 0.05432778224349022\n",
      "step: 9692, loss: 0.00021939081489108503\n",
      "step: 9693, loss: 0.0024782263208180666\n",
      "step: 9694, loss: 0.0017491739708930254\n",
      "step: 9695, loss: 0.0010230669286102057\n",
      "step: 9696, loss: 0.0014207883505150676\n",
      "step: 9697, loss: 0.010494866408407688\n",
      "step: 9698, loss: 0.0009017765987664461\n",
      "step: 9699, loss: 0.0006943929474800825\n",
      "step: 9700, loss: 0.0036164605990052223\n",
      "step: 9701, loss: 0.00923080649226904\n",
      "step: 9702, loss: 0.0005437643849290907\n",
      "step: 9703, loss: 0.0006203219527378678\n",
      "step: 9704, loss: 0.00246561411768198\n",
      "step: 9705, loss: 0.012379004620015621\n",
      "step: 9706, loss: 0.01964324153959751\n",
      "step: 9707, loss: 0.0011243048356845975\n",
      "step: 9708, loss: 0.014390189200639725\n",
      "step: 9709, loss: 0.0013296224642544985\n",
      "step: 9710, loss: 0.045195676386356354\n",
      "step: 9711, loss: 0.08059099316596985\n",
      "step: 9712, loss: 0.0006520411116071045\n",
      "step: 9713, loss: 0.002483263611793518\n",
      "step: 9714, loss: 0.09795630723237991\n",
      "step: 9715, loss: 0.0006528062513098121\n",
      "step: 9716, loss: 0.045916847884655\n",
      "step: 9717, loss: 0.0007930858992040157\n",
      "step: 9718, loss: 0.002224475145339966\n",
      "step: 9719, loss: 0.010867609642446041\n",
      "step: 9720, loss: 0.022943206131458282\n",
      "step: 9721, loss: 0.0010334436083212495\n",
      "step: 9722, loss: 0.0004230434133205563\n",
      "step: 9723, loss: 0.0006518281879834831\n",
      "step: 9724, loss: 0.00045565073378384113\n",
      "step: 9725, loss: 0.0010306686162948608\n",
      "step: 9726, loss: 0.00010217658564215526\n",
      "step: 9727, loss: 0.0016383782494813204\n",
      "step: 9728, loss: 0.0015450079226866364\n",
      "step: 9729, loss: 0.050760794430971146\n",
      "step: 9730, loss: 0.0008499821415171027\n",
      "step: 9731, loss: 0.0012208909029141068\n",
      "step: 9732, loss: 0.04686909541487694\n",
      "step: 9733, loss: 0.05179396644234657\n",
      "step: 9734, loss: 0.025593135505914688\n",
      "step: 9735, loss: 0.002637677127495408\n",
      "step: 9736, loss: 0.0013696652604267001\n",
      "step: 9737, loss: 0.014770997688174248\n",
      "step: 9738, loss: 0.0019411522662267089\n",
      "step: 9739, loss: 0.0028398565482348204\n",
      "step: 9740, loss: 0.012011533603072166\n",
      "step: 9741, loss: 0.000798646651674062\n",
      "step: 9742, loss: 0.009397919289767742\n",
      "step: 9743, loss: 0.002944949548691511\n",
      "step: 9744, loss: 0.024653611704707146\n",
      "step: 9745, loss: 0.03528166562318802\n",
      "step: 9746, loss: 0.0022548537235707045\n",
      "step: 9747, loss: 0.012221479788422585\n",
      "step: 9748, loss: 0.001744955312460661\n",
      "step: 9749, loss: 0.0005378005444072187\n",
      "step: 9750, loss: 0.006998611148446798\n",
      "step: 9751, loss: 0.0011341742938384414\n",
      "step: 9752, loss: 0.0027463550213724375\n",
      "step: 9753, loss: 0.001559936092235148\n",
      "step: 9754, loss: 0.0867064967751503\n",
      "step: 9755, loss: 0.002697755116969347\n",
      "step: 9756, loss: 0.03271763399243355\n",
      "step: 9757, loss: 0.0008924464927986264\n",
      "step: 9758, loss: 0.0010993562173098326\n",
      "step: 9759, loss: 0.0008283228962682188\n",
      "step: 9760, loss: 0.0054128123447299\n",
      "step: 9761, loss: 0.0014577374095097184\n",
      "step: 9762, loss: 0.04776664823293686\n",
      "step: 9763, loss: 0.0012177461758255959\n",
      "step: 9764, loss: 0.0003793063515331596\n",
      "step: 9765, loss: 0.0033891811035573483\n",
      "step: 9766, loss: 0.0008528925245627761\n",
      "step: 9767, loss: 0.000539344095159322\n",
      "step: 9768, loss: 0.03017827495932579\n",
      "step: 9769, loss: 0.03427475318312645\n",
      "step: 9770, loss: 0.05153699219226837\n",
      "step: 9771, loss: 0.017835164442658424\n",
      "step: 9772, loss: 0.0005388068966567516\n",
      "step: 9773, loss: 0.013192672282457352\n",
      "step: 9774, loss: 0.0008044059504754841\n",
      "step: 9775, loss: 9.126970689976588e-05\n",
      "step: 9776, loss: 0.05529022589325905\n",
      "step: 9777, loss: 0.00015507654461544007\n",
      "step: 9778, loss: 0.0026918300427496433\n",
      "step: 9779, loss: 0.008775829337537289\n",
      "step: 9780, loss: 0.0004130865272600204\n",
      "step: 9781, loss: 0.002343989908695221\n",
      "step: 9782, loss: 0.004108793567866087\n",
      "step: 9783, loss: 0.010519015602767467\n",
      "step: 9784, loss: 0.000342269690008834\n",
      "step: 9785, loss: 0.0006975504802539945\n",
      "step: 9786, loss: 0.0002934373915195465\n",
      "step: 9787, loss: 0.0893864557147026\n",
      "step: 9788, loss: 0.0007490044226869941\n",
      "step: 9789, loss: 0.0609191432595253\n",
      "step: 9790, loss: 0.00022102415096014738\n",
      "step: 9791, loss: 0.036780159920454025\n",
      "step: 9792, loss: 0.001512142363935709\n",
      "step: 9793, loss: 0.0008534738444723189\n",
      "step: 9794, loss: 0.008160353638231754\n",
      "step: 9795, loss: 0.051001254469156265\n",
      "step: 9796, loss: 0.0009066593484021723\n",
      "step: 9797, loss: 0.03703472390770912\n",
      "step: 9798, loss: 0.002222934504970908\n",
      "step: 9799, loss: 0.0009350187610834837\n",
      "step: 9800, loss: 0.001234412076883018\n",
      "step: 9801, loss: 0.024447498843073845\n",
      "step: 9802, loss: 0.0008251293911598623\n",
      "step: 9803, loss: 0.0009928139625117183\n",
      "step: 9804, loss: 0.002245549811050296\n",
      "step: 9805, loss: 0.03974434733390808\n",
      "step: 9806, loss: 0.0012325176503509283\n",
      "step: 9807, loss: 0.000804944196715951\n",
      "step: 9808, loss: 0.05408246070146561\n",
      "step: 9809, loss: 0.00018503103638067842\n",
      "step: 9810, loss: 0.0007022483041509986\n",
      "step: 9811, loss: 0.001129631418734789\n",
      "step: 9812, loss: 0.0013364211190491915\n",
      "step: 9813, loss: 0.00138943234924227\n",
      "step: 9814, loss: 0.00862206146121025\n",
      "step: 9815, loss: 0.0008923043496906757\n",
      "step: 9816, loss: 0.03642943874001503\n",
      "step: 9817, loss: 0.0005743674701079726\n",
      "step: 9818, loss: 0.05095040425658226\n",
      "step: 9819, loss: 0.001694551669061184\n",
      "step: 9820, loss: 0.0018907972844317555\n",
      "step: 9821, loss: 0.0005018003284931183\n",
      "step: 9822, loss: 0.007894746027886868\n",
      "step: 9823, loss: 0.0015123651828616858\n",
      "step: 9824, loss: 0.0014786282554268837\n",
      "step: 9825, loss: 0.03342588245868683\n",
      "step: 9826, loss: 0.0004977112403139472\n",
      "step: 9827, loss: 0.05000198259949684\n",
      "step: 9828, loss: 0.0053031002171337605\n",
      "step: 9829, loss: 0.048532627522945404\n",
      "step: 9830, loss: 0.0003914046392310411\n",
      "step: 9831, loss: 0.0008427099674008787\n",
      "step: 9832, loss: 0.0011142799630761147\n",
      "step: 9833, loss: 0.0014257177244871855\n",
      "step: 9834, loss: 0.03517933934926987\n",
      "step: 9835, loss: 0.0023335793521255255\n",
      "step: 9836, loss: 0.010954796336591244\n",
      "step: 9837, loss: 0.0009772826451808214\n",
      "step: 9838, loss: 0.007991109974682331\n",
      "step: 9839, loss: 0.010688208043575287\n",
      "step: 9840, loss: 0.037543393671512604\n",
      "step: 9841, loss: 0.0012414796510711312\n",
      "step: 9842, loss: 0.01045021042227745\n",
      "step: 9843, loss: 0.0012101266765967011\n",
      "step: 9844, loss: 0.0015473434468731284\n",
      "step: 9845, loss: 0.012473988346755505\n",
      "step: 9846, loss: 0.043281130492687225\n",
      "step: 9847, loss: 0.0006027267663739622\n",
      "step: 9848, loss: 0.00304973847232759\n",
      "step: 9849, loss: 0.0503707155585289\n",
      "step: 9850, loss: 0.003611476393416524\n",
      "step: 9851, loss: 0.01139001827687025\n",
      "step: 9852, loss: 0.023042183369398117\n",
      "step: 9853, loss: 0.0003747119044419378\n",
      "step: 9854, loss: 0.001610244158655405\n",
      "step: 9855, loss: 0.07241161167621613\n",
      "step: 9856, loss: 0.001085672527551651\n",
      "step: 9857, loss: 0.001416842918843031\n",
      "step: 9858, loss: 0.0007233854266814888\n",
      "step: 9859, loss: 0.0006459360010921955\n",
      "step: 9860, loss: 0.0026353916618973017\n",
      "step: 9861, loss: 0.0015896721743047237\n",
      "step: 9862, loss: 0.0009688748978078365\n",
      "step: 9863, loss: 0.01217715349048376\n",
      "step: 9864, loss: 0.00033588067162781954\n",
      "step: 9865, loss: 0.04987582564353943\n",
      "step: 9866, loss: 0.001695228274911642\n",
      "step: 9867, loss: 0.0008147557964548469\n",
      "step: 9868, loss: 0.0012747165746986866\n",
      "step: 9869, loss: 0.01621163636445999\n",
      "step: 9870, loss: 0.009814925491809845\n",
      "step: 9871, loss: 0.00035422018845565617\n",
      "step: 9872, loss: 0.003152332967147231\n",
      "step: 9873, loss: 0.0033958188723772764\n",
      "step: 9874, loss: 0.010308903641998768\n",
      "step: 9875, loss: 8.774541493039578e-05\n",
      "step: 9876, loss: 0.0005571941728703678\n",
      "step: 9877, loss: 0.00038806669181212783\n",
      "step: 9878, loss: 0.04868818446993828\n",
      "step: 9879, loss: 0.002964882180094719\n",
      "step: 9880, loss: 0.0008639105944894254\n",
      "step: 9881, loss: 0.017231188714504242\n",
      "step: 9882, loss: 0.010034419596195221\n",
      "step: 9883, loss: 0.0574445016682148\n",
      "step: 9884, loss: 0.0016356930136680603\n",
      "step: 9885, loss: 0.002464449964463711\n",
      "step: 9886, loss: 0.0007521099760197103\n",
      "step: 9887, loss: 0.02223774418234825\n",
      "step: 9888, loss: 0.0012474886607378721\n",
      "step: 9889, loss: 0.0009648182895034552\n",
      "step: 9890, loss: 0.0056409575045108795\n",
      "step: 9891, loss: 0.0014607342891395092\n",
      "step: 9892, loss: 0.06004883721470833\n",
      "step: 9893, loss: 0.002233171835541725\n",
      "step: 9894, loss: 0.0015341853722929955\n",
      "step: 9895, loss: 0.0016171579482033849\n",
      "step: 9896, loss: 0.00038365976070053875\n",
      "step: 9897, loss: 0.002053134376183152\n",
      "step: 9898, loss: 1.0732808113098145\n",
      "step: 9899, loss: 0.009367730468511581\n",
      "step: 9900, loss: 0.052802376449108124\n",
      "step: 9901, loss: 0.0006098659941926599\n",
      "step: 9902, loss: 0.0005005337297916412\n",
      "step: 9903, loss: 0.00037236843490973115\n",
      "step: 9904, loss: 0.0010276687098667026\n",
      "step: 9905, loss: 0.10753239691257477\n",
      "step: 9906, loss: 0.0390675887465477\n",
      "step: 9907, loss: 0.000770952261518687\n",
      "step: 9908, loss: 0.0010253378422930837\n",
      "step: 9909, loss: 0.02270459569990635\n",
      "step: 9910, loss: 0.000888085225597024\n",
      "step: 9911, loss: 0.0006386265158653259\n",
      "step: 9912, loss: 0.001717178849503398\n",
      "step: 9913, loss: 0.08124051988124847\n",
      "step: 9914, loss: 0.0007996969507075846\n",
      "step: 9915, loss: 0.002259188797324896\n",
      "step: 9916, loss: 0.0036176040302962065\n",
      "step: 9917, loss: 0.0007821781327947974\n",
      "step: 9918, loss: 0.03562483564019203\n",
      "step: 9919, loss: 0.04881035163998604\n",
      "step: 9920, loss: 0.0012818202376365662\n",
      "step: 9921, loss: 0.00048805863480083644\n",
      "step: 9922, loss: 0.010348391719162464\n",
      "step: 9923, loss: 0.0002716693561524153\n",
      "step: 9924, loss: 0.0020526982843875885\n",
      "step: 9925, loss: 0.050665076822042465\n",
      "step: 9926, loss: 0.00811727438122034\n",
      "step: 9927, loss: 0.0005718225729651749\n",
      "step: 9928, loss: 0.00021921592997387052\n",
      "step: 9929, loss: 0.0009098750306293368\n",
      "step: 9930, loss: 0.012849545106291771\n",
      "step: 9931, loss: 0.05162576586008072\n",
      "step: 9932, loss: 0.0012426190078258514\n",
      "step: 9933, loss: 0.000582536740694195\n",
      "step: 9934, loss: 0.06573734432458878\n",
      "step: 9935, loss: 0.000394916016375646\n",
      "step: 9936, loss: 0.02398468554019928\n",
      "step: 9937, loss: 0.00035554153146222234\n",
      "step: 9938, loss: 0.004764541517943144\n",
      "step: 9939, loss: 0.0005783783853985369\n",
      "step: 9940, loss: 0.0083903344348073\n",
      "step: 9941, loss: 0.019078243523836136\n",
      "step: 9942, loss: 0.003932170104235411\n",
      "step: 9943, loss: 0.0005937469541095197\n",
      "step: 9944, loss: 0.0007108612335287035\n",
      "step: 9945, loss: 0.02296118065714836\n",
      "step: 9946, loss: 8.366403199033812e-05\n",
      "step: 9947, loss: 0.0006685277912765741\n",
      "step: 9948, loss: 0.0031964415684342384\n",
      "step: 9949, loss: 0.009375796653330326\n",
      "step: 9950, loss: 0.0018153530545532703\n",
      "step: 9951, loss: 0.035225823521614075\n",
      "step: 9952, loss: 0.0007074520690366626\n",
      "step: 9953, loss: 0.03380182385444641\n",
      "step: 9954, loss: 0.0009322161786258221\n",
      "step: 9955, loss: 2.8530395866255276e-05\n",
      "step: 9956, loss: 0.054573655128479004\n",
      "step: 9957, loss: 0.0015366950538009405\n",
      "step: 9958, loss: 0.05681053176522255\n",
      "step: 9959, loss: 0.0008323328802362084\n",
      "step: 9960, loss: 0.0011553320800885558\n",
      "step: 9961, loss: 0.0008113134535960853\n",
      "step: 9962, loss: 0.002536221407353878\n",
      "step: 9963, loss: 0.01331237517297268\n",
      "step: 9964, loss: 0.0022641648538410664\n",
      "step: 9965, loss: 0.22092069685459137\n",
      "step: 9966, loss: 0.035347066819667816\n",
      "step: 9967, loss: 0.0011678400915116072\n",
      "step: 9968, loss: 0.0007696130778640509\n",
      "step: 9969, loss: 0.0007706745527684689\n",
      "step: 9970, loss: 0.0009809656767174602\n",
      "step: 9971, loss: 0.0007622750126756728\n",
      "step: 9972, loss: 0.0017875446937978268\n",
      "step: 9973, loss: 0.036823730915784836\n",
      "step: 9974, loss: 0.0011415306944400072\n",
      "step: 9975, loss: 0.07606054097414017\n",
      "step: 9976, loss: 0.0005675699794664979\n",
      "step: 9977, loss: 0.0005684476927854121\n",
      "step: 9978, loss: 0.002072739414870739\n",
      "step: 9979, loss: 0.0015776464715600014\n",
      "step: 9980, loss: 0.049641385674476624\n",
      "step: 9981, loss: 0.002674881834536791\n",
      "step: 9982, loss: 0.0006737501826137304\n",
      "step: 9983, loss: 0.0007236315286718309\n",
      "step: 9984, loss: 0.00230440148152411\n",
      "step: 9985, loss: 0.0007162248948588967\n",
      "step: 9986, loss: 0.002265573712065816\n",
      "step: 9987, loss: 0.0011098773684352636\n",
      "step: 9988, loss: 0.001379735884256661\n",
      "step: 9989, loss: 0.019948584958910942\n",
      "step: 9990, loss: 0.010806816630065441\n",
      "step: 9991, loss: 0.0034286410082131624\n",
      "step: 9992, loss: 0.0006716745556332171\n",
      "step: 9993, loss: 0.0023474199697375298\n",
      "step: 9994, loss: 0.0012203189544379711\n",
      "step: 9995, loss: 0.0029788606334477663\n",
      "step: 9996, loss: 0.0013077849289402366\n",
      "step: 9997, loss: 0.00017726951045915484\n",
      "step: 9998, loss: 0.01662527397274971\n",
      "step: 9999, loss: 0.0003063772164750844\n",
      "step: 10000, loss: 0.0025862210895866156\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset()\n",
    "    step, MAX_STEP = 0, 100**2\n",
    "    \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        plabels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict= {\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        step += 1\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))   \n",
    "    variables = tf.compat.v1.trainable_variables()\n",
    "    for var in variables:\n",
    "        save_parameters(name=var.name, value=var.eval(),epoch=train_data_reader._num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open('../datasets/saved-paras/'+filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')]for row in range(len(lines))]\n",
    "    return value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n",
      "Accuracy on test data:  0.04580456718003186\n"
     ]
    }
   ],
   "source": [
    "test_data_reader = DataReader(\n",
    "    data_path='../datasets/20news-bydate/test_tf_idf.txt',\n",
    "    batch_size=50,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    epoch = 44\n",
    "    trainable_variables = tf.compat.v1.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "    num_true_preds = 0\n",
    "    while True:\n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_plabels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict = {\n",
    "                mlp._X:test_data,\n",
    "                mlp._real_Y:test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_plabels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch', epoch)\n",
    "    print('Accuracy on test data: ', num_true_preds/ len(test_data_reader._data))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e12ac12a945f382e98440b0c94bbcb50fa5b9249500f0365ebc1c5d407b008b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
