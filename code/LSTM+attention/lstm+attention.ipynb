{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_words = 10000\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=n_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "batch_size=128\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "  def __init__(self, return_sequences=True):\n",
    "      self.return_sequences = return_sequences\n",
    "      super(Attention,self).__init__()\n",
    "\n",
    "  def build(self, input_shape):\n",
    "      self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                              initializer=\"normal\")\n",
    "      self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                              initializer=\"zeros\")\n",
    "\n",
    "      super(Attention,self).build(input_shape)\n",
    "  def call(self, x):\n",
    "      e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "      a = K.softmax(e, axis=1)\n",
    "      output = x*a\n",
    "      if self.return_sequences:\n",
    "\n",
    "          return output\n",
    "      return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 128)          98816     \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 200, 128)          328       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,399,785\n",
      "Trainable params: 1,399,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Attention(return_sequences=True)) # receive 3D and output 3D\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "196/196 [==============================] - 36s 135ms/step - loss: 0.6345 - accuracy: 0.6143 - val_loss: 0.5224 - val_accuracy: 0.7751\n",
      "Epoch 2/12\n",
      "196/196 [==============================] - 23s 117ms/step - loss: 0.4406 - accuracy: 0.8198 - val_loss: 0.3910 - val_accuracy: 0.8358\n",
      "Epoch 3/12\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3037 - accuracy: 0.8874 - val_loss: 0.3467 - val_accuracy: 0.8453\n",
      "Epoch 4/12\n",
      "196/196 [==============================] - 25s 127ms/step - loss: 0.2280 - accuracy: 0.9195 - val_loss: 0.3304 - val_accuracy: 0.8650\n",
      "Epoch 5/12\n",
      "196/196 [==============================] - 24s 125ms/step - loss: 0.1712 - accuracy: 0.9438 - val_loss: 0.3722 - val_accuracy: 0.8630\n",
      "Epoch 6/12\n",
      "196/196 [==============================] - 29s 147ms/step - loss: 0.1647 - accuracy: 0.9435 - val_loss: 0.4210 - val_accuracy: 0.8358\n",
      "Epoch 7/12\n",
      "196/196 [==============================] - 30s 156ms/step - loss: 0.1375 - accuracy: 0.9544 - val_loss: 0.4009 - val_accuracy: 0.8597\n",
      "Epoch 8/12\n",
      "196/196 [==============================] - 29s 146ms/step - loss: 0.0914 - accuracy: 0.9744 - val_loss: 0.5013 - val_accuracy: 0.8553\n",
      "Epoch 9/12\n",
      "196/196 [==============================] - 28s 142ms/step - loss: 0.0790 - accuracy: 0.9784 - val_loss: 0.5617 - val_accuracy: 0.8523\n",
      "Epoch 10/12\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 0.0570 - accuracy: 0.9861 - val_loss: 0.5558 - val_accuracy: 0.8388\n",
      "Epoch 11/12\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.0811 - accuracy: 0.9752 - val_loss: 0.5792 - val_accuracy: 0.8413\n",
      "Epoch 12/12\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.0556 - accuracy: 0.9841 - val_loss: 0.5502 - val_accuracy: 0.8540\n",
      "[0.6344530582427979, 0.4405810534954071, 0.3037141263484955, 0.22802868485450745, 0.17116324603557587, 0.16467730700969696, 0.13747334480285645, 0.09135077148675919, 0.07895759493112564, 0.05699450150132179, 0.08110612630844116, 0.05559977516531944]\n",
      "[0.6143199801445007, 0.8197600245475769, 0.887440025806427, 0.91948002576828, 0.9437599778175354, 0.9434800148010254, 0.9544399976730347, 0.9744399785995483, 0.9783999919891357, 0.9860799908638, 0.9752399921417236, 0.9840800166130066]\n"
     ]
    }
   ],
   "source": [
    "history3d=model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print(history3d.history['loss'])\n",
    "print(history3d.history['accuracy']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e12ac12a945f382e98440b0c94bbcb50fa5b9249500f0365ebc1c5d407b008b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
